:::MLL 1560880567.054702855 submission_org: {"value": "Intel_Corp", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.056440014 submission_platform: {"value": "32xCLX-8260L_CPUs", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.058095467 submission_division: {"value": "closed", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.060484085 submission_status: {"value": "onprem", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.062201336 submission_benchmark: {"value": "minigo", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.063867733 submission_poc_name: {"value": "Guokai Ma, Letian Kang, Christine Cheng, Mingxiao Huang", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.065565375 submission_poc_email: {"value": "guokai.ma@intel.com, letian.kang@intel.com, christine.cheng@intel.com, mingxiao.huang@intel.com", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.067319474 submission_entry: {"value": {"framework": "TensorFlow 1.13.1", "power": "none", "notes": "none", "interconnect": "OPA", "os": "Oracle Linux Server 7.6", "libraries": "MKLDNN (v0.18), MKL (v2019.0.3.20190220), IntelMPI (2018.1.163)", "compilers": "GCC6.3", "nodes": [{"num_nodes": 32, "cpu": "Intel(R) Xeon(R) Platinum 8260L CPU @ 2.40GHz", "num_cores": 48, "num_vcpus": "NA", "accelerator": "NA", "num_accelerators": 0, "sys_mem_size": "192G", "sys_storage_type": "SSD", "sys_storage_size": "800G", "cpu_accel_interconnect": "100Gb OPA", "network_card": "100Gb OPA", "num_network_cards": 1, "notes": "NA"}]}, "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880585.148444393 cache_clear: {value: true, metadata: {lineno: 0, file: manual}}
~/submission/benchmarks/minigo/implementations/tensorflow ~/submission/benchmarks/minigo/clx-8260l-2s-x32
Physical cores = 48
Virtual cores = 96
NUMA cores = 24
KMP_HW_SUBSET = 2T
Output to /lfs/lfs12/gma_akey
./run_mn.sh: line 20: ulimit: max user processes: cannot modify limit: Operation not permitted
Wiping dir /lfs/lfs12/gma_akey/results/epb015
:::MLL 1560880592.507820 init_start: {"value": null, "metadata": {'lineno': 742, 'file': 'ml_perf/reference_implementation.py'}}
Making dir /lfs/lfs12/gma_akey/results/epb015/models
Making dir /lfs/lfs12/gma_akey/results/epb015/data/selfplay
Making dir /lfs/lfs12/gma_akey/results/epb015/data/holdout
Making dir /lfs/lfs12/gma_akey/results/epb015/sgf/eval
Making dir /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks
Making dir /lfs/lfs12/gma_akey/results/epb015/work_dir
Making dir /lfs/lfs12/gma_akey/results/epb015/mpi
[2019-06-18 11:56:35] Selfplay nodes = ['epb015', 'epb016', 'epb018', 'epb019', 'epb048', 'epb013', 'epb012', 'epb011', 'epb009', 'epb008', 'epb007', 'epb006', 'epb005', 'epb004', 'epb073', 'epb074', 'epb075', 'epb076', 'epb077', 'epb078', 'epb079', 'epb100', 'epb021', 'epb022', 'epb023', 'epb024']
[2019-06-18 11:56:35] Train nodes = ['epb025', 'epb026', 'epb027', 'epb028', 'epb029', 'epb003']
[2019-06-18 11:56:35] Eval nodes = ['epb015', 'epb016', 'epb018', 'epb019', 'epb048', 'epb013', 'epb012', 'epb011', 'epb009', 'epb008', 'epb007', 'epb006', 'epb005', 'epb004', 'epb073', 'epb074', 'epb075', 'epb076', 'epb077', 'epb078', 'epb079', 'epb100', 'epb021', 'epb022', 'epb023', 'epb024']
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.66s/it]
[2019-06-18 11:59:26] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py:86: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
[2019-06-18 11:59:26] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
2019-06-18 11:59:26.082858: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-06-18 11:59:26.097014: I tensorflow/core/common_runtime/process_util.cc:113] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[2019-06-18 11:59:26] From ./quantize_graph.py:351: quantize_v2 (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.
Instructions for updating:
`tf.quantize_v2` is deprecated, please use `tf.quantization.quantize` instead.
2019-06-18 11:59:26.655961: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
[2019-06-18 11:59:26] From ./dual_net.py:679: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99']
Reading tf_records from 1 inputs
[2019-06-18 11:59:30] minmax time: 3.862 seconds
2019-06-18 11:59:30.528204: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:59:30.533567: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:59:30.538346: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880770.622013 init_stop: {"value": null, "metadata": {'lineno': 614, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880770.622388 run_start: {"value": null, "metadata": {'lineno': 615, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880770.622793 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:59:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 11:59:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=2 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=1023779833 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=2047559664 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=3071339495 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=4095119326 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=5118899157 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=6142678988 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=7166458819 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=8190238650 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=9214018481 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=10237798312 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=11261578143 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=12285357974 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=13309137805 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=14332917636 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=15356697467 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=16380477298 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=17404257129 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=18428036960 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000001-000000 --seed=19451816791 : \
-host epb0
[2019-06-18 12:00:04] selfplay finished: 34.263 seconds
[2019-06-18 12:00:04] selfplay mn: 34.286 seconds
[2019-06-18 12:00:04] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-2-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779833 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559664 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339495 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119326 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899157 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678988 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458819 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238650 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018481 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798312 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578143 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357974 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137805 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917636 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697467 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477298 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257129 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036960 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816791 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596622 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376453 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156284 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:00:24] divide_golden_chunk finished: 20.044 seconds
[2019-06-18 12:00:25] generate golden chunk: 20.061 seconds
[2019-06-18 12:00:28] train finished: 58.023 seconds
:::MLL 1560880788.394172 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.394547 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.394875 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.916945 47008390894464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.392578 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.392953 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.393277 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.916966 47550544327552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.329262 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.330028 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.330812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.917094 47068022465408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.330803 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.331526 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.332205 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.917124 47053429085056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:48.918054 47550544327552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprjgziru9
W0618 11:59:48.918082 47008390894464 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbwqn3i1q
W0618 11:59:48.918254 47068022465408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_2gazqdr
W0618 11:59:48.918279 47053429085056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf6bjawi5
I0618 11:59:48.919137 47550544327552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprjgziru9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f81e40e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.919143 47008390894464 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbwqn3i1q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac147072e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.919327 47053429085056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf6bjawi5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acbc3836e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.919324 47068022465408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_2gazqdr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf2958ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.919552 47550544327552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.919559 47008390894464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.919753 47053429085056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.919753 47068022465408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880788.424663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.425071 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.425414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.933793 47681979335552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.424100 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.424533 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.424871 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.933799 47443351499648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.347487 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.348363 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.349158 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.933997 47864332063616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.347496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.348371 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.349153 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:48.934049 47918128849792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:48.934878 47681979335552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpicujdr2w
W0618 11:59:48.934904 47443351499648 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1yjg8pw1
W0618 11:59:48.935081 47864332063616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2_tzt6ra
W0618 11:59:48.935112 47918128849792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplq0ii9w3
I0618 11:59:48.935891 47681979335552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpicujdr2w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5e1c071e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.935922 47443351499648 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1yjg8pw1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b268cb34da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.936150 47864332063616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2_tzt6ra', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b889118ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.936172 47918128849792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplq0ii9w3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9517a29e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:48.936303 47681979335552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.936344 47443351499648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.936590 47864332063616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:48.936604 47918128849792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:48.937450 47008390894464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.937573 47053429085056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.937689 47068022465408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.939184 47550544327552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.955839 47864332063616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.955843 47918128849792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.957064 47008390894464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.957300 47068022465408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.957476 47053429085056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.958619 47550544327552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.967338 47443351499648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.968341 47681979335552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:48.976118 47864332063616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.976368 47918128849792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.987043 47443351499648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:48.988037 47681979335552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:49.011698 47550544327552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.011878 47068022465408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.014963 47008390894464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.015981 47550544327552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:49.016172 47068022465408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:49.016214 47053429085056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.019292 47008390894464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:49.020556 47053429085056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:49.020993 47550544327552 estimator.py:1111] Calling model_fn.
W0618 11:59:49.021105 47550544327552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:49.021230 47068022465408 estimator.py:1111] Calling model_fn.
W0618 11:59:49.021343 47068022465408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:49.022495 47550544327552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:49.022722 47068022465408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:49.024389 47008390894464 estimator.py:1111] Calling model_fn.
W0618 11:59:49.024498 47008390894464 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:49.025704 47053429085056 estimator.py:1111] Calling model_fn.
W0618 11:59:49.025819 47053429085056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:49.025902 47008390894464 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:49.027229 47053429085056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:49.035053 47443351499648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.035724 47681979335552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.039387 47443351499648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:49.039595 47864332063616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:49.040061 47681979335552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:49.041275 47918128849792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880788.498439 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.498782 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.499101 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044278 47900578509696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.497600 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.497941 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.498307 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044252 47298878464896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.446133 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.447083 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.447922 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044252 46977498039168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.446132 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.447086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.447942 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044254 47549908386688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:49.044838 47298878464896 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1eu6k55i
W0618 11:59:49.044867 47900578509696 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmparczug40
I0618 11:59:49.045493 47298878464896 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1eu6k55i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04e96fbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.045521 47900578509696 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmparczug40', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b91018dbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880788.506343 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.506718 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.507041 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044099 46921503875968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.505241 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.505617 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.505958 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044091 47833855865728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.451312 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.452029 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.452715 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044134 46933899924352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:59:49.045792 47298878464896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880788.449214 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.449985 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.450756 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.044201 47571433579392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:49.043905 47864332063616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:49.045817 47900578509696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:49.045269 46977498039168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpotcl0loh
W0618 11:59:49.045293 47549908386688 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5480vqe_
I0618 11:59:49.046288 46977498039168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpotcl0loh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba15ab8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.046324 47549908386688 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5480vqe_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f5bfc4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.044494 47443351499648 estimator.py:1111] Calling model_fn.
W0618 11:59:49.044608 47443351499648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:49.046706 46977498039168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.046737 47549908386688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.045144 47681979335552 estimator.py:1111] Calling model_fn.
W0618 11:59:49.045253 47681979335552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:49.045129 46921503875968 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5ra_g1j7
W0618 11:59:49.045155 47833855865728 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1pqzj0we
W0618 11:59:49.045241 47571433579392 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4ca55d6s
W0618 11:59:49.045209 46933899924352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp26_nau0e
I0618 11:59:49.046109 46921503875968 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5ra_g1j7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad0c287dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:49.045598 47918128849792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:49.046129 47833855865728 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1pqzj0we', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8178930e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.046265 47571433579392 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4ca55d6s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b445efc9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.046258 46933899924352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp26_nau0e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafef051e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:49.046021 47443351499648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:49.046504 46921503875968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.046508 47833855865728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.046685 47571433579392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.046680 46933899924352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:49.046655 47681979335552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880788.487493 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.487942 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.488266 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.047293 47157000795008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.490370 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.490743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.491066 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.047334 47394787758976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.452818 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.453686 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.454543 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.047461 46939376939904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.452832 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.453737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.454584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.047477 47354398995328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:59:49.048978 47864332063616 estimator.py:1111] Calling model_fn.
W0618 11:59:49.048343 47157000795008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5c_gq91_
W0618 11:59:49.048370 47394787758976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8tdn4ih2
W0618 11:59:49.049090 47864332063616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:49.048509 46939376939904 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpclkas96i
I0618 11:59:49.048538 47354398995328 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11d6b7cd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.049322 47157000795008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5c_gq91_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae3e0de4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.049344 47394787758976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8tdn4ih2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b3e135dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.049523 46939376939904 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpclkas96i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab13579ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.049694 47354398995328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.049712 47157000795008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.049735 47394787758976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:49.049935 46939376939904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:49.050503 47864332063616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:49.050697 47918128849792 estimator.py:1111] Calling model_fn.
W0618 11:59:49.050805 47918128849792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:49.052204 47918128849792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:49.061408 46939376939904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.061492 47354398995328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.061534 47394787758976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.061551 47157000795008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.065113 47571433579392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.065141 46933899924352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.068422 47298878464896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.068438 47900578509696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.068681 47549908386688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.068752 46977498039168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880788.541576 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.541985 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.542375 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.071495 47285039784832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.538153 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.538632 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.539030 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.071498 47628198138752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.496557 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.497394 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.498217 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.071466 47619856610176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880788.497769 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880788.498615 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880788.499323 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:49.071536 47901706384256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:49.070696 46921503875968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:49.072605 47628198138752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0gzpqka4
W0618 11:59:49.072579 47619856610176 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9dflgdkn
W0618 11:59:49.072653 47285039784832 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmps8ebhb__
W0618 11:59:49.072678 47901706384256 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1h1f7s80
I0618 11:59:49.073580 47619856610176 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9dflgdkn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4fa5398e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.073614 47285039784832 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmps8ebhb__', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b01b0962e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:49.073596 47628198138752 estimator.py:201] Using config: {'_model_[2019-06-18 12:00:28] iteration time 0: 58.049 seconds
2019-06-18 12:00:29.042573: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880828.672159 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 12:00:32] minmax time: 3.234 seconds
2019-06-18 12:00:32.286281: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:00:32.291891: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:00:32.296582: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880832.307251 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 0}}
[2019-06-18 12:00:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:00:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=2 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=1023779833 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=2047559664 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=3071339495 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=4095119326 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=5118899157 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=6142678988 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=7166458819 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=8190238650 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=9214018481 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=10237798312 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=11261578143 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=12285357974 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=13309137805 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=14332917636 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=15356697467 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=16380477298 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=17404257129 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=18428036960 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=19451816791 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000001-000001 --seed=20475596622 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_
[2019-06-18 12:00:44] eval finished: 11.921 seconds
[2019-06-18 12:00:44] Win rate 000001-000001 vs checkpoint: 0.740
:::MLL 1560880844.292177 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 12:00:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=3 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=1023779834 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=2047559665 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=3071339496 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=4095119327 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=5118899158 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=6142678989 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=7166458820 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=8190238651 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=9214018482 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=10237798313 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=11261578144 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=12285357975 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=13309137806 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=14332917637 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=15356697468 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=16380477299 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=17404257130 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000002-000000 --seed=18428036961 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:01:15] selfplay finished: 31.106 seconds
[2019-06-18 12:01:15] selfplay mn: 31.126 seconds
[2019-06-18 12:01:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-3-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779834 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559665 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339496 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119327 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899158 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678989 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458820 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238651 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018482 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798313 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578144 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357975 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137806 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917637 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697468 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477299 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257130 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036961 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816792 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596623 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376454 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156285 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:01:16] train finished: 44.022 seconds
:::MLL 1560880837.547135 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.548001 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.548819 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.620259 47264161567616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.563034 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.563809 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.564561 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.620301 47767864513408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.621324 47264161567616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7i11sdi_
W0618 12:00:37.621361 47767864513408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcxjtnyx4
I0618 12:00:37.622409 47264161567616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7i11sdi_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afcd425eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.622455 47767864513408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcxjtnyx4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b721b2ede10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.622848 47264161567616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.622889 47767864513408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880837.550026 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.550817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.551483 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.623837 47957230453632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.553360 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.554057 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.554747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.623879 47706912129920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.624945 47957230453632 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp417qy6fo
W0618 12:00:37.624972 47706912129920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjsozp_8x
I0618 12:00:37.626032 47957230453632 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp417qy6fo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e3245de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.626065 47706912129920 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjsozp_8x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b63ea234e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.626495 47957230453632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.626532 47706912129920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.628183 47767864513408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.628193 47264161567616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.631895 47957230453632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.631967 47706912129920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880837.619139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.619585 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.619980 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.646566 47844603507584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.647936 47844603507584 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpejjfbn6a
:::MLL 1560880837.620517 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.620889 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.621202 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.649259 47084172567424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.619731 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.620117 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.620477 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.649361 47797057086336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
I0618 12:00:37.648971 47844603507584 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpejjfbn6a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b83f92f0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.649391 47844603507584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.650620 47084172567424 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7az8eqmg
W0618 12:00:37.650648 47797057086336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpn0swx16c
W0618 12:00:37.650668 47264161567616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.650771 47767864513408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:37.651684 47084172567424 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7az8eqmg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad2ebf7be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.651685 47797057086336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpn0swx16c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78e7323e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.652095 47084172567424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.652098 47797057086336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880837.626360 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.626836 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.627232 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.651817 47005380748160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.652809 47005380748160 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8v4_8wvd
W0618 12:00:37.654205 47957230453632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.654412 47706912129920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:37.653810 47005380748160 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8v4_8wvd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0939bfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.654204 47005380748160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.654211 47844603507584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.656897 47084172567424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.656904 47797057086336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.658704 47005380748160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.673810 47844603507584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.676434 47084172567424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.676599 47797057086336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.678549 47005380748160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.699202 47264161567616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.699583 47767864513408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.702393 47957230453632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.702608 47706912129920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.703485 47264161567616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:37.703885 47767864513408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:37.706691 47957230453632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:37.706905 47706912129920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880837.637703 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.638476 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.639182 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.706235 47014124942208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.632087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.633020 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.633910 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.706226 47789024387968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.707312 47789024387968 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxlxsl1x0
I0618 12:00:37.707308 47014124942208 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac29ccdcd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.708370 47789024387968 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxlxsl1x0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b770868fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.708528 47014124942208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.708528 47264161567616 estimator.py:1111] Calling model_fn.
W0618 12:00:37.708635 47264161567616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880837.636250 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.637133 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.637955 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.709003 47051201590144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
I0618 12:00:37.708807 47789024387968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880837.636987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.637884 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.638638 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.709013 47650675430272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
I0618 12:00:37.708950 47767864513408 estimator.py:1111] Calling model_fn.
W0618 12:00:37.709063 47767864513408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:37.709983 47264161567616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.710127 47650675430272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzb1hxola
W0618 12:00:37.710421 47767864513408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.710152 47051201590144 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpk8dtzbaw
I0618 12:00:37.711220 47650675430272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzb1hxola', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b56d22b6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.711236 47051201590144 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpk8dtzbaw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb3ebe6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.711729 47957230453632 estimator.py:1111] Calling model_fn.
I0618 12:00:37.711651 47650675430272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.711668 47051201590144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.711838 47957230453632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:37.711952 47706912129920 estimator.py:1111] Calling model_fn.
W0618 12:00:37.712057 47706912129920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:37.713183 47957230453632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.713413 47706912129920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.713771 47014124942208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.713897 47789024387968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.716880 47650675430272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.716925 47051201590144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.721266 47844603507584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.724024 47084172567424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.724362 47797057086336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:37.725580 47844603507584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:37.726163 47005380748160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880837.690987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.691519 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.691942 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.726068 47900585767808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.690994 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.691520 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.691950 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.726160 47131001287552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.728319 47084172567424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:37.727074 47900585767808 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzierqhpu
W0618 12:00:37.727144 47131001287552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpb5ts8c5c
W0618 12:00:37.728682 47797057086336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:37.728106 47900585767808 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzierqhpu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9101fc8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.728178 47131001287552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpb5ts8c5c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2addd32d3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.728496 47900585767808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.728565 47131001287552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880837.694438 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.694822 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.695167 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.728886 47032754803584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.690986 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.691497 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.691899 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.728884 46923224253312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.730519 47005380748160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:37.730650 47844603507584 estimator.py:1111] Calling model_fn.
W0618 12:00:37.730762 47844603507584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:37.730629 46923224253312 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp3doc50gu
W0618 12:00:37.730658 47032754803584 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw2sl4b_7
I0618 12:00:37.731657 46923224253312 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp3doc50gu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad72b35e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.731677 47032754803584 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw2sl4b_7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac6f33aee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.732042 46923224253312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.732065 47032754803584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.732124 47844603507584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:00:37.733386 47084172567424 estimator.py:1111] Calling model_fn.
W0618 12:00:37.733494 47084172567424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:37.733762 47797057086336 estimator.py:1111] Calling model_fn.
W0618 12:00:37.733870 47797057086336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:37.733223 47900585767808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.733280 47131001287552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.734850 47084172567424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.735227 47797057086336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.734906 47789024387968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.734987 47014124942208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:37.735637 47005380748160 estimator.py:1111] Calling model_fn.
W0618 12:00:37.735747 47005380748160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:37.736790 47032754803584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.736782 46923224253312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.737128 47005380748160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:37.738195 47650675430272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.738309 47051201590144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880837.679956 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.680741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.681428 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.741748 47300712178560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.667206 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.668170 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.669103 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.741773 47612438299520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.742828 47612438299520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8ribe9ow
W0618 12:00:37.742853 47300712178560 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpu2n7q52c
I0618 12:00:37.743899 47612438299520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8ribe9ow', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4deb0f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.743916 47300712178560 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpu2n7q52c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0556bbee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.744354 47612438299520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.744366 47300712178560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880837.675321 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.676055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.676798 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.746984 47075305046912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.673033 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.673816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.674540 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.747030 46973596754816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:37.747928 47075305046912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppqbnf_dd
W0618 12:00:37.747987 46973596754816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmfbp9kya
I0618 12:00:37.748906 47075305046912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppqbnf_dd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0db6c1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.748949 46973596754816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmfbp9kya', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab92d22be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:37.749300 47075305046912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:37.749343 46973596754816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:37.749673 47300712178560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.749702 47612438299520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.754341 47075305046912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.754351 46973596754816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:37.752941 47131001287552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.752979 47900585767808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.756567 46923224253312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:37.756631 47032754803584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880837.732991 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880837.733376 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880837.733696 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:37.761282 47555056317312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880837.734860 global_batch_size: {"value": 8192, "metadata":[2019-06-18 12:01:18] divide_golden_chunk finished: 3.321 seconds
[2019-06-18 12:01:18] generate golden chunk: 3.336 seconds
[2019-06-18 12:01:18] moving /lfs/lfs12/gma_akey/results/epb015/models/000002-000001.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000002-000002.meta
[2019-06-18 12:01:18] moving /lfs/lfs12/gma_akey/results/epb015/models/000002-000001.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000002-000002.data-00000-of-00001
[2019-06-18 12:01:18] moving /lfs/lfs12/gma_akey/results/epb015/models/000002-000001.index --> /lfs/lfs12/gma_akey/results/epb015/models/000002-000002.index
[2019-06-18 12:01:18] moving /lfs/lfs12/gma_akey/results/epb015/models/000002-000001.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb
[2019-06-18 12:01:18] iteration time 1: 50.123 seconds
2019-06-18 12:01:19.201755: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880878.795450 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 12:01:22] minmax time: 3.200 seconds
2019-06-18 12:01:22.411503: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:01:22.416928: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:01:22.421578: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880882.430781 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 1}}
[2019-06-18 12:01:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:01:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=3 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=1023779834 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=2047559665 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=3071339496 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=4095119327 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=5118899158 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=6142678989 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=7166458820 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=8190238651 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=9214018482 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=10237798313 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=11261578144 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=12285357975 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=13309137806 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=14332917637 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=15356697468 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=16380477299 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=17404257130 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=18428036961 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=19451816792 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000002-000002 --seed=20475596623 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:01:35] eval finished: 12.779 seconds
[2019-06-18 12:01:35] Win rate 000002-000002 vs 000001-000001: 0.570
:::MLL 1560880895.271701 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 12:01:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=4 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=1023779835 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=2047559666 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=3071339497 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=4095119328 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=5118899159 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=6142678990 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=7166458821 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=8190238652 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=9214018483 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=10237798314 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=11261578145 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=12285357976 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=13309137807 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=14332917638 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=15356697469 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=16380477300 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=17404257131 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000003-000001 --seed=18428036962 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:02:06] selfplay finished: 31.195 seconds
[2019-06-18 12:02:06] selfplay mn: 31.215 seconds
[2019-06-18 12:02:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-4-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779835 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559666 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339497 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119328 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899159 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678990 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458821 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238652 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018483 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798314 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578145 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357976 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137807 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917638 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697469 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477300 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257131 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036962 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816793 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596624 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376455 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156286 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:02:06] train finished: 44.336 seconds
:::MLL 1560880887.724894 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.725628 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.726364 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.775458 47132286993280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.704287 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.705203 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.706023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.775453 47210333541248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.776497 47132286993280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpd00xk0xl
W0618 12:01:27.776524 47210333541248 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpk5z6ghqm
I0618 12:01:27.777582 47132286993280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpd00xk0xl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade1fcf9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.777605 47210333541248 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpk5z6ghqm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af04bbf4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.778028 47132286993280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.778041 47210333541248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.783522 47210333541248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.783557 47132286993280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.805217 47210333541248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.805449 47132286993280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880887.791463 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.791867 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.792215 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.826662 47377121338240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.791354 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.791759 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.792115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.826730 46978731807616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.827745 47377121338240 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw0kfoigq
W0618 12:01:27.827841 46978731807616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpy79utia2
I0618 12:01:27.828820 47377121338240 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw0kfoigq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1721133e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.828893 46978731807616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpy79utia2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba5f356e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.829241 47377121338240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.829302 46978731807616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880887.782408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.783239 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.783999 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.832261 47766509159296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.761544 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.762458 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.763337 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.832269 47562110665600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.833360 47766509159296 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9qtwheb1
W0618 12:01:27.833315 47562110665600 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4b8b_k6f
I0618 12:01:27.834419 47562110665600 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4b8b_k6f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42334c5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.834441 47766509159296 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9qtwheb1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b71ca65ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:27.834060 47377121338240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.834138 46978731807616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:27.834873 47562110665600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.834892 47766509159296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.840369 47562110665600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.840388 47766509159296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.853871 47377121338240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.854015 46978731807616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.855435 47210333541248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.855999 47132286993280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.859707 47210333541248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.860332 47132286993280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.862850 47766509159296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.862930 47562110665600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:27.864737 47210333541248 estimator.py:1111] Calling model_fn.
W0618 12:01:27.864843 47210333541248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:27.865494 47132286993280 estimator.py:1111] Calling model_fn.
W0618 12:01:27.865597 47132286993280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.866192 47210333541248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:27.866958 47132286993280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880887.841475 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.841857 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.842175 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.876010 47888552682368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.840703 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.841093 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.841457 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.876044 47209980429184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.877114 47888552682368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpud5x1bt9
W0618 12:01:27.877142 47209980429184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpejg9qti8
I0618 12:01:27.878075 47888552682368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpud5x1bt9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e34c22e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.878122 47209980429184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpejg9qti8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af036b35e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.878476 47888552682368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.878508 47209980429184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880887.806977 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.807919 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.808803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.878360 47085376881536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.815622 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.816326 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.817096 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.878381 47356911788928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
I0618 12:01:27.879488 47356911788928 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b126c7ded30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:27.879741 47085376881536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpua2ugxci
I0618 12:01:27.880797 47356911788928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.880842 47085376881536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpua2ugxci', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad333c01e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.881285 47085376881536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.883218 47888552682368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.883312 47209980429184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.886254 47356911788928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.886453 47085376881536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.901161 47377121338240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.901415 46978731807616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.903156 47888552682368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.903188 47209980429184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880887.869460 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.869864 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.870210 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.903862 47154917417856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.869689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.870097 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.870430 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.903887 47993525687168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.905447 47377121338240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.904915 47993525687168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw8cklops
W0618 12:01:27.904879 47154917417856 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgtlae8g6
W0618 12:01:27.905749 46978731807616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:27.905919 47154917417856 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgtlae8g6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae364b07e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.905951 47993525687168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw8cklops', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6a5a32e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.906335 47154917417856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.906362 47993525687168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.908982 47356911788928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.908995 47085376881536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:27.910491 47377121338240 estimator.py:1111] Calling model_fn.
W0618 12:01:27.910598 47377121338240 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:27.910875 46978731807616 estimator.py:1111] Calling model_fn.
W0618 12:01:27.910985 46978731807616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.911154 47154917417856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.911182 47993525687168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.911936 47377121338240 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:27.912363 46978731807616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:27.914539 47766509159296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.914581 47562110665600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.919174 47766509159296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.919222 47562110665600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:27.924639 47766509159296 estimator.py:1111] Calling model_fn.
I0618 12:01:27.924702 47562110665600 estimator.py:1111] Calling model_fn.
W0618 12:01:27.924756 47766509159296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.924816 47562110665600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.926208 47766509159296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:27.926283 47562110665600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880887.852624 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.853506 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.854337 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.928501 47639921591168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.852130 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.852949 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.853803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.928739 46920289682304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.929564 47639921591168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqt4aavry
I0618 12:01:27.930668 47639921591168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqt4aavry', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b545130de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:27.929856 46920289682304 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptezrbicn
I0618 12:01:27.931005 46920289682304 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptezrbicn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacc3c95e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.931117 47639921591168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.931472 46920289682304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.931091 47993525687168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.931069 47154917417856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880887.867989 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.868750 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.869454 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.934021 47708827100032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.863689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.864628 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.865432 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.934158 47511988515712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.935050 47708827100032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpueldb424
W0618 12:01:27.935215 47511988515712 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfq13f96q
I0618 12:01:27.936096 47708827100032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpueldb424', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b645c477e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.936288 47511988515712 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfq13f96q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3687c90e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.936562 47708827100032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:27.936757 47511988515712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.936392 47639921591168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.936881 46920289682304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.941387 47708827100032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.941682 47511988515712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880887.894145 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.894547 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.894901 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.947505 46977506411392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880887.896058 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.896449 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.896793 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.948416 47461031981952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.948531 46977506411392 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf7vt8hjn
I0618 12:01:27.949550 46977506411392 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf7vt8hjn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba162b5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.949936 46977506411392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.949437 47461031981952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp87jp8fia
I0618 12:01:27.950413 47461031981952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp87jp8fia', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2aaa8a0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.950794 47461031981952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.950936 47209980429184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.950949 47888552682368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880887.922929 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880887.923480 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880887.923950 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:27.955321 47395132896128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:27.954543 46977506411392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.955250 47209980429184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.955265 47888552682368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.955602 47461031981952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.956381 47395132896128 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpb466z97r
I0618 12:01:27.957457 47395132896128 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpb466z97r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b52a5cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:27.957889 47395132896128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:27.956559 47356911788928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.956812 47085376881536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:27.957965 47639921591168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.958414 46920289682304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.961425 47708827100032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:27.960353 47209980429184 estimator.py:1111] Calling model_fn.
I0618 12:01:27.960385 47888552682368 estimator.py:1111] Calling model_fn.
W0618 12:01:27.960463 47209980429184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.960489 47888552682368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:27.962157 47511988515712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:27.960870 47356911788928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.961112 47085376881536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:27.963021 47395132896128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:27.961822 47209980429184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:27.961862 47888552682368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1[2019-06-18 12:02:09] divide_golden_chunk finished: 3.299 seconds
[2019-06-18 12:02:09] generate golden chunk: 3.314 seconds
[2019-06-18 12:02:09] moving /lfs/lfs12/gma_akey/results/epb015/models/000003-000002.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000003-000003.data-00000-of-00001
[2019-06-18 12:02:09] moving /lfs/lfs12/gma_akey/results/epb015/models/000003-000002.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb
[2019-06-18 12:02:09] moving /lfs/lfs12/gma_akey/results/epb015/models/000003-000002.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000003-000003.meta
[2019-06-18 12:02:09] moving /lfs/lfs12/gma_akey/results/epb015/models/000003-000002.index --> /lfs/lfs12/gma_akey/results/epb015/models/000003-000003.index
[2019-06-18 12:02:09] iteration time 2: 51.047 seconds
2019-06-18 12:02:10.276743: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880929.842236 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 12:02:13] minmax time: 3.205 seconds
2019-06-18 12:02:13.491518: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:02:13.497063: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:02:13.501543: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880933.510926 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 2}}
[2019-06-18 12:02:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:02:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=4 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=1023779835 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=2047559666 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=3071339497 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=4095119328 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=5118899159 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=6142678990 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=7166458821 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=8190238652 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=9214018483 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=10237798314 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=11261578145 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=12285357976 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=13309137807 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=14332917638 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=15356697469 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=16380477300 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=17404257131 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=18428036962 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=19451816793 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000003-000003 --seed=20475596624 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:02:24] eval finished: 11.165 seconds
[2019-06-18 12:02:24] Win rate 000003-000003 vs 000002-000002: 0.300
:::MLL 1560880944.735145 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 12:02:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=5 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=1023779836 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=2047559667 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=3071339498 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=4095119329 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=5118899160 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=6142678991 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=7166458822 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=8190238653 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=9214018484 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=10237798315 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=11261578146 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=12285357977 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=13309137808 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=14332917639 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=15356697470 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=16380477301 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=17404257132 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000004-000002 --seed=18428036963 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:02:56] selfplay finished: 31.622 seconds
[2019-06-18 12:02:56] selfplay mn: 31.642 seconds
[2019-06-18 12:02:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-5-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779836 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559667 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339498 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119329 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899160 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678991 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458822 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238653 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018484 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798315 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578146 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357977 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137808 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917639 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697470 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477301 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257132 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036963 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816794 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596625 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376456 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156287 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:02:57] train finished: 44.290 seconds
:::MLL 1560880938.749307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.750027 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.750720 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.817497 47943257027456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.747182 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.747860 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.748543 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.817540 47133353599872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.818565 47943257027456 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp54glk3e_
W0618 12:02:18.818662 47133353599872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7gxvaw9f
I0618 12:02:18.819635 47943257027456 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp54glk3e_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9af1644e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.819710 47133353599872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7gxvaw9f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade5f62be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.820030 47943257027456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.820107 47133353599872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.824960 47133353599872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.825005 47943257027456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.846837 47133353599872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.847508 47943257027456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880938.807922 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.808750 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.809495 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.862476 47925188703104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.795707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.796595 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.797413 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.862522 47238773416832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.863567 47925188703104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2x6h80pq
W0618 12:02:18.863603 47238773416832 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2bk87r9k
I0618 12:02:18.864625 47925188703104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2x6h80pq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96bc6f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.864652 47238773416832 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2bk87r9k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af6eae57e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.865052 47925188703104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.865063 47238773416832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880938.827210 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.827636 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.828023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.865539 47024148714368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.827415 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.827846 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.828216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.865599 47076152550272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.866575 47024148714368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2h3hh2lg
W0618 12:02:18.866608 47076152550272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptxmea8du
I0618 12:02:18.867631 47024148714368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2h3hh2lg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4f2446da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.867753 47076152550272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptxmea8du', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad10deffe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.868063 47024148714368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.868190 47076152550272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.870608 47238773416832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.870643 47925188703104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.872843 47024148714368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.872977 47076152550272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.892662 47238773416832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.892930 47925188703104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.892523 47024148714368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.892581 47076152550272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.897959 47133353599872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.898936 47943257027456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.902369 47133353599872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:18.903437 47943257027456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880938.848044 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.848877 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.849675 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.904871 47505808900992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.838019 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.838959 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.839826 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.904888 47555956507520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.906009 47505808900992 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc5cq_vb1
W0618 12:02:18.906041 47555956507520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnota98hf
I0618 12:02:18.907136 47505808900992 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc5cq_vb1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3517738dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.907143 47555956507520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnota98hf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40c47b5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.907593 47555956507520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.907607 47505808900992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.907517 47133353599872 estimator.py:1111] Calling model_fn.
W0618 12:02:18.907643 47133353599872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:18.908697 47943257027456 estimator.py:1111] Calling model_fn.
W0618 12:02:18.908806 47943257027456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:18.909045 47133353599872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:18.910171 47943257027456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:18.913016 47505808900992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.913015 47555956507520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880938.877101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.877478 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.877816 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.913888 47933231625088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.878143 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.878512 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.878835 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.913935 47640057693056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.914967 47640057693056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplrk3kxcx
W0618 12:02:18.914996 47933231625088 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4hi_kdth
I0618 12:02:18.915938 47640057693056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplrk3kxcx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54594d9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.915968 47933231625088 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4hi_kdth', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b989bd4ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.916329 47640057693056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.916362 47933231625088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.920865 47640057693056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.920907 47933231625088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880938.867535 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.868344 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.869115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.923253 47822963958656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.856729 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.857666 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.858527 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.923298 47898524492672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 12:02:18.924409 47898524492672 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90871ffd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:18.924405 47822963958656 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp62ckve_9
I0618 12:02:18.925562 47822963958656 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp62ckve_9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7eef5dae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.925662 47898524492672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.926038 47822963958656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880938.865408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.866157 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.866785 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.929809 47055459619712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.863932 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.864737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.865533 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.929867 47728277574528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.930939 47055459619712 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1d0acr4b
W0618 12:02:18.930971 47728277574528 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpovch9e_p
I0618 12:02:18.932060 47055459619712 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1d0acr4b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc3c8ade10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.932064 47728277574528 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpovch9e_p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68e39e1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.932509 47055459619712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.932509 47728277574528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.931128 47898524492672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.931564 47822963958656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.935268 47555956507520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.935710 47505808900992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.937876 47728277574528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.937942 47055459619712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.940505 47933231625088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.940520 47640057693056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.940199 47024148714368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.940372 47076152550272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.942921 47238773416832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.943580 47925188703104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:18.944551 47024148714368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:18.944721 47076152550272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:18.947225 47238773416832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:18.947945 47925188703104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:02:18.949640 47024148714368 estimator.py:1111] Calling model_fn.
W0618 12:02:18.949747 47024148714368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:18.949806 47076152550272 estimator.py:1111] Calling model_fn.
W0618 12:02:18.949913 47076152550272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:18.951093 47024148714368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:18.951277 47076152550272 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:02:18.952284 47238773416832 estimator.py:1111] Calling model_fn.
W0618 12:02:18.952389 47238773416832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:18.953055 47925188703104 estimator.py:1111] Calling model_fn.
W0618 12:02:18.953167 47925188703104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:18.953745 47238773416832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:18.954535 47925188703104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880938.918072 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.918449 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.918770 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.953823 47920442459008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.954122 47898524492672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880938.917959 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.918336 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.918670 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.954521 47298503086976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.920071 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.920488 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.920849 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.954521 47725531206528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.954714 47822963958656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880938.919912 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.920292 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.920669 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.955273 47582886318976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.954849 47920442459008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdhj1n2i0
W0618 12:02:18.955531 47298503086976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa_gundox
I0618 12:02:18.955871 47920442459008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdhj1n2i0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b95a1899dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.956501 47298503086976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa_gundox', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04d30fee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:18.955509 47725531206528 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyybr4yyj
I0618 12:02:18.956282 47920442459008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.956892 47298503086976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.956225 47582886318976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2t6zksj5
I0618 12:02:18.956531 47725531206528 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyybr4yyj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b683febee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.957238 47582886318976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2t6zksj5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47099fae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.957048 47725531206528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.957657 47582886318976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880938.923905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.924279 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.924607 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.959447 47852534301568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.960456 47728277574528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.960795 47055459619712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880938.925544 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.925922 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.926283 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.960750 47897847153536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.960508 47852534301568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_kb3pz1c
I0618 12:02:18.961537 47852534301568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_kb3pz1c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b85d1e55e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.961924 47852534301568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.961773 47897847153536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpksqj_yzf
I0618 12:02:18.962789 47897847153536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpksqj_yzf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b905ec09e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:18.961025 47920442459008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.961636 47298503086976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:18.963201 47897847153536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.962162 47582886318976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.961687 47725531206528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.966758 47852534301568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.967965 47897847153536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880938.914446 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.915348 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.916069 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.979101 47243860177792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880938.918313 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880938.919070 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880938.919761 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:18.979124 47296746713984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:18.980188 47296746713984 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzg9nyq11
W0618 12:02:18.980219 47243860177792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptrz5p6fz
I0618 12:02:18.981253 47243860177792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptrz5p6fz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af81a173dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.981254 47296746713984 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzg9nyq11', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b046a5fbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:18.981669 47243860177792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:18.981666 47296746713984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:18.980688 47920442459008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.981369 47298503086976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.981746 47582886318976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.981310 47725531206528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:18.986760 47243860177792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:18.986773 47296746713984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatic[2019-06-18 12:02:59] divide_golden_chunk finished: 3.263 seconds
[2019-06-18 12:02:59] generate golden chunk: 3.278 seconds
[2019-06-18 12:02:59] iteration time 3: 49.814 seconds
2019-06-18 12:03:00.144176: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880979.656285 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 12:03:03] minmax time: 3.188 seconds
2019-06-18 12:03:03.342246: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:03.347956: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:03.352456: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880983.364212 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 3}}
[2019-06-18 12:03:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:03:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=5 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=1023779836 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=2047559667 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=3071339498 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=4095119329 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=5118899160 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=6142678991 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=7166458822 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=8190238653 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=9214018484 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=10237798315 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=11261578146 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=12285357977 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=13309137808 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=14332917639 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=15356697470 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=16380477301 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=17404257132 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=18428036963 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=19451816794 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000004-000003 --seed=20475596625 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:03:14] eval finished: 11.086 seconds
[2019-06-18 12:03:14] Win rate 000004-000003 vs 000002-000002: 0.500
:::MLL 1560880994.508401 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 12:03:14] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=6 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=1023779837 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=2047559668 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=3071339499 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=4095119330 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=5118899161 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=6142678992 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=7166458823 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=8190238654 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=9214018485 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=10237798316 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=11261578147 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=12285357978 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=13309137809 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=14332917640 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=15356697471 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=16380477302 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=17404257133 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000005-000002 --seed=18428036964 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:03:43] selfplay finished: 29.089 seconds
[2019-06-18 12:03:43] selfplay mn: 29.109 seconds
[2019-06-18 12:03:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-6-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779837 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559668 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339499 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119330 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899161 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678992 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458823 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238654 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018485 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798316 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578147 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357978 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137809 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917640 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697471 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477302 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257133 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036964 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816795 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596626 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376457 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156288 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:03:46] divide_golden_chunk finished: 3.239 seconds
[2019-06-18 12:03:46] generate golden chunk: 3.254 seconds
[2019-06-18 12:03:47] train finished: 44.205 seconds
:::MLL 1560880988.550209 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.551085 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.551920 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.623271 47571547300736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.555905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.556688 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.557392 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.623304 47305405854592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.624382 47571547300736 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1hn6zh82
W0618 12:03:08.624429 47305405854592 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp65oeg2hk
I0618 12:03:08.625468 47571547300736 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1hn6zh82', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4465c3eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.625503 47305405854592 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp65oeg2hk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b066e7fbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.625902 47571547300736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.625925 47305405854592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.631246 47305405854592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.631249 47571547300736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.653939 47571547300736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.654448 47305405854592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880988.583299 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.584168 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.585003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.655575 47466803352448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.583306 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.584197 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.585039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.655611 48009303647104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.656627 48009303647104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp50yctmyg
W0618 12:03:08.656661 47466803352448 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_1676xaq
I0618 12:03:08.657698 48009303647104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp50yctmyg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baa5213bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.657738 47466803352448 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_1676xaq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c028a2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.658126 48009303647104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.658154 47466803352448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.663030 47466803352448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.663013 48009303647104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880988.625633 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.626006 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.626356 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.665032 46944367137664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.627580 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.628025 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.628436 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.666447 47250374587264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.666083 46944367137664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmphkl57k6l
I0618 12:03:08.667069 46944367137664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmphkl57k6l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab25eea2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.667464 46944367137664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.667402 47250374587264 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw7dxt58u
I0618 12:03:08.668403 47250374587264 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw7dxt58u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af99e614e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.668822 47250374587264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.672202 46944367137664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.673556 47250374587264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.685724 48009303647104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.686163 47466803352448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.691855 46944367137664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.693133 47250374587264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880988.660814 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.661294 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.661709 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.700891 47425473307520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.661103 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.661531 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.661919 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.700907 47954102088576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.701971 47954102088576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw53r9ce3
W0618 12:03:08.702001 47425473307520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzyd9g4l5
I0618 12:03:08.702969 47954102088576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw53r9ce3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d77cece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.702994 47425473307520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzyd9g4l5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b226313bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.703358 47954102088576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.703378 47425473307520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.704539 47305405854592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.704715 47571547300736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.708203 47954102088576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.708223 47425473307520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.708830 47305405854592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:08.709103 47571547300736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880988.640343 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.641055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.641726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.709744 47639125750656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.635839 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.636745 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.637601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.709808 46942988841856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.637918 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.638651 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.639324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.710487 47126005535616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.635624 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.636402 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.637140 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.710544 47392642241408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.710874 47639125750656 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6jr70gv5
W0618 12:03:08.710901 46942988841856 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1v2ixily
I0618 12:03:08.712007 47639125750656 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6jr70gv5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5421c13e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.712007 46942988841856 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1v2ixily', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab20cc30e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.712462 46942988841856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.712476 47639125750656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.711616 47392642241408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6t5km61w
I0618 12:03:08.711639 47126005535616 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adca9682d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.712709 47392642241408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6t5km61w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1abe316e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.712878 47126005535616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.713148 47392642241408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.713914 47305405854592 estimator.py:1111] Calling model_fn.
W0618 12:03:08.714025 47305405854592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:08.714236 47571547300736 estimator.py:1111] Calling model_fn.
W0618 12:03:08.714347 47571547300736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:08.715379 47305405854592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.715710 47571547300736 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.717807 47639125750656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.717817 46942988841856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.718268 47126005535616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.718477 47392642241408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.727861 47954102088576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.728077 47425473307520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.735466 48009303647104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.735641 47466803352448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880988.688126 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.688594 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.688968 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.736855 47932303065984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.691183 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.691576 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.691938 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.737117 47287908578176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.737901 47932303065984 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpidlo7ojo
W0618 12:03:08.738103 47287908578176 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpz0ow3wsr
I0618 12:03:08.738892 47932303065984 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpidlo7ojo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98647c1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.739109 47287908578176 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpz0ow3wsr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b025b948e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:08.739770 48009303647104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:08.739281 47932303065984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.739947 47466803352448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:08.739504 47287908578176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.739567 46944367137664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.740321 46942988841856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.740433 47639125750656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.740392 47126005535616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.740527 47392642241408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.740875 47250374587264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880988.704520 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.704937 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.705271 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.743439 47679582315392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.704233 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.704635 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.705007 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.743491 47740116415360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.743895 46944367137664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:08.744151 47932303065984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880988.673159 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.673900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.674574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.745888 47824709489536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
I0618 12:03:08.744818 48009303647104 estimator.py:1111] Calling model_fn.
:::MLL 1560880988.676187 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.676930 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.677642 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.746029 47802275791744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.744930 48009303647104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:08.745016 47466803352448 estimator.py:1111] Calling model_fn.
W0618 12:03:08.745123 47466803352448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:08.744590 47287908578176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.744455 47679582315392 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvjqawuiy
W0618 12:03:08.744483 47740116415360 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsf5rjpsc
I0618 12:03:08.745453 47679582315392 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvjqawuiy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5d8d277dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.745474 47740116415360 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsf5rjpsc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ba5447e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.745844 47679582315392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.745226 47250374587264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:08.745866 47740116415360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.746283 48009303647104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.746858 47824709489536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdzxkmk_i
W0618 12:03:08.746486 47466803352448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.746992 47802275791744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppkztqjkb
I0618 12:03:08.747853 47824709489536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdzxkmk_i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7f57686e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.747975 47802275791744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppkztqjkb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7a1e415e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.748253 47824709489536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.748381 47802275791744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.748978 46944367137664 estimator.py:1111] Calling model_fn.
W0618 12:03:08.749092 46944367137664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:08.750640 47679582315392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.750660 47740116415360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:08.750327 47250374587264 estimator.py:1111] Calling model_fn.
W0618 12:03:08.750436 47250374587264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:08.750449 46944367137664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.753351 47824709489536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.753459 47802275791744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.751803 47250374587264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.763627 47932303065984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.764513 47287908578176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.770068 47679582315392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.770575 47740116415360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.775994 47824709489536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880988.705707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.706620 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.707491 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.775962 47155018322816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880988.705713 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880988.706646 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880988.707505 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:08.775960 47002694824832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:08.776195 47802275791744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:08.775583 47954102088576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.775836 47425473307520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:08.777074 47155018322816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpuokuysbq
W0618 12:03:08.777101 47002694824832 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpm9r4h_3l
I0618 12:03:08.778111 47155018322816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpuokuysbq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae36ab42e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.778137 47002694824832 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpm9r4h_3l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abff3840dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:08.778534 47155018322816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:08.778556 47002694824832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:08.779902 47954102088576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:08.780181 47425473307520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:08.783648 47002694824832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:08.783707 47155018322816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:08.784947 47954102088576 estimator.py:1111] Calling model_fn.
W0618 12:03:08.785062 47954102088576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:08.785295 47425473307520 estimator.py:1111] Calling model_fn.
W0618 12:03:08.785404 47425473307520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:08.786421 47954102088576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:08.786779 47425473307520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init[2019-06-18 12:03:47] moving /lfs/lfs12/gma_akey/results/epb015/models/000005-000003.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000005-000004.meta
[2019-06-18 12:03:47] moving /lfs/lfs12/gma_akey/results/epb015/models/000005-000003.index --> /lfs/lfs12/gma_akey/results/epb015/models/000005-000004.index
[2019-06-18 12:03:47] moving /lfs/lfs12/gma_akey/results/epb015/models/000005-000003.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000005-000004.data-00000-of-00001
[2019-06-18 12:03:47] moving /lfs/lfs12/gma_akey/results/epb015/models/000005-000003.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb
[2019-06-18 12:03:47] iteration time 4: 47.983 seconds
2019-06-18 12:03:48.165356: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881027.639481 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 12:03:51] minmax time: 3.219 seconds
2019-06-18 12:03:51.394373: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:51.399925: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:51.404591: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881031.414477 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 4}}
[2019-06-18 12:03:51] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:03:51] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=6 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=1023779837 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=2047559668 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=3071339499 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=4095119330 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=5118899161 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=6142678992 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=7166458823 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=8190238654 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=9214018485 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=10237798316 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=11261578147 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=12285357978 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=13309137809 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=14332917640 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=15356697471 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=16380477302 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=17404257133 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=18428036964 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=19451816795 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000005-000004 --seed=20475596626 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:04:03] eval finished: 12.007 seconds
[2019-06-18 12:04:03] Win rate 000005-000004 vs 000004-000003: 0.270
:::MLL 1560881043.486245 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 12:04:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=7 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=1023779838 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=2047559669 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=3071339500 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=4095119331 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=5118899162 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=6142678993 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=7166458824 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=8190238655 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=9214018486 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=10237798317 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=11261578148 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=12285357979 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=13309137810 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=14332917641 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=15356697472 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=16380477303 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=17404257134 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000006-000003 --seed=18428036965 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:04:33] selfplay finished: 30.211 seconds
[2019-06-18 12:04:33] selfplay mn: 30.229 seconds
[2019-06-18 12:04:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-7-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779838 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559669 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339500 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119331 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899162 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678993 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458824 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238655 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018486 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798317 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578148 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357979 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137810 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917641 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697472 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477303 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257134 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036965 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816796 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596627 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376458 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156289 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:04:35] train finished: 44.107 seconds
:::MLL 1560881036.605901 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.606796 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.607613 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.681023 47204399850368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.613161 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.613842 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.614462 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.681177 46921178772352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.682039 47204399850368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpah7i9sbq
W0618 12:03:56.682214 46921178772352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7qmdjzty
I0618 12:03:56.683173 47204399850368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpah7i9sbq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeeea127e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.683344 46921178772352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7qmdjzty', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacf8c7be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.683621 47204399850368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.683768 46921178772352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.688949 47204399850368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.689043 46921178772352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.708757 47204399850368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.708827 46921178772352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881036.679734 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.680482 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.681154 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.748028 47412993434496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.674596 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.675473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.676327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.748075 47978643096448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.707180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.707632 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.708027 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.748626 47058706314112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.707175 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.707635 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.708030 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.748577 47944346928000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.749117 47978643096448 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpabza96a2
W0618 12:03:56.749149 47412993434496 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsi5ws5jd
I0618 12:03:56.750188 47978643096448 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpabza96a2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba32e90ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.750201 47412993434496 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsi5ws5jd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1f7b37fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.750620 47978643096448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.750632 47412993434496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.749595 47058706314112 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdleencsm
W0618 12:03:56.749564 47944346928000 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnm1s_vdz
I0618 12:03:56.750531 47944346928000 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnm1s_vdz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b325ace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.750558 47058706314112 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdleencsm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2accfe0f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.750918 47944346928000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.750952 47058706314112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.755821 47978643096448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.755838 47412993434496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.755580 47058706314112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.755548 47944346928000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.761315 47204399850368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.761776 46921178772352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.765642 47204399850368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:56.766096 46921178772352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:56.770747 47204399850368 estimator.py:1111] Calling model_fn.
W0618 12:03:56.770856 47204399850368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:56.771165 46921178772352 estimator.py:1111] Calling model_fn.
W0618 12:03:56.771276 46921178772352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:56.772222 47204399850368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.772643 46921178772352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.775034 47944346928000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.775219 47058706314112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.777289 47978643096448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.777359 47412993434496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881036.718087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.718887 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.719659 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.795456 47018665153408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.720553 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.721316 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.722036 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.795799 47933752095616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.796552 47018665153408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpn6s0a8lx
I0618 12:03:56.797663 47018665153408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpn6s0a8lx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3ab6bee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.796923 47933752095616 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98bada8d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.798108 47018665153408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.798177 47933752095616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881036.757225 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.757645 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.758010 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.800167 47330034779008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.757616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.758035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.758353 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.800226 46958366835584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.801156 47330034779008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqa9tmqel
W0618 12:03:56.801193 46958366835584 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyf1qlc_h
I0618 12:03:56.802172 47330034779008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqa9tmqel', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0c2a7f5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.802210 46958366835584 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyf1qlc_h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5a15c9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.802597 47330034779008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.802627 46958366835584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.803378 47018665153408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.803577 47933752095616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.807328 47330034779008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.807428 46958366835584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881036.735106 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.735988 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.736836 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.811025 47992281297792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.744829 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.745580 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.746336 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.811117 47017094431616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.812091 47992281297792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9vlicr80
W0618 12:03:56.812136 47017094431616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6bf1kunq
I0618 12:03:56.813158 47992281297792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9vlicr80', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba65b773e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.813214 47017094431616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6bf1kunq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac34dcc9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.813588 47992281297792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.813632 47017094431616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.818968 47017094431616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.818989 47992281297792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881036.771534 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.772016 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.772416 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.822070 47107815773056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.775663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.776069 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.776418 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.822083 47492405244800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.822308 47944346928000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.822651 47058706314112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.823228 47107815773056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbuz63dq9
W0618 12:03:56.823203 47492405244800 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpwkbpzxti
I0618 12:03:56.824169 47492405244800 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpwkbpzxti', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b31f887fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.824203 47107815773056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbuz63dq9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad86d366e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.824559 47492405244800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.824589 47107815773056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.825113 47018665153408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.825296 47933752095616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.826962 47330034779008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.827123 46958366835584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881036.752167 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.753125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.754038 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.828269 47183072600960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.826604 47944346928000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881036.758023 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.758772 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.759492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.828434 47830155887488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.826962 47058706314112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:56.827769 47978643096448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.828078 47412993434496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.829245 47183072600960 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6o_f5d6x
W0618 12:03:56.829420 47830155887488 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0ik3wjlj
I0618 12:03:56.830225 47183072600960 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6o_f5d6x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9f2de7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.830378 47830155887488 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0ik3wjlj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b809c09ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.830621 47183072600960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.830772 47830155887488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.829307 47107815773056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.829293 47492405244800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.832081 47978643096448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:56.832390 47412993434496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:56.831662 47944346928000 estimator.py:1111] Calling model_fn.
W0618 12:03:56.831770 47944346928000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:56.832049 47058706314112 estimator.py:1111] Calling model_fn.
W0618 12:03:56.832164 47058706314112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:56.833135 47944346928000 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.833532 47058706314112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.835471 47183072600960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.835525 47830155887488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:56.837153 47978643096448 estimator.py:1111] Calling model_fn.
W0618 12:03:56.837263 47978643096448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:56.837475 47412993434496 estimator.py:1111] Calling model_fn.
W0618 12:03:56.837584 47412993434496 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:56.838635 47978643096448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.838955 47412993434496 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:56.841427 47992281297792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.841542 47017094431616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881036.807977 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.808366 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.808738 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.847518 47577480844160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.808902 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.809362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.809682 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.847597 47266513838976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.848498 47577480844160 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpk5czo2x8
W0618 12:03:56.848560 47266513838976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppq3b3aw2
W0618 12:03:56.848828 47492405244800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.848916 47107815773056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:56.849503 47577480844160 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpk5czo2x8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45c76eae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.849521 47266513838976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppq3b3aw2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd605abdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.849895 47577480844160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:56.849909 47266513838976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.854585 47577480844160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.854597 47266513838976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.857589 47183072600960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.857998 47830155887488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881036.829281 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.829680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.830003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.872652 47434616689536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881036.828813 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881036.829211 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881036.829561 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:56.872759 47281362486144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:56.873611 47434616689536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpem4zxsw_
W0618 12:03:56.873713 47281362486144 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpow30ycpt
I0618 12:03:56.874645 47434616689536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpem4zxsw_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2484109e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.874749 47281362486144 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpow30ycpt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b00d5670e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:56.875080 47434616689536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.873266 47933752095616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:03:56.875177 47281362486144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:56.873352 47018665153408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.874206 47577480844160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.874372 47330034779008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.874221 47266513838976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:56.874633 46958366835584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:56.877561 47933752095616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:56.877695 47018665153408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:56.879761 47434616689536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:56.879851 47281362486144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/tra[2019-06-18 12:04:37] divide_golden_chunk finished: 3.287 seconds
[2019-06-18 12:04:37] generate golden chunk: 3.302 seconds
[2019-06-18 12:04:37] iteration time 5: 49.380 seconds
2019-06-18 12:04:37.588464: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881077.019219 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 12:04:40] minmax time: 3.207 seconds
2019-06-18 12:04:40.805554: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:04:40.810852: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:04:40.815454: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881080.827395 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 5}}
[2019-06-18 12:04:40] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:04:40] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=7 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=1023779838 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=2047559669 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=3071339500 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=4095119331 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=5118899162 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=6142678993 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=7166458824 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=8190238655 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=9214018486 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=10237798317 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=11261578148 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=12285357979 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=13309137810 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=14332917641 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=15356697472 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=16380477303 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=17404257134 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=18428036965 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=19451816796 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000006-000004 --seed=20475596627 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:04:52] eval finished: 11.438 seconds
[2019-06-18 12:04:52] Win rate 000006-000004 vs 000004-000003: 0.390
:::MLL 1560881092.324994 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 12:04:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=8 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=1023779839 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=2047559670 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=3071339501 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=4095119332 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=5118899163 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=6142678994 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=7166458825 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=8190238656 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=9214018487 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=10237798318 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=11261578149 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=12285357980 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=13309137811 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=14332917642 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=15356697473 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=16380477304 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=17404257135 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000007-000003 --seed=18428036966 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:05:22] selfplay finished: 30.054 seconds
[2019-06-18 12:05:22] selfplay mn: 30.071 seconds
[2019-06-18 12:05:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-8-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779839 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559670 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339501 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119332 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899163 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678994 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458825 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238656 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018487 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798318 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578149 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357980 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137811 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917642 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697473 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477304 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257135 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036966 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816797 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596628 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376459 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156290 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:05:24] train finished: 43.616 seconds
:::MLL 1560881086.035282 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.036152 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.036946 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.117370 46980089316224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.043537 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.044277 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.044948 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.117435 47160555312000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.118453 46980089316224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_b1rqzvm
W0618 12:04:46.118501 47160555312000 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc6wykdyf
I0618 12:04:46.119500 46980089316224 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_b1rqzvm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abab01f5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.119561 47160555312000 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc6wykdyf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae4b4bbee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.119893 46980089316224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.119951 47160555312000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.125060 47160555312000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.125094 46980089316224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.147667 46980089316224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.147706 47160555312000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881086.118819 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.119264 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.119687 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.163602 47310550188928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.164708 47310550188928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptexx9h66
I0618 12:04:46.165801 47310550188928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptexx9h66', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b07a1200e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.166219 47310550188928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881086.123505 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.123949 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.124347 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.166753 47546963821440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.167766 47546963821440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpb1xrx10c
I0618 12:04:46.168793 47546963821440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpb1xrx10c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3eac79de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.169215 47546963821440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.171193 47310550188928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.174002 47546963821440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.190956 47310550188928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.193653 47546963821440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.197973 46980089316224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.198549 47160555312000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.202282 46980089316224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:46.202886 47160555312000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:46.207342 46980089316224 estimator.py:1111] Calling model_fn.
W0618 12:04:46.207450 46980089316224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:46.207975 47160555312000 estimator.py:1111] Calling model_fn.
W0618 12:04:46.208088 47160555312000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:46.208815 46980089316224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:46.209446 47160555312000 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881086.147082 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.147802 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.148486 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.218949 47146803225472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.142474 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.143395 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.144227 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.219003 47354987484032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.220051 47146803225472 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_1bj6x1r
W0618 12:04:46.220087 47354987484032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyrugh1q8
I0618 12:04:46.221157 47146803225472 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_1bj6x1r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1810bbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.221158 47354987484032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyrugh1q8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11f9cb6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.221601 47146803225472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.221603 47354987484032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.226934 47354987484032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.226956 47146803225472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881086.161490 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.162284 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.163153 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.238185 46926888690560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.163275 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.164043 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.164741 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.238274 47073840141184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.238714 47310550188928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.239264 46926888690560 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpq541py5q
W0618 12:04:46.239331 47073840141184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpiwjq3uqf
I0618 12:04:46.240290 46926888690560 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpq541py5q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae4d1e3dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.240363 47073840141184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpiwjq3uqf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0841b6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.240704 46926888690560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.240765 47073840141184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.241110 47546963821440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.243036 47310550188928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:46.245921 46926888690560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.245979 47073840141184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.245443 47546963821440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:46.248102 47310550188928 estimator.py:1111] Calling model_fn.
W0618 12:04:46.248214 47310550188928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:46.249377 47354987484032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.249543 47146803225472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.249588 47310550188928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:04:46.250510 47546963821440 estimator.py:1111] Calling model_fn.
W0618 12:04:46.250616 47546963821440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881086.172503 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.173409 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.174249 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.250695 47292419117952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.172737 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.173602 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.174423 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.250900 47199775323008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.251969 47546963821440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:46.251802 47292419117952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpo7yxq_09
I0618 12:04:46.252032 47199775323008 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aedd66dccc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.252892 47292419117952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpo7yxq_09', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b03686dedd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.253295 47199775323008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.253342 47292419117952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881086.210491 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.210977 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.211299 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.254965 47274849887104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.209995 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.210454 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.210857 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.255724 47679485633408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.256001 47274849887104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbz3owky1
I0618 12:04:46.257079 47274849887104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbz3owky1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff51389e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.257517 47274849887104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.256856 47679485633408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkwqeas41
I0618 12:04:46.258089 47679485633408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkwqeas41', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5d87643e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.258526 47679485633408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.258582 47292419117952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.258641 47199775323008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.262566 47274849887104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.263673 47679485633408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.268418 46926888690560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.268553 47073840141184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881086.228918 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.229336 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.229687 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.280442 47016368288640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.226980 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.227386 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.227733 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.280706 47700649997184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.280903 47292419117952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.281141 47199775323008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881086.238843 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.239343 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.239661 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.282191 47883656024960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.281450 47016368288640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpij4k1b8d
W0618 12:04:46.281672 47700649997184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpm2wz5lw4
I0618 12:04:46.282432 47016368288640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpij4k1b8d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac322848e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.282639 47700649997184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpm2wz5lw4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6274e2ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.282835 47016368288640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.283027 47700649997184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.283662 47274849887104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.283289 47883656024960 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpv4ta0u1s
I0618 12:04:46.284363 47883656024960 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpv4ta0u1s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d10e52e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.284793 47883656024960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881086.241822 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.242318 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.242760 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.284653 46966206776192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.286068 47679485633408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.285681 46966206776192 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl_5vwzhq
I0618 12:04:46.286712 46966206776192 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl_5vwzhq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab774a89e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.287164 46966206776192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.287462 47016368288640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.287575 47700649997184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.289829 47883656024960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.291940 46966206776192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881086.218348 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.219252 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.220138 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.295189 47308285830016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881086.219041 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.219954 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.220732 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.295513 47128477791104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.296251 47308285830016 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1zd5mkbm
I0618 12:04:46.297301 47308285830016 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1zd5mkbm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b071a28be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:46.296541 47128477791104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmkw3tphf
I0618 12:04:46.297648 47128477791104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmkw3tphf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2add3cc3ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.297757 47308285830016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.298081 47128477791104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.298054 47354987484032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.298338 47146803225472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.303086 47308285830016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.303630 47128477791104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.302367 47354987484032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:46.302674 47146803225472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:46.307406 47354987484032 estimator.py:1111] Calling model_fn.
W0618 12:04:46.307513 47354987484032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:46.307013 47700649997184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:46.307759 47146803225472 estimator.py:1111] Calling model_fn.
W0618 12:04:46.307209 47016368288640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.307868 47146803225472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:46.308863 47354987484032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:46.309215 47146803225472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881086.228301 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.229060 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.229770 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.310901 47279432774528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.309669 47883656024960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881086.220888 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881086.221808 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881086.222692 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:46.311031 47654676013952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:46.311992 47279432774528 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgdp9m2m0
W0618 12:04:46.312056 47654676013952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpd_vkx5z4
I0618 12:04:46.313034 47279432774528 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgdp9m2m0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b006261fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:46.313091 47654676013952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpd_vkx5z4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b57c09f7dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:46.312005 46966206776192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:46.313723 47279432774528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:46.313771 47654676013952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:46.318724 47279432774528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.318732 47654676013952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:46.319520 46926888690560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.319601 47073840141184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:46.323834 46926888690560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:46.323952 47073840141184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:46.325732 47308285830016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:46.327245 47128477791104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:46.328960 46926888690560 estimator.py:1111] Calling model_fn.
W0618 12:04:46.329071 46926888690560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:46.329099 47073840141184 estimator.py:1111] Calling model_fn.
W0618 12:04:46.329208 47073840141184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:46.330430 46926888690560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:46.330570 47073840141184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init[2019-06-18 12:05:25] divide_golden_chunk finished: 3.332 seconds
[2019-06-18 12:05:25] generate golden chunk: 3.346 seconds
[2019-06-18 12:05:25] iteration time 6: 48.725 seconds
2019-06-18 12:05:26.351086: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881125.744359 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 12:05:29] minmax time: 3.233 seconds
2019-06-18 12:05:29.594282: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:05:29.599663: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:05:29.604297: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881129.616030 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 6}}
[2019-06-18 12:05:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir 
[2019-06-18 12:05:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=8 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=1023779839 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=2047559670 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=3071339501 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=4095119332 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=5118899163 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=6142678994 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=7166458825 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=8190238656 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=9214018487 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=10237798318 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=11261578149 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=12285357980 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=13309137811 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=14332917642 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=15356697473 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=16380477304 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=17404257135 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=18428036966 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=19451816797 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000007-000004 --seed=20475596628 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:05:41] eval finished: 11.624 seconds
[2019-06-18 12:05:41] Win rate 000007-000004 vs 000004-000003: 0.650
:::MLL 1560881141.301715 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 12:05:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=9 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=1023779840 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=2047559671 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=3071339502 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=4095119333 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=5118899164 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=6142678995 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=7166458826 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=8190238657 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=9214018488 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=10237798319 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=11261578150 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=12285357981 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=13309137812 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=14332917643 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=15356697474 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=16380477305 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=17404257136 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000008-000003 --seed=18428036967 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/
[2019-06-18 12:06:10] selfplay finished: 28.846 seconds
[2019-06-18 12:06:10] selfplay mn: 28.864 seconds
[2019-06-18 12:06:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-9-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779840 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559671 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339502 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119333 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899164 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678995 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458826 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238657 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018488 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798319 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578150 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357981 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137812 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917643 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697474 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477305 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257136 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036967 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816798 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596629 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376460 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156291 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_
[2019-06-18 12:06:13] train finished: 43.741 seconds
:::MLL 1560881134.889488 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.890388 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.891207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:34.975252 47036011090816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.903320 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.904053 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.904729 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:34.975391 47675818529664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.898067 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.898964 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.899774 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:34.976433 47740538418048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.906647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.907386 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.908034 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:34.976477 47878380266368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:34.976273 47036011090816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7dv0bbzf
W0618 12:05:34.976392 47675818529664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmvblbnnl
I0618 12:05:34.977319 47036011090816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7dv0bbzf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7b551ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:34.977431 47675818529664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmvblbnnl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5cacd0ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:34.977518 47740538418048 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6hx756il
W0618 12:05:34.977545 47878380266368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbprf30z6
I0618 12:05:34.977743 47036011090816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:34.977847 47675818529664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:34.978615 47740538418048 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6hx756il', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6bbe6bce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:34.978619 47878380266368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbprf30z6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8bd66f7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:34.979066 47878380266368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:34.979073 47740538418048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:34.982707 47036011090816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:34.982886 47675818529664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:34.984370 47878380266368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:34.984372 47740538418048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.004599 47036011090816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.005396 47675818529664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.006839 47878380266368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.006910 47740538418048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881134.967596 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.968048 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.968394 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.014573 47089304097664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.969066 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.969439 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.969765 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.014754 47072575808384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.015566 47089304097664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjhb55guh
W0618 12:05:35.015714 47072575808384 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcle4nimx
I0618 12:05:35.016608 47089304097664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjhb55guh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad41dd4ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.016762 47072575808384 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcle4nimx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad038bf3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.017037 47089304097664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.017168 47072575808384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881134.940056 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.940894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.941601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.020494 47133259367296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.939128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.940002 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.940814 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.020853 47900280324992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.021664 47089304097664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.021783 47072575808384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.021635 47133259367296 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpefhud_6k
I0618 12:05:35.022771 47133259367296 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpefhud_6k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade59c4cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.022012 47900280324992 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90efc7dd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.023248 47133259367296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.023324 47900280324992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.028565 47133259367296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.028854 47900280324992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881134.983159 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.983529 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.983848 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.029273 46962981811072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.950135 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.951066 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.951940 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.030611 47597409235840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.984786 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.985162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.985482 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.030110 46959545348992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881134.959343 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881134.960080 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881134.960806 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.030716 47833140138880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.030306 46962981811072 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_uf2_3pi
I0618 12:05:35.031333 46962981811072 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_uf2_3pi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab6b46f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.031729 46962981811072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.031127 46959545348992 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_2_xuvmr
W0618 12:05:35.031729 47597409235840 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2z_gtjos
W0618 12:05:35.031802 47833140138880 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx4lgtzle
I0618 12:05:35.032104 46959545348992 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_2_xuvmr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5e79b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.032834 47597409235840 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2z_gtjos', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4a6b41cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.032917 47833140138880 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx4lgtzle', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b814de9cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.032506 46959545348992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.033291 47597409235840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.033374 47833140138880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.036525 46962981811072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.037157 46959545348992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.038624 47597409235840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.038655 47833140138880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.041025 47089304097664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.041432 47072575808384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.050962 47133259367296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.051887 47900280324992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881135.006253 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.006673 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.007044 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.052897 47796968813440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881135.001748 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.002220 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.002628 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.052885 46931634045824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.053860 47796968813440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbhcyjgcr
W0618 12:05:35.053903 46931634045824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzmifo3vp
I0618 12:05:35.054915 47796968813440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbhcyjgcr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78e1ef3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.054934 46931634045824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzmifo3vp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaf67f68e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.055334 47796968813440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.055348 46931634045824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.056253 47740538418048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.056332 47878380266368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.056252 46962981811072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.056738 46959545348992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.056753 47036011090816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.058134 47675818529664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.060099 46931634045824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.060127 47796968813440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.060681 47740538418048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.060812 47878380266368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.061133 47597409235840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.061505 47833140138880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.061326 47036011090816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.062766 47675818529664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:35.065839 47740538418048 estimator.py:1111] Calling model_fn.
W0618 12:05:35.065961 47740538418048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:35.066007 47878380266368 estimator.py:1111] Calling model_fn.
W0618 12:05:35.066122 47878380266368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:35.067403 47740538418048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:35.066730 47036011090816 estimator.py:1111] Calling model_fn.
W0618 12:05:35.066843 47036011090816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:35.067568 47878380266368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:35.068193 47675818529664 estimator.py:1111] Calling model_fn.
W0618 12:05:35.068268 47036011090816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:35.068310 47675818529664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:35.069776 47675818529664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881135.026862 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.027296 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.027614 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.073515 47735946711936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881135.029667 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.030043 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.030416 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.074948 47372449039232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.074550 47735946711936 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx9bi683c
I0618 12:05:35.075596 47735946711936 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx9bi683c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6aacbbee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.075984 47735946711936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.075992 47372449039232 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpaui7tfyw
I0618 12:05:35.076968 47372449039232 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpaui7tfyw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b160a959e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.077367 47372449039232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.079583 46931634045824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.079709 47796968813440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.080683 47735946711936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.082071 47372449039232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881135.019801 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.020563 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.021277 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.088989 47006646625152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881135.011281 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.012216 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.013128 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.089123 47387771765632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.088670 47089304097664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.088950 47072575808384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.090077 47006646625152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpocbf0f4e
W0618 12:05:35.090212 47387771765632 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxhz8x_yx
I0618 12:05:35.091242 47006646625152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpocbf0f4e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0df0fbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.091356 47387771765632 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxhz8x_yx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b199be3de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.091701 47006646625152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.091825 47387771765632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:35.092984 47089304097664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.093273 47072575808384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881135.025049 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.025830 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.026522 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.096418 47761789244288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881135.015256 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881135.016206 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881135.017102 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:35.096516 47637853766528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:35.097012 47006646625152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.097353 47387771765632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.097425 47761789244288 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph2t_bm7e
W0618 12:05:35.097506 47637853766528 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa2a9plx6
I0618 12:05:35.098443 47761789244288 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph2t_bm7e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70b111ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.098541 47637853766528 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa2a9plx6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53d5f05e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:35.098851 47761789244288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.098958 47637853766528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:35.098019 47089304097664 estimator.py:1111] Calling model_fn.
W0618 12:05:35.098128 47089304097664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:35.098367 47072575808384 estimator.py:1111] Calling model_fn.
W0618 12:05:35.098475 47072575808384 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:35.099483 47089304097664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:35.098877 47133259367296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.099844 47072575808384 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:35.100110 47735946711936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.099652 47900280324992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.101842 47372449039232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:35.103903 47761789244288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.103904 47637853766528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:35.103174 47133259367296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.103984 47900280324992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:35.104256 46962981811072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:35.104421 46959545348992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:35.108239 47133259367296 estimator.py:1111] Calling model_fn.
W0618 12:05:35.108346 47133259367296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:35.108613 46962981811072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from te[2019-06-18 12:06:13] divide_golden_chunk finished: 3.429 seconds
[2019-06-18 12:06:13] generate golden chunk: 3.444 seconds
[2019-06-18 12:06:13] moving /lfs/lfs12/gma_akey/results/epb015/models/000008-000004.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000008-000005.meta
[2019-06-18 12:06:13] moving /lfs/lfs12/gma_akey/results/epb015/models/000008-000004.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000008-000005.data-00000-of-00001
[2019-06-18 12:06:13] moving /lfs/lfs12/gma_akey/results/epb015/models/000008-000004.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb
[2019-06-18 12:06:13] moving /lfs/lfs12/gma_akey/results/epb015/models/000008-000004.index --> /lfs/lfs12/gma_akey/results/epb015/models/000008-000005.index
[2019-06-18 12:06:13] iteration time 7: 47.914 seconds
2019-06-18 12:06:14.302553: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881173.658525 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 12:06:17] minmax time: 3.210 seconds
2019-06-18 12:06:17.523335: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:06:17.528851: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:06:17.533616: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881177.543976 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 7}}
[2019-06-18 12:06:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:06:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=9 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=1023779840 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=2047559671 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=3071339502 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=4095119333 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=5118899164 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=6142678995 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=7166458826 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=8190238657 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=9214018488 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=10237798319 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=11261578150 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=12285357981 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=13309137812 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=14332917643 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=15356697474 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=16380477305 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=17404257136 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=18428036967 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=19451816798 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000008-000005 --seed=20475596629 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:06:28] eval finished: 10.734 seconds
[2019-06-18 12:06:28] Win rate 000008-000005 vs 000007-000004: 0.770
:::MLL 1560881188.335320 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 12:06:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=10 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=1023779841 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=2047559672 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=3071339503 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=4095119334 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=5118899165 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=6142678996 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=7166458827 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=8190238658 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=9214018489 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=10237798320 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=11261578151 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=12285357982 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=13309137813 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=14332917644 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=15356697475 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=16380477306 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=17404257137 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000009-000004 --seed=18428036968 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:06:57] selfplay finished: 29.065 seconds
[2019-06-18 12:06:57] selfplay mn: 29.085 seconds
[2019-06-18 12:06:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-10-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779841 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559672 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339503 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119334 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899165 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678996 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458827 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238658 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018489 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798320 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578151 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357982 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137813 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917644 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697475 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477306 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257137 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036968 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816799 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596630 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376461 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156292 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:07:00] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 12:07:00] generate golden chunk: 3.317 seconds
[2019-06-18 12:07:01] train finished: 44.035 seconds
:::MLL 1560881182.768393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.769127 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.769787 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.842214 47136841565056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.763966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.764845 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.765679 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.842443 47543110284160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.843228 47136841565056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptramwh0i
W0618 12:06:22.843416 47543110284160 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1_6jgi3l
I0618 12:06:22.844221 47136841565056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptramwh0i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf2f48de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.844415 47543110284160 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1_6jgi3l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3dc6c97e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.844623 47136841565056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.844814 47543110284160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.849743 47136841565056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.849965 47543110284160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881182.775581 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.776254 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.776896 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.851526 47519611323264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.769710 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.770602 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.771408 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.851610 47205503366016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.852661 47519611323264 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpm4va9wpb
W0618 12:06:22.852717 47205503366016 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgs_jf3c9
I0618 12:06:22.853761 47519611323264 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpm4va9wpb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b384e23ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.853809 47205503366016 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgs_jf3c9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aef2bd8ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.854202 47519611323264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.854252 47205503366016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.859532 47519611323264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.859550 47205503366016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.871644 47136841565056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.872128 47543110284160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.882047 47519611323264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.882264 47205503366016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881182.847809 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.848216 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.848569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.897673 47134128395136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.847737 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.848138 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.848493 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.897704 47571698881408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.898741 47571698881408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgf80h5ug
W0618 12:06:22.898768 47134128395136 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp23dxnvxp
I0618 12:06:22.899740 47571698881408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgf80h5ug', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b446eccee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.899743 47134128395136 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp23dxnvxp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade8d911e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.900142 47571698881408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.900146 47134128395136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881182.851290 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.851672 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.852027 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.900926 47176449758080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.852050 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.852436 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.852756 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.901315 47123184087936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.901927 47176449758080 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpucuc81l2
I0618 12:06:22.902893 47176449758080 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpucuc81l2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8681dde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:22.902296 47123184087936 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8gbba_x2
I0618 12:06:22.903288 47176449758080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.903285 47123184087936 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8gbba_x2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adc013c3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.903680 47123184087936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.904877 47571698881408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.904910 47134128395136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.908127 47176449758080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.908460 47123184087936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.924274 47571698881408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.924365 47134128395136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.924525 47136841565056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.924938 47543110284160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.927834 47176449758080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.928075 47123184087936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.929105 47136841565056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.929508 47543110284160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881182.857773 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.858597 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.859290 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.933267 47861417984896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.856567 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.857379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.858203 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.933383 47537503712128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.934075 47519611323264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.934036 47205503366016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.934365 47861417984896 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc3te79xe
I0618 12:06:22.934560 47136841565056 estimator.py:1111] Calling model_fn.
W0618 12:06:22.934440 47537503712128 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpaas65wwm
W0618 12:06:22.934677 47136841565056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:22.935462 47861417984896 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc3te79xe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b87e3678e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.934991 47543110284160 estimator.py:1111] Calling model_fn.
I0618 12:06:22.935613 47537503712128 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpaas65wwm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c789c0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:22.935115 47543110284160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:22.935904 47861417984896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.936087 47537503712128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.936147 47136841565056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.936595 47543110284160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.938688 47519611323264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.938644 47205503366016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.941190 47861417984896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.941524 47537503712128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881182.866744 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.867592 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.868388 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.943731 47262639178624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.867389 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.868283 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.869003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.943728 47176822047616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 12:06:22.944205 47205503366016 estimator.py:1111] Calling model_fn.
I0618 12:06:22.944257 47519611323264 estimator.py:1111] Calling model_fn.
W0618 12:06:22.944322 47205503366016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:22.944370 47519611323264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:22.944842 47262639178624 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvyv3qbbn
W0618 12:06:22.944813 47176822047616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9c9pknzp
I0618 12:06:22.945936 47262639178624 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvyv3qbbn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc79680e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.945936 47176822047616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9c9pknzp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae87e4e8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.946377 47262639178624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.946389 47176822047616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.945764 47205503366016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.945844 47519611323264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.951663 47262639178624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.951685 47176822047616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881182.873101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.874000 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.874747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.957420 47073578976128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.872590 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.873436 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.874266 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.957432 46962113901440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.958556 46962113901440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9vvb9fsz
I0618 12:06:22.958583 47073578976128 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0748a5d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.959676 46962113901440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9vvb9fsz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab680b44e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.959852 47073578976128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.960126 46962113901440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.963707 47861417984896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.965188 47537503712128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.965200 47073578976128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.965310 46962113901440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.971856 47571698881408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.972250 47134128395136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.974331 47262639178624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.974348 47176822047616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.975300 47176449758080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.976157 47571698881408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.975569 47123184087936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:22.976610 47134128395136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.979613 47176449758080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:22.979895 47123184087936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:06:22.981225 47571698881408 estimator.py:1111] Calling model_fn.
W0618 12:06:22.981334 47571698881408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881182.931782 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.932159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.932484 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.981267 47394166637440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.933366 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.933739 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.934062 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.981280 47755731202944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 12:06:22.981700 47134128395136 estimator.py:1111] Calling model_fn.
W0618 12:06:22.981813 47134128395136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:22.982690 47571698881408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.983178 47134128395136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.982391 47394166637440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdl4qr3bf
W0618 12:06:22.982420 47755731202944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx2pc1txq
I0618 12:06:22.983448 47394166637440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdl4qr3bf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b190dde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.983450 47755731202944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx2pc1txq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f47fb3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.983846 47394166637440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.983844 47755731202944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881182.937314 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.937712 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.938040 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.985473 47405086864256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881182.937048 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.937450 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.937798 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.985567 47383666754432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 12:06:22.984688 47176449758080 estimator.py:1111] Calling model_fn.
W0618 12:06:22.984798 47176449758080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:22.984966 47123184087936 estimator.py:1111] Calling model_fn.
W0618 12:06:22.985081 47123184087936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:22.986520 47405086864256 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpd5la5u0u
W0618 12:06:22.986597 47383666754432 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsjakkamt
I0618 12:06:22.987571 47405086864256 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpd5la5u0u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1da3f34e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:22.987643 47383666754432 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsjakkamt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18a7365e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881182.924358 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.924761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.925112 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.986032 46932750701440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 12:06:22.987963 47405086864256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881182.921295 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881182.921777 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881182.922147 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:22.986096 47776060601216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:22.986167 47176449758080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:06:22.988038 47383666754432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.986459 47123184087936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:22.987667 46962113901440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.987696 47073578976128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:22.987044 46932750701440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsu36zce5
W0618 12:06:22.987075 47776060601216 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4r3hlgmx
I0618 12:06:22.988052 46932750701440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsu36zce5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafaa855e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:22.988484 47394166637440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:06:22.988074 47776060601216 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4r3hlgmx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7403b53e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:22.988494 47755731202944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:06:22.988445 46932750701440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:22.988464 47776060601216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:22.992726 47405086864256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.992813 47383666754432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.993253 46932750701440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:22.993213 47776060601216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:23.008038 47394166637440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:23.008084 47755731202944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:23.012130 47405086864256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:23.012210 47383666754432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions [2019-06-18 12:07:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000009-000005.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb
[2019-06-18 12:07:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000009-000005.index --> /lfs/lfs12/gma_akey/results/epb015/models/000009-000006.index
[2019-06-18 12:07:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000009-000005.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000009-000006.meta
[2019-06-18 12:07:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000009-000005.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000009-000006.data-00000-of-00001
[2019-06-18 12:07:01] iteration time 8: 47.987 seconds
2019-06-18 12:07:02.318197: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881221.646081 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 12:07:05] minmax time: 3.259 seconds
2019-06-18 12:07:05.587151: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:05.592693: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:05.597406: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881225.608207 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 8}}
[2019-06-18 12:07:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:07:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=10 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=1023779841 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=2047559672 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=3071339503 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=4095119334 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=5118899165 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=6142678996 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=7166458827 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=8190238658 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=9214018489 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=10237798320 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=11261578151 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=12285357982 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=13309137813 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=14332917644 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=15356697475 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=16380477306 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=17404257137 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=18428036968 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=19451816799 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000009-000006 --seed=20475596630 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:07:16] eval finished: 11.079 seconds
[2019-06-18 12:07:16] Win rate 000009-000006 vs 000008-000005: 0.460
:::MLL 1560881236.746624 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 12:07:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=11 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=1023779842 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=2047559673 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=3071339504 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=4095119335 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=5118899166 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=6142678997 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=7166458828 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=8190238659 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=9214018490 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=10237798321 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=11261578152 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=12285357983 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=13309137814 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=14332917645 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=15356697476 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=16380477307 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=17404257138 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000010-000005 --seed=18428036969 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:07:46] selfplay finished: 29.911 seconds
[2019-06-18 12:07:46] selfplay mn: 29.929 seconds
[2019-06-18 12:07:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-11-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779842 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559673 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339504 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119335 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899166 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678997 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458828 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238659 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018490 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798321 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578152 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357983 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137814 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917645 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697476 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477307 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257138 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036969 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816800 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596631 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376462 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156293 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:07:49] train finished: 43.796 seconds
:::MLL 1560881230.841331 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.842061 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.842734 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.918393 47881278079872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.838624 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.839328 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.839993 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.918523 47312171557760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.919466 47881278079872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpy7rhaxir
W0618 12:07:10.919562 47312171557760 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkj8wxzq9
I0618 12:07:10.920538 47881278079872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpy7rhaxir', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c83289e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.920622 47312171557760 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkj8wxzq9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0801c43e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.920968 47881278079872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.921046 47312171557760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.926251 47881278079872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.841836 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.842738 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.843461 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.926846 47398178759552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.926281 47312171557760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.845826 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.846514 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.847242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.926910 47668404532096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.927951 47398178759552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5h060cvd
W0618 12:07:10.927998 47668404532096 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpy4yui464
I0618 12:07:10.929064 47398178759552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5h060cvd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1c0831fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.929093 47668404532096 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpy4yui464', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5af2e80e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.929512 47398178759552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.929534 47668404532096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.934861 47668404532096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.934864 47398178759552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.947326 47312171557760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.947425 47881278079872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.957355 47398178759552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.957520 47668404532096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.916603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.917015 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.917335 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.963768 47608691512192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.964832 47608691512192 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpz5gq6cvy
I0618 12:07:10.965870 47608691512192 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpz5gq6cvy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d0bbbae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.966287 47608691512192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881230.919119 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.919501 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.919829 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.970437 47024747602816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.971070 47608691512192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.971537 47024747602816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpufmj3hds
I0618 12:07:10.972594 47024747602816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpufmj3hds', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac515f6ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.972989 47024747602816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.977477 47024747602816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.931235 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.931613 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.931939 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.985581 47179858465664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.928729 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.929145 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.929492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.985786 47631394050944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.986588 47179858465664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmps7zicg43
W0618 12:07:10.986754 47631394050944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptn47ygng
I0618 12:07:10.987576 47179858465664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmps7zicg43', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9334a9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.987722 47631394050944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptn47ygng', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5254e8de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.987967 47179858465664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.988117 47631394050944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.990769 47608691512192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.992621 47179858465664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.992826 47631394050944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.909546 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.910233 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.910940 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.994199 46956233053056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.907530 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.908313 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.909041 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.994577 47554988000128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.995282 46956233053056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7grn7ci0
W0618 12:07:10.996825 47024747602816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:10.996377 46956233053056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7grn7ci0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5222dae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.995691 47554988000128 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b408ac11d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.996823 46956233053056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.996952 47554988000128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881230.917583 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.918485 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.919324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.997420 47891909133184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.922578 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.923313 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.923968 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.997511 46962794886016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.998451 47891909133184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpok0_uza2
W0618 12:07:10.998524 46962794886016 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmjzbav_6
I0618 12:07:10.999513 47891909133184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpok0_uza2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8efcd19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.999558 46962794886016 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmjzbav_6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab6a94b3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.999931 47891909133184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.999953 46962794886016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:11.000085 47312171557760 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.000316 47881278079872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.002034 46956233053056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.002219 47554988000128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.004754 46962794886016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.004766 47891909133184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.004528 47312171557760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.004759 47881278079872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881230.923556 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.924514 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.925416 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.008942 47093518717824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.959471 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.960297 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.961038 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.009213 47935850001280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:11.009575 47668404532096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.009943 47093518717824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0ua0x878
W0618 12:07:11.009627 47398178759552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.010186 47935850001280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdreodige
I0618 12:07:11.011046 47093518717824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0ua0x878', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5190aae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.011287 47935850001280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdreodige', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9937e5fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.011493 47093518717824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:11.011712 47935850001280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:11.009907 47312171557760 estimator.py:1111] Calling model_fn.
W0618 12:07:11.010022 47312171557760 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:11.010174 47881278079872 estimator.py:1111] Calling model_fn.
W0618 12:07:11.010292 47881278079872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.011473 47312171557760 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.011744 47881278079872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.011956 47179858465664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.012476 47631394050944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.014209 47668404532096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.014275 47398178759552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.016714 47093518717824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.016769 47935850001280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:11.019649 47668404532096 estimator.py:1111] Calling model_fn.
I0618 12:07:11.019718 47398178759552 estimator.py:1111] Calling model_fn.
W0618 12:07:11.019762 47668404532096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.019832 47398178759552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.021214 47668404532096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.021287 47398178759552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.023990 46956233053056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.024447 47554988000128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.955362 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.955785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.956159 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.024473 47240929801088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.956890 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.957298 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.957657 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.024465 47472733848448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:11.026604 46962794886016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.025521 47240929801088 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4ijvpphb
W0618 12:07:11.026733 47891909133184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.025550 47472733848448 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfuzocw7s
I0618 12:07:11.026514 47240929801088 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4ijvpphb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af76b6d3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.026526 47472733848448 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfuzocw7s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2d64064e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.026911 47240929801088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:11.026921 47472733848448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:11.031689 47240929801088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.031686 47472733848448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.038104 47093518717824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.038385 47935850001280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.038215 47608691512192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.042524 47608691512192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.046817 47024747602816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:11.047555 47608691512192 estimator.py:1111] Calling model_fn.
W0618 12:07:11.047669 47608691512192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.049031 47608691512192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.051114 47024747602816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.051087 47472733848448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:11.051193 47240929801088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.999516 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.999970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881231.000303 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.054836 47047891317632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881231.003099 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881231.003703 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881231.004191 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.055083 47460507190144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 12:07:11.056154 47024747602816 estimator.py:1111] Calling model_fn.
W0618 12:07:11.056263 47024747602816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.055819 47047891317632 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2aewdagq
W0618 12:07:11.056063 47460507190144 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_p7nspo4
I0618 12:07:11.056818 47047891317632 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2aewdagq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca796fce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.057038 47460507190144 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_p7nspo4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2a8b425e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:11.057216 47047891317632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:11.057437 47460507190144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:11.057631 47024747602816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:11.059535 47179858465664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.059913 47631394050944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.061835 47047891317632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.062074 47460507190144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:11.063832 47179858465664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:11.064264 47631394050944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:11.068975 47179858465664 estimator.py:1111] Calling model_fn.
W0618 12:07:11.069101 47179858465664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:11.069519 47631394050944 estimator.py:1111] Calling model_fn.
W0618 12:07:11.069641 47631394050944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:11.070535 47179858465664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881230.996403 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.997260 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.998089 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.072524 47577657418624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.997304 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.998190 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.998926 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.072744 47216370451328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:11.071008 47631394050944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881231.023866 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881231.024248 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881231.024566 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:11.072703 47477607445376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:11.072040 46956233053056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.072249 47554988000128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:11.073536 47577657418624 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzmekb3yp
W0618 12:07:11.073729 47216370451328 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5d1f88au
I0618 12:07:11.074587 47577657418624 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzmekb3yp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45d1f4fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:11.073778 47477607445376 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpti8humbj
:::MLL 1560881231.026328 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881231.026792 opt_base_learning_rate: {"value": [0.32, 0.032, 0.003[2019-06-18 12:07:49] divide_golden_chunk finished: 3.303 seconds
[2019-06-18 12:07:49] generate golden chunk: 3.317 seconds
[2019-06-18 12:07:49] iteration time 9: 48.349 seconds
2019-06-18 12:07:50.694227: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881269.995200 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 12:07:53] minmax time: 3.220 seconds
2019-06-18 12:07:53.923968: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:53.929408: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:53.934100: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881273.946626 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 9}}
[2019-06-18 12:07:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:07:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=11 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=1023779842 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=2047559673 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=3071339504 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=4095119335 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=5118899166 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=6142678997 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=7166458828 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=8190238659 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=9214018490 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=10237798321 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=11261578152 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=12285357983 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=13309137814 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=14332917645 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=15356697476 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=16380477307 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=17404257138 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=18428036969 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=19451816800 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000010-000006 --seed=20475596631 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:06] eval finished: 12.494 seconds
[2019-06-18 12:08:06] Win rate 000010-000006 vs 000008-000005: 0.420
:::MLL 1560881286.500155 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 12:08:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=12 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=1023779843 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=2047559674 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=3071339505 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=4095119336 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=5118899167 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=6142678998 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=7166458829 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=8190238660 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=9214018491 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=10237798322 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=11261578153 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=12285357984 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=13309137815 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=14332917646 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=15356697477 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=16380477308 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=17404257139 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000011-000005 --seed=18428036970 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:08:36] selfplay finished: 30.132 seconds
[2019-06-18 12:08:36] selfplay mn: 30.150 seconds
[2019-06-18 12:08:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-12-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779843 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559674 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339505 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119336 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899167 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678998 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458829 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238660 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018491 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798322 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578153 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357984 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137815 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917646 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697477 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477308 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257139 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036970 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816801 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596632 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376463 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156294 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000011-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:08:37] train finished: 43.986 seconds
:::MLL 1560881279.192604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.193462 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.194188 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.274167 47981647713152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.191792 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.192610 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.193411 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.274186 47318224401280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.275192 47318224401280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8crotffy
W0618 12:07:59.275218 47981647713152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7uglis6k
I0618 12:07:59.276293 47318224401280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8crotffy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b096a8b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.276423 47981647713152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7uglis6k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba3e1a7ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.276746 47318224401280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.276896 47981647713152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.281982 47318224401280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.282411 47981647713152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881279.219663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.220388 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.221066 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.290518 46912996082560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.211288 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.212159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.212996 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.290643 47288982545280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.291549 46912996082560 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpynxtxn07
W0618 12:07:59.291800 47288982545280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpiuxhg3vx
I0618 12:07:59.292587 46912996082560 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpynxtxn07', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab110dce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.292814 47288982545280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpiuxhg3vx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b029b97fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.292988 46912996082560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.293206 47288982545280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.297833 46912996082560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.297899 47288982545280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.304369 47318224401280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.305280 47981647713152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.319710 46912996082560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.319991 47288982545280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881279.272799 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.273220 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.273541 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.327148 47564809466752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.272394 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.272812 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.273180 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.327222 47560357057408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.328154 47564809466752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5f1hlt9_
W0618 12:07:59.328202 47560357057408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw7zz_kd2
I0618 12:07:59.329140 47564809466752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5f1hlt9_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42d428ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.329159 47560357057408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw7zz_kd2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41cac66e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.329540 47564809466752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.329550 47560357057408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.334214 47564809466752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.334233 47560357057408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881279.292297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.292696 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.293022 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.352876 47440871125888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.294447 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.294857 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.295196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.352854 47433788797824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.353713 47564809466752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.353788 47560357057408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.353890 47440871125888 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw1cou8ob
W0618 12:07:59.353861 47433788797824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpuu4l2846
I0618 12:07:59.354870 47433788797824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpuu4l2846', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2452b7fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.354889 47440871125888 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw1cou8ob', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b25f8dbce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.354245 47318224401280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:59.355285 47433788797824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.355301 47440871125888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.354903 47981647713152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.358543 47318224401280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.359215 47981647713152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.359920 47440871125888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.359965 47433788797824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:59.363611 47318224401280 estimator.py:1111] Calling model_fn.
W0618 12:07:59.363718 47318224401280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:59.364261 47981647713152 estimator.py:1111] Calling model_fn.
W0618 12:07:59.364371 47981647713152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.365086 47318224401280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.365718 47981647713152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.372176 47288982545280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.372195 46912996082560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881279.287889 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.288736 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.289545 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.371805 47054255145856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.289080 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.289922 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.290648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.371925 47803231687552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.372936 47054255145856 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acbf4c01d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.373019 47803231687552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp90pei7ta
I0618 12:07:59.374101 47803231687552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp90pei7ta', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7a573b2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.374179 47054255145856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.374545 47803231687552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.376795 46912996082560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.376789 47288982545280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.379210 47440871125888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.379371 47433788797824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.379465 47054255145856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.379665 47803231687552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:59.382224 46912996082560 estimator.py:1111] Calling model_fn.
I0618 12:07:59.382210 47288982545280 estimator.py:1111] Calling model_fn.
W0618 12:07:59.382326 47288982545280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.382340 46912996082560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.383801 46912996082560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.383774 47288982545280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881279.301284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.302066 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.302872 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.384513 47849962144640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.302955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.303680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.304353 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.384577 47991355159424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.385617 47849962144640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfjpp2601
W0618 12:07:59.385653 47991355159424 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl28aq8mi
I0618 12:07:59.386693 47849962144640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfjpp2601', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8538955e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.386763 47991355159424 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl28aq8mi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba624438e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.387149 47849962144640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.387212 47991355159424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.392403 47991355159424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.392449 47849962144640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.401433 47564809466752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.401697 47054255145856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.401714 47560357057408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.402010 47803231687552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.405743 47564809466752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.406069 47560357057408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881279.352667 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.353052 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.353381 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.409404 47417102340992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.347462 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.347939 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.348383 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.409517 47879536796544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.410838 47564809466752 estimator.py:1111] Calling model_fn.
W0618 12:07:59.410950 47564809466752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.410449 47417102340992 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnfncbza9
W0618 12:07:59.410562 47879536796544 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxxfpksq1
I0618 12:07:59.411183 47560357057408 estimator.py:1111] Calling model_fn.
W0618 12:07:59.411294 47560357057408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:59.411511 47417102340992 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnfncbza9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b207020ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.411589 47879536796544 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxxfpksq1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c1b5ebe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.411934 47417102340992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.412012 47879536796544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.412321 47564809466752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.412655 47560357057408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.414817 47991355159424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.414994 47849962144640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.416669 47417102340992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.416693 47879536796544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.426498 47440871125888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.427029 47433788797824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881279.373355 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.373760 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.374115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.428249 47451267777408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.429293 47451267777408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6xmpg7ci
I0618 12:07:59.430338 47451267777408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6xmpg7ci', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b28648c0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.430764 47451267777408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.430788 47440871125888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.431339 47433788797824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881279.369949 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.370416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.370809 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.433493 47806578316160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.434466 47806578316160 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbvkg1cks
I0618 12:07:59.435429 47806578316160 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbvkg1cks', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7b1eb4ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.435684 47451267777408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:59.435822 47806578316160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.435807 47440871125888 estimator.py:1111] Calling model_fn.
W0618 12:07:59.435917 47440871125888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:59.436400 47433788797824 estimator.py:1111] Calling model_fn.
W0618 12:07:59.436511 47433788797824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.436253 47417102340992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.436347 47879536796544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.437272 47440871125888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.437864 47433788797824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.440323 47806578316160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881279.371231 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.371953 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.372645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.448399 47352795808640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.366688 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.367614 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.368449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.448491 47739533198208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.449397 47352795808640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpo9dwukq3
W0618 12:07:59.449473 47739533198208 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0aejsact
I0618 12:07:59.450426 47352795808640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpo9dwukq3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1177290e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.450509 47739533198208 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0aejsact', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b82815e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.450866 47352795808640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.450946 47739533198208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.449876 47054255145856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.450063 47803231687552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.455692 47352795808640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.455697 47739533198208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.454180 47054255145856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.454394 47803231687552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.455280 47451267777408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881279.379210 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.379981 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.380673 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.459983 47032278651776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.371741 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.372662 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.373530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.460048 47095248069504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.459254 47054255145856 estimator.py:1111] Calling model_fn.
W0618 12:07:59.459362 47054255145856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.459847 47806578316160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:59.459453 47803231687552 estimator.py:1111] Calling model_fn.
W0618 12:07:59.459565 47803231687552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.461129 47032278651776 estimator.py:1760] Using temporary folder [2019-06-18 12:08:39] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 12:08:39] generate golden chunk: 3.317 seconds
[2019-06-18 12:08:39] iteration time 10: 49.973 seconds
2019-06-18 12:08:40.749075: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881319.968197 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:08:43] minmax time: 3.173 seconds
2019-06-18 12:08:43.932746: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:08:43.938309: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:08:43.942974: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881323.955622 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 10}}
[2019-06-18 12:08:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000012-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:08:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=12 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=1023779843 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=2047559674 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=3071339505 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=4095119336 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=5118899167 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=6142678998 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=7166458829 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=8190238660 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=9214018491 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=10237798322 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=11261578153 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=12285357984 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=13309137815 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=14332917646 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=15356697477 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=16380477308 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=17404257139 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=18428036970 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=19451816801 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000011-000006 --seed=20475596632 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:54] eval finished: 10.732 seconds
[2019-06-18 12:08:54] Win rate 000011-000006 vs 000008-000005: 0.640
:::MLL 1560881334.749821 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 12:08:54] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=13 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=1023779844 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=2047559675 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=3071339506 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=4095119337 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=5118899168 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=6142678999 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=7166458830 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=8190238661 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=9214018492 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=10237798323 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=11261578154 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=12285357985 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=13309137816 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=14332917647 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=15356697478 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=16380477309 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=17404257140 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000012-000005 --seed=18428036971 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:09:26] selfplay finished: 32.042 seconds
[2019-06-18 12:09:26] selfplay mn: 32.059 seconds
[2019-06-18 12:09:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-13-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779844 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559675 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339506 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119337 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899168 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142678999 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458830 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238661 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018492 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798323 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578154 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357985 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137816 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917647 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697478 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477309 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257140 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036971 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816802 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596633 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376464 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156295 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000012-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:09:27] train finished: 43.989 seconds
:::MLL 1560881329.132291 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.133188 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.134039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.217027 47361667670912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.218140 47361667670912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpian38iqg
I0618 12:08:49.219238 47361667670912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpian38iqg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1387f6ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.219673 47361667670912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881329.157445 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.158303 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.159098 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.220621 47952087159680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.221635 47952087159680 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp31ojhjyi
I0618 12:08:49.222692 47952087159680 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp31ojhjyi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9cffb55da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.223115 47952087159680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.224884 47361667670912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.228130 47952087159680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881329.153871 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.154752 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.155586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.238532 47197249999744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.160581 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.161296 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.161974 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.238562 47496789853056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.239681 47197249999744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp__mubfda
W0618 12:08:49.239711 47496789853056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjmc54ywa
I0618 12:08:49.240749 47197249999744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp__mubfda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aed3fe86e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.240763 47496789853056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjmc54ywa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b32fddfde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.241144 47197249999744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.241152 47496789853056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.246075 47361667670912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.246313 47496789853056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.246358 47197249999744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.250204 47952087159680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.268309 47496789853056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.268574 47197249999744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881329.229864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.230253 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.230570 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.286431 47520626504576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.232423 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.232877 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.233267 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.287709 47772096021376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.287499 47520626504576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppm0ckav8
I0618 12:08:49.288522 47520626504576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppm0ckav8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b388aa64e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.288928 47520626504576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.288697 47772096021376 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9xsei6jx
I0618 12:08:49.289691 47772096021376 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9xsei6jx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7317669e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.290086 47772096021376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.293659 47520626504576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.294746 47772096021376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881329.239875 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.240261 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.240577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.297100 47265463567232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.298576 47361667670912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881329.242081 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.242461 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.242815 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.297977 47673049928576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.228024 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.228788 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.229543 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.298569 47619296576384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.215133 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.216046 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.216939 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.298713 47685281624960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.298091 47265463567232 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpej37s4cv
I0618 12:08:49.299074 47265463567232 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpej37s4cv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd21c0de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.299471 47265463567232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.298933 47673049928576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcdllubnm
I0618 12:08:49.299919 47673049928576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcdllubnm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5c07cb1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.300322 47673049928576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.299742 47619296576384 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqtewq9cl
I0618 12:08:49.299863 47685281624960 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ee0dc0d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.300848 47619296576384 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqtewq9cl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f83d80dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.301135 47685281624960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.301288 47619296576384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.302036 47952087159680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.302956 47361667670912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.304096 47265463567232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.305099 47673049928576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.306527 47952087159680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.306508 47619296576384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.306584 47685281624960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:49.308248 47361667670912 estimator.py:1111] Calling model_fn.
W0618 12:08:49.308374 47361667670912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.309825 47361667670912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:08:49.311955 47952087159680 estimator.py:1111] Calling model_fn.
W0618 12:08:49.312082 47952087159680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.313002 47520626504576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.313549 47952087159680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.314157 47772096021376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.321033 47197249999744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.321127 47496789853056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.323539 47265463567232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.325221 47673049928576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.325703 47197249999744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.325791 47496789853056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.328828 47619296576384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.329590 47685281624960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:49.331253 47197249999744 estimator.py:1111] Calling model_fn.
I0618 12:08:49.331342 47496789853056 estimator.py:1111] Calling model_fn.
W0618 12:08:49.331368 47197249999744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.331459 47496789853056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.332822 47197249999744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.332910 47496789853056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881329.264087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.265009 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.265857 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.346307 47484587565952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.273706 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.274446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.275152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.346421 47536119145344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.347411 47484587565952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpokukmdnn
W0618 12:08:49.347514 47536119145344 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpaly8j8ie
I0618 12:08:49.348443 47484587565952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpokukmdnn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b30268fbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.348509 47536119145344 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpaly8j8ie', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c26153e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.348849 47484587565952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.348903 47536119145344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881329.281566 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.282055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.282478 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.347277 47195142353792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.289087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.289464 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.289793 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.347498 47741084025728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.348272 47195142353792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6w649id8
W0618 12:08:49.348455 47741084025728 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzptvqpz0
I0618 12:08:49.349237 47195142353792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6w649id8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aecc2484dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.349441 47741084025728 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzptvqpz0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6bdef11e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.349631 47195142353792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.349839 47741084025728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.353600 47484587565952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.353609 47536119145344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.354320 47195142353792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.354517 47741084025728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881329.268737 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.269620 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.270316 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.358806 47226846290816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.267628 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.268522 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.269343 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.358913 47367695586176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.359951 47226846290816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl0gmk9e_
W0618 12:08:49.359994 47367695586176 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6d0xchq0
I0618 12:08:49.361070 47226846290816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl0gmk9e_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af423fbde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.361073 47367695586176 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6d0xchq0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14ef41ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.361521 47226846290816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.361532 47367695586176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:49.360850 47520626504576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.361630 47772096021376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.365192 47520626504576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.366799 47367695586176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.366832 47226846290816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.365965 47772096021376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881329.285887 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.286647 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.287350 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.366421 47029018379136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.277095 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.278010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.278858 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.366806 47179749081984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.367547 47029018379136 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkmhn59_w
I0618 12:08:49.368636 47029018379136 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkmhn59_w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac614859e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:49.367938 47179749081984 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptdavi4ff
I0618 12:08:49.369055 47179749081984 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptdavi4ff', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae92cc59e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:49.369086 47029018379136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.369547 47179749081984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:49.370303 47520626504576 estimator.py:1111] Calling model_fn.
W0618 12:08:49.370415 47520626504576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:49.371129 47772096021376 estimator.py:1111] Calling model_fn.
W0618 12:08:49.371239 47772096021376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.371766 47520626504576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.371100 47265463567232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.373342 47536119145344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.373463 47484587565952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.372617 47772096021376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.373662 47195142353792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.373791 47741084025728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.374413 47029018379136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.374862 47179749081984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:49.375352 47673049928576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.375407 47265463567232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.379991 47673049928576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:49.380507 47265463567232 estimator.py:1111] Calling model_fn.
W0618 12:08:49.380617 47265463567232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.380715 47619296576384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.381341 47685281624960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:49.381968 47265463567232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.385202 47619296576384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:49.385433 47673049928576 estimator.py:1111] Calling model_fn.
W0618 12:08:49.385542 47673049928576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:49.385839 47685281624960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:49.386944 47673049928576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:49.389007 47367695586176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:49.389113 47226846290816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881329.333387 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.333840 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.334207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.389271 47911735780224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.332816 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.333279 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.333674 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:49.389580 47785623937920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:49.390281 47911735780224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph_n42wak
W0618 12:08:49.390554 47785623937920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfby8xxbp
I0618 12:08:49.391260 47911735780224 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph_n42wak', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.p[2019-06-18 12:09:30] divide_golden_chunk finished: 3.420 seconds
[2019-06-18 12:09:30] generate golden chunk: 3.435 seconds
[2019-06-18 12:09:30] moving /lfs/lfs12/gma_akey/results/epb015/models/000012-000006.index --> /lfs/lfs12/gma_akey/results/epb015/models/000012-000007.index
[2019-06-18 12:09:30] moving /lfs/lfs12/gma_akey/results/epb015/models/000012-000006.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb
[2019-06-18 12:09:30] moving /lfs/lfs12/gma_akey/results/epb015/models/000012-000006.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000012-000007.meta
[2019-06-18 12:09:30] moving /lfs/lfs12/gma_akey/results/epb015/models/000012-000006.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000012-000007.data-00000-of-00001
[2019-06-18 12:09:30] iteration time 11: 50.317 seconds
2019-06-18 12:09:31.085659: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881370.285801 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:09:34] minmax time: 3.228 seconds
2019-06-18 12:09:34.324281: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:09:34.329842: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:09:34.334389: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881374.345166 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 11}}
[2019-06-18 12:09:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:09:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=13 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=1023779844 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=2047559675 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=3071339506 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=4095119337 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=5118899168 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=6142678999 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=7166458830 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=8190238661 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=9214018492 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=10237798323 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=11261578154 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=12285357985 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=13309137816 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=14332917647 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=15356697478 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=16380477309 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=17404257140 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=18428036971 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=19451816802 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000012-000007 --seed=20475596633 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:09:47] eval finished: 12.800 seconds
[2019-06-18 12:09:47] Win rate 000012-000007 vs 000011-000006: 0.540
:::MLL 1560881387.208061 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:09:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=14 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=1023779845 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=2047559676 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=3071339507 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=4095119338 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=5118899169 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=6142679000 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=7166458831 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=8190238662 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=9214018493 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=10237798324 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=11261578155 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=12285357986 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=13309137817 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=14332917648 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=15356697479 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=16380477310 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=17404257141 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000013-000006 --seed=18428036972 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:10:16] selfplay finished: 29.157 seconds
[2019-06-18 12:10:16] selfplay mn: 29.177 seconds
[2019-06-18 12:10:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-14-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779845 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559676 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339507 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119338 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899169 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679000 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458831 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238662 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018493 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798324 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578155 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357986 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137817 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917648 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697479 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477310 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257141 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036972 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816803 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596634 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376465 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156296 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:10:18] train finished: 43.954 seconds
:::MLL 1560881379.630670 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.631432 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.632104 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.708472 47138456474496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.621451 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.622367 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.623201 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.708527 47009308545920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.709584 47138456474496 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp14fdrtdu
W0618 12:09:39.709621 47009308545920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1exhj_xn
I0618 12:09:39.710705 47138456474496 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp14fdrtdu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf8f8a6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.710705 47009308545920 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1exhj_xn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac17db95da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.711145 47009308545920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.711156 47138456474496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881379.642509 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.643222 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.643939 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.715528 47429070435200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.626191 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.627126 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.627958 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.715636 47916564751232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.716439 47009308545920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.716518 47138456474496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.716562 47429070435200 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_9bu_mbk
W0618 12:09:39.716615 47916564751232 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2qws6uq0
I0618 12:09:39.717550 47429070435200 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_9bu_mbk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b23397b7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.717621 47916564751232 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2qws6uq0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94ba686da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.717948 47429070435200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.718016 47916564751232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.723253 47429070435200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.723322 47916564751232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881379.651616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.652507 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.653345 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.736156 47474452149120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.652374 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.653236 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.653960 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.736470 47991582491520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:09:39.737260 47474452149120 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2dca717d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:39.738773 47009308545920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.738867 47138456474496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.737594 47991582491520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1ejh3stb
I0618 12:09:39.738494 47474452149120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.738788 47991582491520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1ejh3stb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba631d05e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.739264 47991582491520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.743803 47474452149120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.744723 47991582491520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.745583 47429070435200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.746240 47916564751232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881379.699669 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.700042 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.700367 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.757601 47429978616704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.758587 47429978616704 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpn0esroo9
:::MLL 1560881379.701237 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.701622 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.701999 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.759305 47762129200000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:09:39.759556 47429978616704 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpn0esroo9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b236f9d4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.759961 47429978616704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.760378 47762129200000 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpehq2u87w
I0618 12:09:39.761462 47762129200000 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpehq2u87w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70c554fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.761858 47762129200000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.764726 47429978616704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.766053 47474452149120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.766904 47762129200000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.767763 47991582491520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881379.713461 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.713859 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.714233 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.770096 47669956649856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.771162 47669956649856 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9wn1rpu5
I0618 12:09:39.772143 47669956649856 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9wn1rpu5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b4f6b7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.772537 47669956649856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881379.713756 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.714183 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.714509 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.776800 47097674838912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.777382 47669956649856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881379.717106 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.717519 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.717911 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.778218 47164367315840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.777786 47097674838912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplcy9q162
I0618 12:09:39.778781 47097674838912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplcy9q162', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad610c40e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.779182 47097674838912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.779220 47164367315840 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpe9ud6azr
I0618 12:09:39.780224 47164367315840 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpe9ud6azr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae597f26e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.780616 47164367315840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.784192 47429978616704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.783763 47097674838912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.785405 47164367315840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881379.717301 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.717746 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.718115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.785731 47048012026752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.786798 47048012026752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmphbqsagis
I0618 12:09:39.787772 47048012026752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmphbqsagis', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca80a19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:39.788259 47009308545920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:39.788166 47048012026752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.788713 47138456474496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:39.788683 47762129200000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881379.705263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.706099 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.706782 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.790880 47635077268352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.708637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.709398 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.710053 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.791130 47019948495744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.791965 47635077268352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp62izhtxx
W0618 12:09:39.792874 47009308545920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.792232 47019948495744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph20hpxj5
I0618 12:09:39.793046 47635077268352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp62izhtxx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5330725e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.793327 47019948495744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph20hpxj5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3f7ea2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:39.793347 47138456474496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.792947 47048012026752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:39.793515 47635077268352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.793760 47019948495744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.797051 47669956649856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.797169 47429070435200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:39.797224 47916564751232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:39.798073 47009308545920 estimator.py:1111] Calling model_fn.
W0618 12:09:39.798203 47009308545920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:39.798619 47138456474496 estimator.py:1111] Calling model_fn.
W0618 12:09:39.798738 47138456474496 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.798774 47635077268352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.798955 47019948495744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.799614 47009308545920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:39.800176 47138456474496 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:39.801523 47916564751232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.801524 47429070435200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.803404 47097674838912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.804864 47164367315840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:09:39.806709 47916564751232 estimator.py:1111] Calling model_fn.
I0618 12:09:39.806789 47429070435200 estimator.py:1111] Calling model_fn.
W0618 12:09:39.806827 47916564751232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.806904 47429070435200 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.808239 47916564751232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:39.808323 47429070435200 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881379.713377 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.714144 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.714868 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.810481 47476962120576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.708840 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.709738 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.710557 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.810615 47588910117760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.811516 47476962120576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph0c73ms6
W0618 12:09:39.811645 47588910117760 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp78k9lr34
I0618 12:09:39.812526 47476962120576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph0c73ms6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e600cae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.812631 47588910117760 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp78k9lr34', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4870ab6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881379.724864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.725694 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.726431 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.812343 47283814335360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.723753 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.724636 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.725417 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.812382 47608519644032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:09:39.812927 47476962120576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.813024 47588910117760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.813448 47283814335360 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7gckpiuk
W0618 12:09:39.813479 47608519644032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_1jo98dh
I0618 12:09:39.814540 47283814335360 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7gckpiuk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b01678b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.814553 47608519644032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_1jo98dh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d017d1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.814978 47283814335360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:39.814986 47608519644032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.813609 47048012026752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.815690 47474452149120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:39.817788 47588910117760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.817800 47476962120576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.817563 47991582491520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:39.820349 47283814335360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.820361 47608519644032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:39.820114 47474452149120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.820972 47635077268352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.821266 47019948495744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.821979 47991582491520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:39.825268 47474452149120 estimator.py:1111] Calling model_fn.
W0618 12:09:39.825376 47474452149120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.826735 47474452149120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:09:39.827033 47991582491520 estimator.py:1111] Calling model_fn.
W0618 12:09:39.827143 47991582491520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.828500 47991582491520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:39.831922 47429978616704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:39.837409 47476962120576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.836212 47429978616704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:39.837615 47588910117760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881379.780433 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.780870 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.781263 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.836984 47021638820736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.837995 47021638820736 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpt6phd6rg
I0618 12:09:39.838975 47021638820736 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpt6phd6rg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac45caa5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881379.783673 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.784116 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.784519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.839008 47269961970560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:09:39.839414 47021638820736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.840205 47269961970560 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdec4ubi1
I0618 12:09:39.841188 47269961970560 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdec4ubi1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe2de10e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:39.841247 47429978616704 estimator.py:1111] Calling model_fn.
W0618 12:09:39.841352 47429978616704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:39.842565 47608519644032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.841536 47762129200000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:39.841578 47269961970560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:39.842898 47283814335360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:39.842718 47429978616704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:39.844151 47021638820736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881379.768190 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.768671 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.769069 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.845607 47573392712576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881379.767195 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881379.767692 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881379.768143 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:39.845786 47062635983744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:39.845262 47669956649856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function tak[2019-06-18 12:10:19] divide_golden_chunk finished: 3.354 seconds
[2019-06-18 12:10:19] generate golden chunk: 3.369 seconds
[2019-06-18 12:10:19] moving /lfs/lfs12/gma_akey/results/epb015/models/000013-000007.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000013-000008.meta
[2019-06-18 12:10:19] moving /lfs/lfs12/gma_akey/results/epb015/models/000013-000007.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb
[2019-06-18 12:10:19] moving /lfs/lfs12/gma_akey/results/epb015/models/000013-000007.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000013-000008.data-00000-of-00001
[2019-06-18 12:10:19] moving /lfs/lfs12/gma_akey/results/epb015/models/000013-000007.index --> /lfs/lfs12/gma_akey/results/epb015/models/000013-000008.index
[2019-06-18 12:10:19] iteration time 12: 49.507 seconds
2019-06-18 12:10:20.648144: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881419.793207 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:10:23] minmax time: 3.238 seconds
2019-06-18 12:10:23.896979: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:10:23.902386: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:10:23.906971: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881423.917842 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 12}}
[2019-06-18 12:10:23] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:10:23] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=14 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=1023779845 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=2047559676 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=3071339507 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=4095119338 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=5118899169 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=6142679000 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=7166458831 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=8190238662 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=9214018493 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=10237798324 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=11261578155 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=12285357986 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=13309137817 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=14332917648 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=15356697479 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=16380477310 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=17404257141 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=18428036972 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=19451816803 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000013-000008 --seed=20475596634 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:10:34] eval finished: 10.267 seconds
[2019-06-18 12:10:34] Win rate 000013-000008 vs 000012-000007: 0.640
:::MLL 1560881434.246539 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:10:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=15 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=1023779846 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=2047559677 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=3071339508 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=4095119339 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=5118899170 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=6142679001 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=7166458832 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=8190238663 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=9214018494 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=10237798325 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=11261578156 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=12285357987 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=13309137818 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=14332917649 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=15356697480 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=16380477311 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=17404257142 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000014-000007 --seed=18428036973 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:11:04] selfplay finished: 30.287 seconds
[2019-06-18 12:11:04] selfplay mn: 30.306 seconds
[2019-06-18 12:11:04] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-15-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779846 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559677 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339508 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119339 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899170 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679001 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458832 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238663 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018494 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798325 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578156 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357987 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137818 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917649 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697480 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477311 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257142 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036973 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816804 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596635 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376466 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156297 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:11:07] train finished: 43.703 seconds
:::MLL 1560881429.183549 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.184363 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.185196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.275274 47128602395520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.185047 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.185790 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.186447 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.275346 47406201602944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.276334 47128602395520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5602au6m
W0618 12:10:29.276394 47406201602944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp99ft_wt3
I0618 12:10:29.277413 47128602395520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5602au6m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2add44311da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.277462 47406201602944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp99ft_wt3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1de664de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.277850 47128602395520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.277894 47406201602944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.283151 47406201602944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.283197 47128602395520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881429.200453 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.201180 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.201874 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.286565 47574225003392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.197656 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.198398 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.199112 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.286745 47769253569408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.287741 47574225003392 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp59wjbneg
W0618 12:10:29.287837 47769253569408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpp8nq5kxq
I0618 12:10:29.288813 47574225003392 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp59wjbneg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45055e7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.288913 47769253569408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpp8nq5kxq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b726dfa3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.289240 47574225003392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.289329 47769253569408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.294542 47574225003392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.294615 47769253569408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.305379 47128602395520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.305583 47406201602944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.316776 47769253569408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.316876 47574225003392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881429.245654 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.246055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.246420 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.316315 47421334696832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.239227 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.239764 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.240216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.316353 47865322726272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.317324 47421334696832 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvzzolz6m
W0618 12:10:29.317353 47865322726272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpv633wztm
I0618 12:10:29.318300 47421334696832 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvzzolz6m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b216c655e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.318323 47865322726272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpv633wztm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b88cc253e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.318695 47421334696832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.318718 47865322726272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881429.238000 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.238713 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.239420 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.321461 47779233829760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.234377 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.235220 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.235919 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.321606 47424934642560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
I0618 12:10:29.322613 47779233829760 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b74c0d8dcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:29.322715 47424934642560 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplq9i5g7l
W0618 12:10:29.323337 47421334696832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.323345 47865322726272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:29.323795 47424934642560 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplq9i5g7l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2242f84e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.323879 47779233829760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.324236 47424934642560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.329240 47779233829760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.329483 47424934642560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881429.278278 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.278693 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.279086 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.341747 47408027005824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.278779 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.279191 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.279506 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.341764 47466413716352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.342581 47421334696832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.342605 47865322726272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.342705 47408027005824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpv4nr9ohs
W0618 12:10:29.342732 47466413716352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp70e4zddm
I0618 12:10:29.343667 47408027005824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpv4nr9ohs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1e53324e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.343706 47466413716352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp70e4zddm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2beb50ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.344069 47408027005824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.344094 47466413716352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.348864 47408027005824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.348865 47466413716352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.351511 47424934642560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.351524 47779233829760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.352956 47128602395520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.353233 47406201602944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.357239 47128602395520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:29.357554 47406201602944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881429.297038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.297437 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.297793 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.360730 47363678233472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.361719 47363678233472 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmuzgszoh
I0618 12:10:29.362275 47128602395520 estimator.py:1111] Calling model_fn.
W0618 12:10:29.362383 47128602395520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:29.362688 47363678233472 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmuzgszoh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b13ffcdbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.362615 47406201602944 estimator.py:1111] Calling model_fn.
W0618 12:10:29.362720 47406201602944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:29.363090 47363678233472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881429.291247 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.291743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.292178 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.363005 47915606111104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.363719 47128602395520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.364083 47406201602944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.363985 47915606111104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpj_b2wv4l
I0618 12:10:29.364946 47915606111104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpj_b2wv4l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b948144be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.365327 47915606111104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.367120 47574225003392 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.367141 47769253569408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.368046 47466413716352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.368253 47408027005824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.367789 47363678233472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.369886 47915606111104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.371656 47574225003392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:29.371698 47769253569408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881429.270089 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.271005 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.271787 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.375483 47623014753152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.275037 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.275782 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.276476 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.375590 47623613969280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
I0618 12:10:29.376950 47574225003392 estimator.py:1111] Calling model_fn.
I0618 12:10:29.376979 47769253569408 estimator.py:1111] Calling model_fn.
W0618 12:10:29.377077 47574225003392 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:29.377110 47769253569408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:29.376597 47623014753152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprib8nmzs
W0618 12:10:29.376673 47623613969280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp21dd5wzq
:::MLL 1560881429.296315 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.297048 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.297749 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.378412 46936642757504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
I0618 12:10:29.377701 47623014753152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprib8nmzs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b506176fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881429.287475 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.288412 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.289299 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.378655 47349034734464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
I0618 12:10:29.377783 47623613969280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp21dd5wzq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b50852e3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.378142 47623014753152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.378244 47623613969280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.378496 47574225003392 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.378511 47769253569408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.379533 46936642757504 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqtjjt1vn
W0618 12:10:29.379736 47349034734464 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2_q5rmzj
I0618 12:10:29.380665 46936642757504 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqtjjt1vn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab092816e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.380854 47349034734464 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2_q5rmzj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1096fbae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.381103 46936642757504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.381309 47349034734464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.383447 47623014753152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.383566 47623613969280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.386430 46936642757504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.386478 47349034734464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.387146 47363678233472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.389153 47915606111104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.389794 47421334696832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.389952 47865322726272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.394098 47421334696832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:29.394260 47865322726272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:29.399125 47421334696832 estimator.py:1111] Calling model_fn.
W0618 12:10:29.399233 47421334696832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:29.399272 47865322726272 estimator.py:1111] Calling model_fn.
W0618 12:10:29.399380 47865322726272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:29.399494 47424934642560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.399952 47779233829760 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.400588 47421334696832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.400725 47865322726272 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881429.309202 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.309949 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.310654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.403416 47149195498368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.290617 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.291553 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.292445 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.403444 47938483266432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.404420 47938483266432 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2oodpe7s
W0618 12:10:29.404452 47149195498368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpi9wx5rke
I0618 12:10:29.405401 47938483266432 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2oodpe7s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b99d4da7dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.405461 47149195498368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpi9wx5rke', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae20fa2ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:29.403797 47424934642560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:29.405819 47938483266432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.405889 47149195498368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.404268 47779233829760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:29.405232 47623014753152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.405761 47623613969280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.408623 47349034734464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:29.408820 46936642757504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:10:29.408804 47424934642560 estimator.py:1111] Calling model_fn.
W0618 12:10:29.408908 47424934642560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:29.410697 47149195498368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:29.410694 47938483266432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:29.409357 47779233829760 estimator.py:1111] Calling model_fn.
W0618 12:10:29.409467 47779233829760 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:29.410259 47424934642560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881429.327367 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.327751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.328093 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.410502 47756497474432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881429.325707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881429.326109 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881429.326456 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:29.410475 47815667745664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:29.410836 47779233829760 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:29.411551 47756497474432 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1_h0p8b3
W0618 12:10:29.411520 47815667745664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpj4_tzo4s
I0618 12:10:29.412512 47815667745664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpj4_tzo4s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d3c7a5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.412541 47756497474432 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1_h0p8b3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f75a79e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:29.412906 47815667745664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:29.412934 47756497474432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:29.415322 47466413716352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:29.415602 47408027005824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the[2019-06-18 12:11:07] divide_golden_chunk finished: 3.299 seconds
[2019-06-18 12:11:07] generate golden chunk: 3.316 seconds
[2019-06-18 12:11:07] moving /lfs/lfs12/gma_akey/results/epb015/models/000014-000008.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb
[2019-06-18 12:11:07] moving /lfs/lfs12/gma_akey/results/epb015/models/000014-000008.index --> /lfs/lfs12/gma_akey/results/epb015/models/000014-000009.index
[2019-06-18 12:11:07] moving /lfs/lfs12/gma_akey/results/epb015/models/000014-000008.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000014-000009.meta
[2019-06-18 12:11:07] moving /lfs/lfs12/gma_akey/results/epb015/models/000014-000008.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000014-000009.data-00000-of-00001
[2019-06-18 12:11:07] iteration time 13: 48.117 seconds
2019-06-18 12:11:08.807053: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881467.910779 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:11:12] minmax time: 3.270 seconds
2019-06-18 12:11:12.087520: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:11:12.092820: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:11:12.097486: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881472.108584 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 13}}
[2019-06-18 12:11:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:11:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=15 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=1023779846 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=2047559677 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=3071339508 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=4095119339 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=5118899170 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=6142679001 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=7166458832 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=8190238663 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=9214018494 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=10237798325 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=11261578156 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=12285357987 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=13309137818 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=14332917649 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=15356697480 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=16380477311 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=17404257142 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=18428036973 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=19451816804 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000014-000009 --seed=20475596635 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:11:22] eval finished: 10.824 seconds
[2019-06-18 12:11:22] Win rate 000014-000009 vs 000013-000008: 0.450
:::MLL 1560881482.995169 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:11:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=16 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=1023779847 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=2047559678 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=3071339509 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=4095119340 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=5118899171 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=6142679002 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=7166458833 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=8190238664 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=9214018495 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=10237798326 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=11261578157 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=12285357988 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=13309137819 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=14332917650 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=15356697481 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=16380477312 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=17404257143 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000015-000008 --seed=18428036974 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:11:53] selfplay finished: 30.255 seconds
[2019-06-18 12:11:53] selfplay mn: 30.273 seconds
[2019-06-18 12:11:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-16-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779847 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559678 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339509 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119340 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899171 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679002 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458833 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238664 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018495 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798326 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578157 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357988 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137819 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917650 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697481 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477312 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257143 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036974 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816805 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596636 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376467 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156298 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:11:55] train finished: 43.609 seconds
:::MLL 1560881477.342097 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.342988 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.343827 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.432757 46951792771968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.348709 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.349419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.350091 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.432798 47277881869184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.433930 47277881869184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8jhvmv9n
W0618 12:11:17.433962 46951792771968 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmkcvc8mv
I0618 12:11:17.435047 47277881869184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8jhvmv9n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0005f10e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.435067 46951792771968 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmkcvc8mv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab419844e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.435508 47277881869184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.435514 46951792771968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.440752 46951792771968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.440881 47277881869184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881477.354940 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.355805 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.356662 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.443148 47343600669568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.354784 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.355644 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.356476 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.443182 47005319553920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.444268 47343600669568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmyawhsd4
W0618 12:11:17.444315 47005319553920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcpreob26
I0618 12:11:17.445381 47343600669568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmyawhsd4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0f53166da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.445395 47005319553920 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcpreob26', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac08ff62e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.445827 47343600669568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.445845 47005319553920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.451225 47005319553920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.451253 47343600669568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.462873 46951792771968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.463471 47277881869184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.473097 47343600669568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.473095 47005319553920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881477.417099 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.417545 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.417928 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.478626 47281948955520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.479638 47281948955520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5c0rvnya
I0618 12:11:17.480621 47281948955520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5c0rvnya', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b00f85bde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.481024 47281948955520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881477.423257 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.423758 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.424142 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.482134 47977287771008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.483153 47977287771008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpctx95p9p
I0618 12:11:17.484127 47977287771008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpctx95p9p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba2ddc82e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.484515 47977287771008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881477.395346 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.396158 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.396838 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.485562 47407649981312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.394037 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.394793 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.395556 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.485772 47270151934848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.485905 47281948955520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881477.415775 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.416183 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.416538 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.487044 47467443360640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.417384 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.417783 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.418130 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.487054 47776677671808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.486687 47407649981312 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw_7ja2w0
W0618 12:11:17.486855 47270151934848 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpwzo27tkk
I0618 12:11:17.487785 47407649981312 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw_7ja2w0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1e3cb92e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.487945 47270151934848 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpwzo27tkk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe3933add8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.488234 47407649981312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.488392 47270151934848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.488055 47467443360640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx6ggg89b
W0618 12:11:17.488084 47776677671808 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdu9s4ujd
I0618 12:11:17.489055 47467443360640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx6ggg89b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c28afeda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.489055 47776677671808 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdu9s4ujd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b74287d0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.489450 47467443360640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.489448 47776677671808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.489015 47977287771008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.493523 47407649981312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.493625 47270151934848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.494188 47467443360640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.494192 47776677671808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881477.404088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.404854 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.405528 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.498313 47519838262144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.402559 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.403394 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.404190 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.498331 47115237917568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.499465 47115237917568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyvoof518
W0618 12:11:17.499499 47519838262144 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprnoe5lq5
I0618 12:11:17.500551 47115237917568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyvoof518', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada279b4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.500566 47519838262144 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprnoe5lq5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b385baaada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.500997 47115237917568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.501000 47519838262144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.506348 47519838262144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.506368 47115237917568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.505352 47281948955520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.508316 47977287771008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.511921 46951792771968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.512710 47277881869184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.513442 47776677671808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.513544 47467443360640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.515675 47270151934848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.515755 47407649981312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.516209 46951792771968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.517060 47277881869184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.521270 47005319553920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.521332 47343600669568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:17.521299 46951792771968 estimator.py:1111] Calling model_fn.
W0618 12:11:17.521412 46951792771968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:17.522166 47277881869184 estimator.py:1111] Calling model_fn.
W0618 12:11:17.522279 47277881869184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.522793 46951792771968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:17.523643 47277881869184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:17.525570 47005319553920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.525635 47343600669568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.528482 47519838262144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.528615 47115237917568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:11:17.530625 47005319553920 estimator.py:1111] Calling model_fn.
I0618 12:11:17.530704 47343600669568 estimator.py:1111] Calling model_fn.
W0618 12:11:17.530737 47005319553920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.530809 47343600669568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.532090 47005319553920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:17.532170 47343600669568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881477.438663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.439402 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.440146 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.532268 47701691147136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.441400 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.442151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.442825 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.532342 47182743479168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
I0618 12:11:17.533395 47701691147136 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b62b2f17d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:17.533471 47182743479168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqhepqlp8
I0618 12:11:17.534647 47701691147136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.534643 47182743479168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqhepqlp8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9df406e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.535105 47182743479168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881477.465995 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.466406 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.466748 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.537350 47208214537088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.468136 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.468589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.468943 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.537383 47550557139840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.538356 47208214537088 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpd2lsqkjj
W0618 12:11:17.538391 47550557139840 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpn3wgmj5l
I0618 12:11:17.539341 47208214537088 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpd2lsqkjj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aefcd71fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.539367 47550557139840 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpn3wgmj5l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f82a78dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.539736 47208214537088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.539753 47550557139840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.540135 47701691147136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.540468 47182743479168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881477.478088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.478522 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.478886 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.542127 47753937339264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.475784 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.476205 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.476570 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.542143 47322487722880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:17.543121 47753937339264 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbov3ke4r
W0618 12:11:17.543154 47322487722880 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkofx2g5q
I0618 12:11:17.544090 47753937339264 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbov3ke4r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6edd0f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.544133 47322487722880 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkofx2g5q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0a68a86e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:17.544492 47753937339264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:17.544537 47322487722880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:17.544510 47208214537088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.544514 47550557139840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.549234 47753937339264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.549235 47322487722880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:17.553309 47281948955520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.555926 47977287771008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.557639 47281948955520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.560256 47977287771008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.561247 47467443360640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.561563 47776677671808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.562278 47701691147136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.562695 47182743479168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:11:17.562721 47281948955520 estimator.py:1111] Calling model_fn.
W0618 12:11:17.562832 47281948955520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.564137 47550557139840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.564200 47208214537088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.564199 47281948955520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:17.565567 47467443360640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:17.565901 47776677671808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:17.565334 47977287771008 estimator.py:1111] Calling model_fn.
W0618 12:11:17.565905 47407649981312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.565444 47977287771008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.566535 47270151934848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:17.568407 47322487722880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.568536 47753937339264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:17.566803 47977287771008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:17.570187 47407649981312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881477.469420 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.470189 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.470833 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.571252 47908589183872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881477.462560 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881477.463471 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881477.464272 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:17.571353 47137165345664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000005-000002.tfrecord.zz_0_0
I0618 12:11:17.570634 47467443360640 estimator.py:1111] Calling model_fn.
W0618 12:11:17.570743 47467443360640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.570866 47270151934848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:17.570977 47776677671808 estimator.py:1111] Calling model_fn.
W0618 12:11:17.571086 47776677671808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:17.572255 47908589183872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl4invb7a
W0618 12:11:17.572358 47137165345664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplv0x62x9
I0618 12:11:17.573253 47908589183872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl4invb7a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_[2019-06-18 12:11:56] divide_golden_chunk finished: 3.309 seconds
[2019-06-18 12:11:56] generate golden chunk: 3.324 seconds
[2019-06-18 12:11:56] iteration time 14: 48.683 seconds
2019-06-18 12:11:57.614238: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881516.593796 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:12:00] minmax time: 3.221 seconds
2019-06-18 12:12:00.845423: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:00.850813: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:00.855433: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881520.868206 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 14}}
[2019-06-18 12:12:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:12:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=16 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=1023779847 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=2047559678 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=3071339509 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=4095119340 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=5118899171 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=6142679002 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=7166458833 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=8190238664 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=9214018495 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=10237798326 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=11261578157 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=12285357988 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=13309137819 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=14332917650 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=15356697481 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=16380477312 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=17404257143 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=18428036974 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=19451816805 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000015-000009 --seed=20475596636 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:12:11] eval finished: 10.473 seconds
[2019-06-18 12:12:11] Win rate 000015-000009 vs 000013-000008: 0.680
:::MLL 1560881531.402793 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:12:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=17 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=1023779848 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=2047559679 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=3071339510 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=4095119341 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=5118899172 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=6142679003 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=7166458834 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=8190238665 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=9214018496 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=10237798327 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=11261578158 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=12285357989 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=13309137820 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=14332917651 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=15356697482 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=16380477313 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=17404257144 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000016-000008 --seed=18428036975 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:12:41] selfplay finished: 30.508 seconds
[2019-06-18 12:12:41] selfplay mn: 30.525 seconds
[2019-06-18 12:12:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-17-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779848 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559679 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339510 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119341 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899172 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679003 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458834 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238665 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018496 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798327 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578158 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357989 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137820 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917651 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697482 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477313 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257144 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036975 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816806 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596637 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376468 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156299 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:12:44] train finished: 43.677 seconds
:::MLL 1560881526.069809 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.070675 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.071546 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.160977 47938913072000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.077079 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.077827 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.078506 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.161003 47354233041792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.162165 47938913072000 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2ou3o68n
W0618 12:12:06.162196 47354233041792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxstndbse
I0618 12:12:06.163290 47938913072000 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2ou3o68n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b99ee78be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.163351 47354233041792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxstndbse', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11ccd37e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.163741 47938913072000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.163812 47354233041792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.168973 47938913072000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.169111 47354233041792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.191258 47938913072000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.191356 47354233041792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881526.110204 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.111084 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.111949 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.198953 47965850145664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.119140 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.119885 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.120530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.199009 47944897540992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.200033 47965850145664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_19365y0
W0618 12:12:06.200065 47944897540992 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcatjho0_
I0618 12:12:06.201050 47965850145664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_19365y0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0340bde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.201066 47944897540992 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcatjho0_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b532c7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.201457 47965850145664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.201463 47944897540992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.206371 47944897540992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.206409 47965850145664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881526.154484 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.154900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.155290 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.221206 47416817591168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.154891 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.155375 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.155699 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.222653 47113715639168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.222192 47416817591168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpe8b_x6gl
I0618 12:12:06.223172 47416817591168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpe8b_x6gl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b205f27ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.223577 47416817591168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.223707 47113715639168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcm015yrh
I0618 12:12:06.224924 47113715639168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcm015yrh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad9ccdf2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.225381 47113715639168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.228299 47416817591168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.228909 47944897540992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.229098 47965850145664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.230231 47113715639168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.242257 47354233041792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.242248 47938913072000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.246554 47354233041792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.246531 47938913072000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.247564 47416817591168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.251420 47113715639168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:06.251601 47354233041792 estimator.py:1111] Calling model_fn.
I0618 12:12:06.251579 47938913072000 estimator.py:1111] Calling model_fn.
W0618 12:12:06.251687 47938913072000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.251708 47354233041792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.253049 47354233041792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:06.253064 47938913072000 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881526.167341 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.168215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.168950 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.257798 47975247446912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.166892 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.167744 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.168560 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.257977 47619247702912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.258909 47975247446912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgi8encm3
W0618 12:12:06.259066 47619247702912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcio33t80
I0618 12:12:06.260037 47975247446912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgi8encm3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba2642b5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.260190 47619247702912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcio33t80', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f80ee5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.260481 47975247446912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.260658 47619247702912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.265657 47975247446912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.265974 47619247702912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881526.202896 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.203288 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.203640 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.268894 46987486147456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.204804 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.205195 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.205530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.269574 47423651988352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.269904 46987486147456 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzohhxbbt
I0618 12:12:06.270952 46987486147456 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzohhxbbt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc69020e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:06.270567 47423651988352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf6u5j37e
I0618 12:12:06.271371 46987486147456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.271610 47423651988352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf6u5j37e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b21f6848e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.272036 47423651988352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.276144 46987486147456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.276672 47423651988352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881526.195983 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.196741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.197389 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.277194 47340107891584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.185211 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.186086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.186962 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.277297 47453876454272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
I0618 12:12:06.278346 47340107891584 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e82e6cd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:06.278399 47453876454272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpalzdmuz9
I0618 12:12:06.279473 47453876454272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpalzdmuz9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2900093e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.279615 47340107891584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.279924 47453876454272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.281609 47944897540992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.281741 47965850145664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.284908 47340107891584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.285114 47453876454272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.285906 47944897540992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.286016 47965850145664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.287580 47975247446912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.288325 47619247702912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:06.290933 47944897540992 estimator.py:1111] Calling model_fn.
W0618 12:12:06.291041 47944897540992 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:06.291061 47965850145664 estimator.py:1111] Calling model_fn.
W0618 12:12:06.291171 47965850145664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.292394 47944897540992 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:06.292518 47965850145664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:06.295362 46987486147456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.295023 47416817591168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.296024 47423651988352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.299320 47416817591168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.302329 47113715639168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:12:06.304377 47416817591168 estimator.py:1111] Calling model_fn.
W0618 12:12:06.304485 47416817591168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.305830 47416817591168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:06.307023 47340107891584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.307096 47453876454272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.307036 47113715639168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:06.312627 47113715639168 estimator.py:1111] Calling model_fn.
W0618 12:12:06.312733 47113715639168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.314121 47113715639168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881526.242317 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.242694 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.243015 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.315921 47381974098816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.244268 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.244644 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.244965 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.316041 47065170281344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.316942 47381974098816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdmmzwxk3
W0618 12:12:06.317062 47065170281344 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0h8bu0sa
I0618 12:12:06.317950 47381974098816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdmmzwxk3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1842527dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.318043 47065170281344 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0h8bu0sa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace7f57ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.318349 47381974098816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.318446 47065170281344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881526.252707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.253121 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.253463 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.321107 47908263322496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.221439 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.222326 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.223166 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.322953 47335431119744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.226263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.227010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.227704 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.323077 47670455665536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.322989 47381974098816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.323026 47065170281344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.322175 47908263322496 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc_fqvc6u
I0618 12:12:06.323148 47908263322496 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc_fqvc6u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b92cb9aae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:06.324016 47335431119744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpv0uhg88r
W0618 12:12:06.324129 47670455665536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptzw9sc5y
I0618 12:12:06.325063 47335431119744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpv0uhg88r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d6c24fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.325146 47670455665536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptzw9sc5y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b6d29de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.323537 47908263322496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881526.251347 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.251801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.252286 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.323460 47170922615680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
I0618 12:12:06.325465 47335431119744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.325541 47670455665536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.324523 47170922615680 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptaxgwp2u
I0618 12:12:06.325651 47170922615680 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptaxgwp2u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae71eac6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.326107 47170922615680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881526.239622 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.240362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.241196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.328541 47526558417792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881526.241485 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881526.242229 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881526.242931 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:06.328719 47997575181184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:06.328250 47908263322496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.330178 47335431119744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.330191 47670455665536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.329671 47526558417792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5z6iruhe
W0618 12:12:06.329791 47997575181184 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplgxlccrg
I0618 12:12:06.330797 47526558417792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5z6iruhe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39ec381e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.331029 47997575181184 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplgxlccrg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba797017da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:06.331245 47526558417792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:06.331471 47997575181184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:06.331157 47170922615680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.336505 47526558417792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.336602 47997575181184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:06.338032 47975247446912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.338530 47619247702912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.342256 47381974098816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.342323 47065170281344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:06.342470 47975247446912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.342974 47619247702912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:06.343116 46987486147456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.343855 47423651988352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:06.347427 46987486147456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:06.347652 47975247446912 estimator.py:1111] Calling model_fn.
W0618 12:12:06.347760 47975247446912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:06.347561 47908263322496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:06.348134 47619247702912 estimator.py:1111] Calling model_fn[2019-06-18 12:12:45] divide_golden_chunk finished: 3.300 seconds
[2019-06-18 12:12:45] generate golden chunk: 3.314 seconds
[2019-06-18 12:12:45] moving /lfs/lfs12/gma_akey/results/epb015/models/000016-000009.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000016-000010.data-00000-of-00001
[2019-06-18 12:12:45] moving /lfs/lfs12/gma_akey/results/epb015/models/000016-000009.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000016-000010.meta
[2019-06-18 12:12:45] moving /lfs/lfs12/gma_akey/results/epb015/models/000016-000009.index --> /lfs/lfs12/gma_akey/results/epb015/models/000016-000010.index
[2019-06-18 12:12:45] moving /lfs/lfs12/gma_akey/results/epb015/models/000016-000009.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb
[2019-06-18 12:12:45] iteration time 15: 48.691 seconds
2019-06-18 12:12:46.283464: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881565.284895 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:12:49] minmax time: 3.250 seconds
2019-06-18 12:12:49.543735: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:49.549260: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:49.553863: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881569.565486 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 15}}
[2019-06-18 12:12:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:12:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=17 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=1023779848 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=2047559679 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=3071339510 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=4095119341 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=5118899172 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=6142679003 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=7166458834 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=8190238665 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=9214018496 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=10237798327 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=11261578158 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=12285357989 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=13309137820 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=14332917651 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=15356697482 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=16380477313 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=17404257144 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=18428036975 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=19451816806 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000016-000010 --seed=20475596637 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:01] eval finished: 11.515 seconds
[2019-06-18 12:13:01] Win rate 000016-000010 vs 000015-000009: 0.740
:::MLL 1560881581.141338 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:13:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=18 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=1023779849 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=2047559680 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=3071339511 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=4095119342 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=5118899173 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=6142679004 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=7166458835 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=8190238666 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=9214018497 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=10237798328 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=11261578159 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=12285357990 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=13309137821 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=14332917652 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=15356697483 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=16380477314 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=17404257145 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000017-000009 --seed=18428036976 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:13:31] selfplay finished: 29.874 seconds
[2019-06-18 12:13:31] selfplay mn: 29.892 seconds
[2019-06-18 12:13:31] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-18-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779849 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559680 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339511 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119342 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899173 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679004 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458835 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238666 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018497 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798328 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578159 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357990 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137821 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917652 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697483 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477314 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257145 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036976 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816807 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596638 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376469 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156300 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:13:33] train finished: 44.286 seconds
:::MLL 1560881574.762940 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.763752 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.764448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.843313 47953481937792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.749239 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.750082 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.750891 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.843313 47017850205056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.844431 47017850205056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf02l77xz
W0618 12:12:54.844461 47953481937792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfi469gg5
I0618 12:12:54.845417 47017850205056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf02l77xz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac37ad8ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.845453 47953481937792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfi469gg5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d52d7fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.845820 47017850205056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.845856 47953481937792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.850678 47017850205056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.850736 47953481937792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.872417 47017850205056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881574.779061 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.779983 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.780837 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.872863 47610369823616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.784191 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.784942 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.785634 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.872893 47033984959360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.872889 47953481937792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.874021 47033984959360 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp25r7zomm
W0618 12:12:54.874052 47610369823616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkwaw7ejw
I0618 12:12:54.875143 47033984959360 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp25r7zomm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac73c8d9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.875166 47610369823616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkwaw7ejw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d6fc4ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.875601 47033984959360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.875629 47610369823616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.880925 47033984959360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.880948 47610369823616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.903495 47033984959360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.903584 47610369823616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881574.842486 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.842858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.843197 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.915314 47850039665536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.839927 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.840312 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.840634 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.915530 47996098794368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.916355 47850039665536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2iy25bh7
W0618 12:12:54.916537 47996098794368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgib0gomo
I0618 12:12:54.917413 47850039665536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2iy25bh7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b853d343da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.917561 47996098794368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgib0gomo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba73f01ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.917814 47850039665536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.917957 47996098794368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.922401 47850039665536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.922498 47996098794368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.924665 47017850205056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:54.924997 47953481937792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881574.858505 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.858883 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.859199 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.926321 47381443638144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.860427 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.860807 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.861130 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.927156 47588786221952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.927439 47381443638144 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpu7aohwxb
I0618 12:12:54.928581 47381443638144 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpu7aohwxb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1822b44e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:54.928262 47588786221952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjqz1phz8
I0618 12:12:54.928986 47381443638144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.929235 47588786221952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjqz1phz8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4869490e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.929632 47588786221952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.929045 47017850205056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.929350 47953481937792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.933583 47381443638144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.934123 47588786221952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:54.934135 47017850205056 estimator.py:1111] Calling model_fn.
W0618 12:12:54.934243 47017850205056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:54.934415 47953481937792 estimator.py:1111] Calling model_fn.
W0618 12:12:54.934525 47953481937792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:54.935613 47017850205056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:54.935882 47953481937792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:54.941615 47850039665536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.941758 47996098794368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881574.836620 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.837356 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.838065 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.952176 47607712355200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.834002 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.834745 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.835449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.952356 47274562999168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.952920 47381443638144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.952993 47033984959360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:54.953180 47610369823616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:54.953402 47588786221952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.953321 47607712355200 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpegi8jbah
I0618 12:12:54.953472 47274562999168 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff401f2d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.954413 47607712355200 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpegi8jbah', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4cd15ebdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.954730 47274562999168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.954862 47607712355200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.957312 47033984959360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.957499 47610369823616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.960006 47274562999168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.960059 47607712355200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:54.962383 47033984959360 estimator.py:1111] Calling model_fn.
W0618 12:12:54.962490 47033984959360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:54.962576 47610369823616 estimator.py:1111] Calling model_fn.
W0618 12:12:54.962690 47610369823616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881574.875437 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.876168 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.876843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.962738 47501515748224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.865182 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.866139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.866992 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.962842 47130439631744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.963863 47033984959360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:54.964051 47610369823616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:54.963857 47501515748224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2gy45kvo
W0618 12:12:54.963942 47130439631744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyclo20a8
I0618 12:12:54.964997 47501515748224 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2gy45kvo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b34178f4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.965037 47130439631744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyclo20a8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2addb1b31e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.965458 47501515748224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.965499 47130439631744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.970754 47130439631744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.970785 47501515748224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.982014 47607712355200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.982085 47274562999168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881574.896961 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.897707 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.898414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.984956 47827390522240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.887375 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.888315 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.889183 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.985541 47168763487104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.986057 47827390522240 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl34vjosj
I0618 12:12:54.987172 47827390522240 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl34vjosj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ff735ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:54.986649 47168763487104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsaf55p8r
I0618 12:12:54.987667 47827390522240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.987777 47168763487104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsaf55p8r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae69dfabe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.988229 47168763487104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881574.884946 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.885456 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.885899 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.987319 47745673024384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.889189 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.889601 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.889971 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:54.987907 47826978145152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:54.988365 47745673024384 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvrzmy3qv
I0618 12:12:54.989417 47745673024384 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvrzmy3qv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6cf0779e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:54.989209 47850039665536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:54.989379 47996098794368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:54.988945 47826978145152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpysd6xdq8
I0618 12:12:54.989818 47745673024384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:54.989945 47826978145152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpysd6xdq8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7fdea14e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:54.990337 47826978145152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:54.992918 47827390522240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.993585 47168763487104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.993109 47130439631744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.993218 47501515748224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:54.993513 47850039665536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.993702 47996098794368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:54.994389 47745673024384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:54.994914 47826978145152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:54.998570 47850039665536 estimator.py:1111] Calling model_fn.
W0618 12:12:54.998679 47850039665536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:54.998795 47996098794368 estimator.py:1111] Calling model_fn.
W0618 12:12:54.998907 47996098794368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:55.000034 47850039665536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:55.000631 47588786221952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881574.904952 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.905726 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.906436 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:55.001766 47177662731136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:55.000805 47381443638144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881574.907812 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.908593 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.909288 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:55.001884 47734128210816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:55.000281 47996098794368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:55.002857 47734128210816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpoyal7xpz
W0618 12:12:55.002758 47177662731136 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1zqm7hb9
I0618 12:12:55.003958 47734128210816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpoyal7xpz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a4057cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:55.004013 47177662731136 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1zqm7hb9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8b06a6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:55.004396 47734128210816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:55.004456 47177662731136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:55.004941 47588786221952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:55.005147 47381443638144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:55.009150 47734128210816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:55.009147 47177662731136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:55.009990 47588786221952 estimator.py:1111] Calling model_fn.
W0618 12:12:55.010104 47588786221952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:55.010262 47381443638144 estimator.py:1111] Calling model_fn.
W0618 12:12:55.010374 47381443638144 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:55.011469 47588786221952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:55.011733 47381443638144 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:55.014867 47827390522240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:55.013602 47745673024384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:55.014268 47826978145152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881574.947277 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.947679 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.948026 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:55.014591 46933675971456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881574.947734 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881574.948140 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881574.948462 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:55.015008 47645151904640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:55.016847 47168763487104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:55.015632 46933675971456 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpy82nkt3c
I0618 12:12:55.016684 46933675971456 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpy82nkt3c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafe1abee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:55.016026 47645151904640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpt2issdb9
I0618 12:12:55.017045 47645151904640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpt2issdb9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5588f11e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:55.017086 46933675971456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:55.017445 47645151904640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:55.021644 46933675971456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:55.021975 47645151904640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:55.028450 47177662731136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:55.028515 47734128210816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be re[2019-06-18 12:13:34] divide_golden_chunk finished: 3.310 seconds
[2019-06-18 12:13:34] generate golden chunk: 3.324 seconds
[2019-06-18 12:13:34] moving /lfs/lfs12/gma_akey/results/epb015/models/000017-000010.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb
[2019-06-18 12:13:34] moving /lfs/lfs12/gma_akey/results/epb015/models/000017-000010.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000017-000011.meta
[2019-06-18 12:13:34] moving /lfs/lfs12/gma_akey/results/epb015/models/000017-000010.index --> /lfs/lfs12/gma_akey/results/epb015/models/000017-000011.index
[2019-06-18 12:13:34] moving /lfs/lfs12/gma_akey/results/epb015/models/000017-000010.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000017-000011.data-00000-of-00001
[2019-06-18 12:13:34] iteration time 16: 49.121 seconds
2019-06-18 12:13:35.428948: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
Got 343572 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000009.tfrecord.zz: 14.455 seconds
Got 377518 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz: 0.312 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000004.tfrecord.zz: 14.588 seconds
Got 380290 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000005.tfrecord.zz: 14.207 seconds
Got 347426 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz: 0.314 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000006.tfrecord.zz: 14.039 seconds
Got 391566 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000000.tfrecord.zz: 14.050 seconds
Got 382984 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000002.tfrecord.zz: 14.850 seconds
Got 348718 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000007.tfrecord.zz: 13.138 seconds
Got 345792 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000008.tfrecord.zz: 9.928 seconds
Got 389066 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000003.tfrecord.zz: 15.618 seconds
Got 388049 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz: 0.312 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000000-000001.tfrecord.zz: 15.038 seconds
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/checkpoint_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/checkpointlog.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000001-000001_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000001-000001log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000002-000002_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000002-000002log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000003-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000003-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000004-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000004-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000005-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000005-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000006-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000006-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000007-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000007-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000008-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000008-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000009-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000009-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000010-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000010-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000011-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000011-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000012-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000012-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000013-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000013-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000014-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000014-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000015-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000015-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000016-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000016-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000017-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000017-000011log.txt['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881614.406467 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:13:38] minmax time: 3.310 seconds
2019-06-18 12:13:38.749809: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:13:38.755332: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:13:38.759897: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881618.771496 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 16}}
[2019-06-18 12:13:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:13:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=18 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=1023779849 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=2047559680 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=3071339511 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=4095119342 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=5118899173 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=6142679004 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=7166458835 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=8190238666 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=9214018497 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=10237798328 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=11261578159 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=12285357990 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=13309137821 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=14332917652 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=15356697483 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=16380477314 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=17404257145 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=18428036976 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=19451816807 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000017-000011 --seed=20475596638 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:50] eval finished: 11.395 seconds
[2019-06-18 12:13:50] Win rate 000017-000011 vs 000016-000010: 0.510
:::MLL 1560881630.227322 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:13:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=19 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=1023779850 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=2047559681 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=3071339512 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=4095119343 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=5118899174 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=6142679005 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=7166458836 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=8190238667 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=9214018498 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=10237798329 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=11261578160 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=12285357991 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=13309137822 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=14332917653 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=15356697484 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=16380477315 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=17404257146 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000018-000010 --seed=18428036977 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:14:20] selfplay finished: 30.510 seconds
[2019-06-18 12:14:20] selfplay mn: 30.531 seconds
[2019-06-18 12:14:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-19-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779850 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559681 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339512 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119343 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899174 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679005 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458836 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238667 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018498 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798329 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578160 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357991 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137822 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917653 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697484 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477315 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257146 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036977 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816808 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596639 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376470 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156301 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:14:22] train finished: 43.645 seconds
:::MLL 1560881624.043957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.044681 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.045331 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.130954 47546555208576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.034312 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.035233 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.036001 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.131011 47166086378368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.132047 47546555208576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5d94mtu9
W0618 12:13:44.132077 47166086378368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpr5t0v04c
I0618 12:13:44.133034 47546555208576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5d94mtu9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3e941ede10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.133065 47166086378368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpr5t0v04c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5fe694e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.133444 47546555208576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.133468 47166086378368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.138252 47546555208576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.138247 47166086378368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881624.037686 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.038548 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.039373 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.145676 47497456018304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.042338 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.043067 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.043744 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.145736 47257404441472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.146804 47497456018304 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdbv7159u
W0618 12:13:44.146842 47257404441472 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp351vjwzt
I0618 12:13:44.147919 47497456018304 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdbv7159u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b332594bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.147954 47257404441472 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp351vjwzt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afb41643e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.148385 47497456018304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.148400 47257404441472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.153719 47497456018304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.153771 47257404441472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.159033 47546555208576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.159748 47166086378368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.175820 47497456018304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.176088 47257404441472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881624.113468 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.113974 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.114414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.194303 47425971295104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.123669 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.124074 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.124424 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.194453 47266328617856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.195318 47425971295104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpd461jo_m
W0618 12:13:44.195422 47266328617856 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxvgtit_f
I0618 12:13:44.196390 47425971295104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpd461jo_m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2280c26e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.196499 47266328617856 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxvgtit_f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd55506e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.196814 47425971295104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.196934 47266328617856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.201472 47425971295104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.201564 47266328617856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881624.080362 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.081231 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.082060 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.203005 47199313183616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.080457 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.081336 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.082145 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.203547 47344123044736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.104446 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.105369 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.106222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.204381 47109672252288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.204154 47199313183616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprd5iq_n6
I0618 12:13:44.205226 47199313183616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprd5iq_n6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aedbae21e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.204637 47344123044736 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0f72393cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.205681 47199313183616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.205922 47344123044736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.205512 47109672252288 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpl73vfc1n
I0618 12:13:44.206635 47109672252288 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpl73vfc1n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8dbde0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.207090 47109672252288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881624.132339 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.132704 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.133023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.208809 47525707158400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.129028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.129476 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.129865 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.208811 47745718010752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.209851 47525707158400 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw2z6gqs5
:::MLL 1560881624.122812 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.123653 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.124473 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.211126 47781795648384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.209883 47745718010752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnq_lm6ri
W0618 12:13:44.211052 47199313183616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:44.210843 47525707158400 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw2z6gqs5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39b97aee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.210865 47745718010752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnq_lm6ri', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6cf3261e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.211297 47344123044736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:44.211238 47525707158400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.211273 47745718010752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.211463 47546555208576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:44.212669 47109672252288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.212226 47781795648384 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprb3ytg6y
I0618 12:13:44.213363 47781795648384 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprb3ytg6y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75598b2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.212797 47166086378368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:13:44.213819 47781795648384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.215787 47546555208576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:44.215875 47525707158400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.215898 47745718010752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.217105 47166086378368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881624.145138 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.145962 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.146756 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.219729 47601911870336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.126169 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.127124 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.127996 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.219721 47505903588224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.219155 47781795648384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.220863 47601911870336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpwu_ug2e5
W0618 12:13:44.220830 47505903588224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpwkn6m0ps
W0618 12:13:44.220476 47425971295104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:44.221920 47505903588224 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpwkn6m0ps', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b351d184e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.221949 47601911870336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpwu_ug2e5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4b77a27e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.220702 47266328617856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:44.222325 47505903588224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.222356 47601911870336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.220895 47546555208576 estimator.py:1111] Calling model_fn.
W0618 12:13:44.221008 47546555208576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:44.222225 47166086378368 estimator.py:1111] Calling model_fn.
W0618 12:13:44.222336 47166086378368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:44.222372 47546555208576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:44.223706 47166086378368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:44.227158 47601911870336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.227161 47505903588224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.227532 47497456018304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:44.227567 47257404441472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:44.232184 47257404441472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:44.232173 47497456018304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:44.232810 47199313183616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.233289 47344123044736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.234663 47109672252288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.235001 47745718010752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.235127 47525707158400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:44.237677 47257404441472 estimator.py:1111] Calling model_fn.
I0618 12:13:44.237714 47497456018304 estimator.py:1111] Calling model_fn.
W0618 12:13:44.237792 47257404441472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:44.237825 47497456018304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:44.239251 47257404441472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:44.239281 47497456018304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881624.137370 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.137793 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.138148 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.240312 47045366727552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.139580 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.139986 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.140338 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.240500 46940530299776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.241612 47781795648384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.241338 47045366727552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzrh5_oed
W0618 12:13:44.241466 46940530299776 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpaiufflqs
I0618 12:13:44.242325 47045366727552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzrh5_oed', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac9e2f59e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.242449 46940530299776 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpaiufflqs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab17a389e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.242720 47045366727552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.242844 46940530299776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.246464 47601911870336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.246477 47505903588224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.247415 47045366727552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.247554 46940530299776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881624.153817 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.154790 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.155693 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.249142 47468893041536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.165283 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.166023 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.166726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.249426 47644698542976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.250276 47468893041536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppj7x2ypj
I0618 12:13:44.251410 47468893041536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppj7x2ypj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c7f184e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.250576 47644698542976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph4e4equt
I0618 12:13:44.251722 47644698542976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph4e4equt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b556deb4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.251855 47468893041536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.252189 47644698542976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881624.176695 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.177098 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.177449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.254616 47497317909376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881624.173217 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.173704 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.174145 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.254701 47672334607232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.257171 47468893041536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.255660 47497317909376 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpgcgv8k1u
W0618 12:13:44.255694 47672334607232 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpupyti1gy
I0618 12:13:44.256659 47497317909376 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpgcgv8k1u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b331d594e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.256674 47672334607232 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpupyti1gy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5bdd283e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.257670 47644698542976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:44.257057 47497317909376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:44.257079 47672334607232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.261772 47497317909376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.261764 47672334607232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:44.266480 47045366727552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.266627 46940530299776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.268015 47425971295104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:44.268455 47266328617856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:44.272313 47425971295104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:44.272772 47266328617856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881624.206641 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.207072 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.207457 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.278165 47053655131008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
I0618 12:13:44.277404 47425971295104 estimator.py:1111] Calling model_fn.
W0618 12:13:44.277514 47425971295104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:44.277840 47266328617856 estimator.py:1111] Calling model_fn.
W0618 12:13:44.277953 47266328617856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881624.209034 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881624.209410 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881624.209776 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:44.279274 46927702512512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:44.279257 47468893041536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.279201 47053655131008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6m0081_z
W0618 12:13:44.278876 47425971295104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:13:44.280178 47053655131008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6m0081_z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acbd0fc8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:44.279310 47266328617856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:13:44.280592 47053655131008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.280445 47644698542976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.280257 46927702512512 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7lgv6ghc
I0618 12:13:44.281235 46927702512512 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7lgv6ghc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae7da01e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:44.281637 46927702512512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:44.280777 47497317909376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:44.280860 47672334607232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel[2019-06-18 12:14:24] divide_golden_chunk finished: 3.325 seconds
[2019-06-18 12:14:24] generate golden chunk: 3.340 seconds
[2019-06-18 12:14:24] moving /lfs/lfs12/gma_akey/results/epb015/models/000018-000011.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000018-000012.meta
[2019-06-18 12:14:24] moving /lfs/lfs12/gma_akey/results/epb015/models/000018-000011.index --> /lfs/lfs12/gma_akey/results/epb015/models/000018-000012.index
[2019-06-18 12:14:24] moving /lfs/lfs12/gma_akey/results/epb015/models/000018-000011.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000018-000012.data-00000-of-00001
[2019-06-18 12:14:24] moving /lfs/lfs12/gma_akey/results/epb015/models/000018-000011.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb
[2019-06-18 12:14:24] iteration time 17: 49.737 seconds
2019-06-18 12:14:25.209715: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881664.143929 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:14:28] minmax time: 3.251 seconds
2019-06-18 12:14:28.471055: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:14:28.476564: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:14:28.481120: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881668.492498 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 17}}
[2019-06-18 12:14:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:14:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=19 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=1023779850 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=2047559681 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=3071339512 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=4095119343 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=5118899174 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=6142679005 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=7166458836 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=8190238667 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=9214018498 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=10237798329 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=11261578160 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=12285357991 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=13309137822 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=14332917653 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=15356697484 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=16380477315 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=17404257146 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=18428036977 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=19451816808 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000018-000012 --seed=20475596639 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:14:39] eval finished: 10.843 seconds
[2019-06-18 12:14:39] Win rate 000018-000012 vs 000017-000011: 0.460
:::MLL 1560881679.397934 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:14:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=20 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=1023779851 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=2047559682 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=3071339513 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=4095119344 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=5118899175 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=6142679006 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=7166458837 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=8190238668 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=9214018499 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=10237798330 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=11261578161 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=12285357992 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=13309137823 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=14332917654 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=15356697485 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=16380477316 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=17404257147 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000019-000011 --seed=18428036978 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:15:09] selfplay finished: 30.243 seconds
[2019-06-18 12:15:09] selfplay mn: 30.261 seconds
[2019-06-18 12:15:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-20-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779851 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559682 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339513 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119344 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899175 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679006 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458837 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238668 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018499 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798330 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578161 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357992 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137823 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917654 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697485 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477316 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257147 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036978 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816809 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596640 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376471 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156302 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:15:11] train finished: 43.453 seconds
:::MLL 1560881673.736939 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.737680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.738360 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.834610 47773771539328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.734928 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.735675 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.736434 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.834902 47871288148864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.835748 47773771539328 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw38u8lqd
W0618 12:14:33.836020 47871288148864 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpav_yzr04
I0618 12:14:33.836860 47773771539328 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw38u8lqd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b737b44de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.837161 47871288148864 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpav_yzr04', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a2fb65e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.837326 47773771539328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.837616 47871288148864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.842615 47773771539328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.842862 47871288148864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881673.743660 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.744573 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.745419 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.846460 47044218479488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.754790 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.755545 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.756270 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.846503 47373404484480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.847611 47044218479488 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4r3vq7qk
W0618 12:14:33.847645 47373404484480 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqomm9_gn
I0618 12:14:33.848707 47044218479488 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4r3vq7qk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac99e84ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.848756 47373404484480 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqomm9_gn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1643888e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.849153 47044218479488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.849214 47373404484480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.854441 47044218479488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.854540 47373404484480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.864784 47773771539328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.865359 47871288148864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.876478 47044218479488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.876655 47373404484480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881673.779926 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.780892 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.781795 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.878190 46987590607744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.798738 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.799577 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.800401 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.878336 47746261005184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.879310 46987590607744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpz4af8ci9
I0618 12:14:33.879483 47746261005184 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d13838d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.880392 46987590607744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpz4af8ci9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc6f3bddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.880756 47746261005184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.880840 46987590607744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.886058 46987590607744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.886146 47746261005184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881673.819877 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.820259 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.820588 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.892478 46958923699072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.817366 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.817743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.818067 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.892597 47591630975872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.893476 46958923699072 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpiuolkaob
W0618 12:14:33.893597 47591630975872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxro74p8p
I0618 12:14:33.894468 46958923699072 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpiuolkaob', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5c28dae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.894596 47591630975872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxro74p8p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4912d88e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.894869 46958923699072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.894995 47591630975872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881673.821102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.821572 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.821984 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.897084 47194084103040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.818604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.819014 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.819374 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.897279 47093860569984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.898113 47194084103040 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfakejdxr
W0618 12:14:33.898248 47093860569984 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpndriix31
I0618 12:14:33.899109 47194084103040 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfakejdxr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aec83349da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.899237 47093860569984 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpndriix31', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad52d6aeda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.899506 47194084103040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.899637 47093860569984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.899455 46958923699072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.899599 47591630975872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.904095 47194084103040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.904187 47093860569984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.907983 46987590607744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.908538 47746261005184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.914556 47871288148864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.914670 47773771539328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.918481 46958923699072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.918827 47591630975872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.918853 47871288148864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.919027 47773771539328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.923179 47194084103040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.923399 47093860569984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:14:33.923986 47871288148864 estimator.py:1111] Calling model_fn.
W0618 12:14:33.924104 47871288148864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:33.924209 47773771539328 estimator.py:1111] Calling model_fn.
W0618 12:14:33.924322 47773771539328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881673.852963 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.853416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.853815 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.924390 47559337108352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.833479 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.834206 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.834905 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.926243 47037825893248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.925431 47044218479488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881673.826527 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.827456 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.828303 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.926789 47221353120640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.925802 47373404484480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.925479 47871288148864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.925696 47773771539328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.925450 47559337108352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkvxwztrd
I0618 12:14:33.926440 47559337108352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkvxwztrd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b418dfb3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:33.927409 47037825893248 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnzymh65i
I0618 12:14:33.926840 47559337108352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.928479 47037825893248 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnzymh65i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8217d8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:33.927911 47221353120640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpso9but6e
I0618 12:14:33.928872 47037825893248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.928925 47221353120640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpso9but6e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af2dc90ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.929316 47221353120640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.929730 47044218479488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.930196 47373404484480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.931693 47559337108352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881673.853059 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.853518 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.853919 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.931654 47002506621824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.933583 47037825893248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.933897 47221353120640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.932613 47002506621824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzzvkh_ye
I0618 12:14:33.933586 47002506621824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzzvkh_ye', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abfe84c4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.933982 47002506621824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.935030 47044218479488 estimator.py:1111] Calling model_fn.
W0618 12:14:33.935137 47044218479488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:33.935461 47373404484480 estimator.py:1111] Calling model_fn.
W0618 12:14:33.935575 47373404484480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:33.936492 47044218479488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.936946 47373404484480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881673.827906 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.828840 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.829559 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.938298 47937378526080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.831800 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.832561 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.833220 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.938380 47843159577472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.938506 47002506621824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.939445 47937378526080 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmps21h4d7s
W0618 12:14:33.939476 47843159577472 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4q4r1yrd
I0618 12:14:33.940554 47937378526080 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmps21h4d7s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9993016e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.940576 47843159577472 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4q4r1yrd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b83a31e5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.940996 47937378526080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.941024 47843159577472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.946313 47937378526080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.946331 47843159577472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.951658 47559337108352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.953185 47037825893248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.953336 47221353120640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881673.856411 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.857332 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.858158 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.954161 47520298013568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881673.856848 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.857794 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.858574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.954337 47021507900288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.955308 47520298013568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_hhj6h63
W0618 12:14:33.955481 47021507900288 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprkn57q1r
I0618 12:14:33.956417 47520298013568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_hhj6h63', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b387711ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.956593 47021507900288 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprkn57q1r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac454dcce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:33.956866 47520298013568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:33.957040 47021507900288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:33.956837 46987590607744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.957068 47746261005184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.957447 47002506621824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.962109 47520298013568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.962246 47021507900288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:33.961140 46987590607744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.961371 47746261005184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:33.966212 46987590607744 estimator.py:1111] Calling model_fn.
W0618 12:14:33.966321 46987590607744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:33.966012 46958923699072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:14:33.966469 47746261005184 estimator.py:1111] Calling model_fn.
W0618 12:14:33.966576 47746261005184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:33.966493 47591630975872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.967672 46987590607744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.968040 47937378526080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.968172 47843159577472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:33.967937 47746261005184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.970929 47194084103040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.970339 46958923699072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.971328 47093860569984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:33.970818 47591630975872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.975256 47194084103040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:33.975657 47093860569984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:33.975415 46958923699072 estimator.py:1111] Calling model_fn.
W0618 12:14:33.975523 46958923699072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881673.897365 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.897842 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.898267 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.977540 47338036949888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
I0618 12:14:33.975948 47591630975872 estimator.py:1111] Calling model_fn.
W0618 12:14:33.976063 47591630975872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881673.892584 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881673.893143 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881673.893635 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:33.977888 46918919689088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:33.976875 46958923699072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.977432 47591630975872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:33.978560 47338036949888 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplwflu7v8
I0618 12:14:33.979559 47338036949888 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplwflu7v8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e0776be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': [2019-06-18 12:15:12] divide_golden_chunk finished: 3.325 seconds
[2019-06-18 12:15:13] generate golden chunk: 3.340 seconds
[2019-06-18 12:15:13] iteration time 18: 48.857 seconds
2019-06-18 12:15:14.113828: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881713.001278 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:15:17] minmax time: 3.334 seconds
2019-06-18 12:15:17.457843: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:15:17.463301: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:15:17.467874: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881717.480695 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 18}}
[2019-06-18 12:15:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:15:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=20 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=1023779851 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=2047559682 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=3071339513 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=4095119344 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=5118899175 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=6142679006 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=7166458837 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=8190238668 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=9214018499 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=10237798330 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=11261578161 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=12285357992 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=13309137823 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=14332917654 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=15356697485 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=16380477316 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=17404257147 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=18428036978 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=19451816809 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000019-000012 --seed=20475596640 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:15:28] eval finished: 11.352 seconds
[2019-06-18 12:15:28] Win rate 000019-000012 vs 000017-000011: 0.650
:::MLL 1560881728.894312 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:15:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=21 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=1023779852 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=2047559683 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=3071339514 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=4095119345 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=5118899176 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=6142679007 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=7166458838 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=8190238669 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=9214018500 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=10237798331 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=11261578162 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=12285357993 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=13309137824 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=14332917655 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=15356697486 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=16380477317 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=17404257148 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000020-000011 --seed=18428036979 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:15:58] selfplay finished: 29.855 seconds
[2019-06-18 12:15:58] selfplay mn: 29.872 seconds
[2019-06-18 12:15:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-21-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779852 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559683 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339514 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119345 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899176 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679007 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458838 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238669 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018500 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798331 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578162 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357993 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137824 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917655 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697486 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477317 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257148 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036979 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816810 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596641 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376472 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156303 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:16:01] train finished: 43.853 seconds
:::MLL 1560881722.668941 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.669647 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.670350 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.761760 47900343071616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.660257 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.661144 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.661945 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.761814 47637065937792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.762931 47900343071616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpml8i7jmo
W0618 12:15:22.762962 47637065937792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp99z1jbqx
I0618 12:15:22.764028 47900343071616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpml8i7jmo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90f3853e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.764076 47637065937792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp99z1jbqx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53a6fafe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.764478 47900343071616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.764520 47637065937792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.769863 47900343071616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.769921 47637065937792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.791941 47900343071616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.792115 47637065937792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881722.694630 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.695456 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.696177 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.794089 47737889530752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.693717 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.694539 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.695359 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.794174 47813510738816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.795278 47737889530752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7pzre_fx
W0618 12:15:22.795308 47813510738816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmx4xtuhk
I0618 12:15:22.796407 47737889530752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7pzre_fx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b2088fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.796452 47813510738816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmx4xtuhk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7cbbe90e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.796863 47737889530752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.796924 47813510738816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.802250 47737889530752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.802302 47813510738816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881722.739079 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.739573 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.739982 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.814813 47477899064192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.815833 47477899064192 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp52qvj2sz
I0618 12:15:22.816833 47477899064192 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp52qvj2sz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e97e54e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.817230 47477899064192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881722.745442 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.745879 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.746265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.818522 46937635328896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.819477 46937635328896 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6iustph9
I0618 12:15:22.820469 46937635328896 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6iustph9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab0cdaade10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.820864 46937635328896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.821942 47477899064192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.824349 47737889530752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.825019 47813510738816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.825458 46937635328896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.840906 47900343071616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.841279 47477899064192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.841405 47637065937792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.844704 46937635328896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.845171 47900343071616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.845718 47637065937792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:15:22.850191 47900343071616 estimator.py:1111] Calling model_fn.
W0618 12:15:22.850302 47900343071616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881722.749867 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.750757 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.751599 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.850534 47297439576960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.761431 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.762201 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.762910 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.850820 47993270702976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
I0618 12:15:22.850773 47637065937792 estimator.py:1111] Calling model_fn.
W0618 12:15:22.850885 47637065937792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:22.851662 47900343071616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:22.852245 47637065937792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:22.851668 47297439576960 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpwg2w4wdo
I0618 12:15:22.851933 47993270702976 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba696703cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.852767 47297439576960 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpwg2w4wdo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0493ac0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881722.767779 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.768202 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.768559 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.853070 47673148375936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.767014 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.767451 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.767850 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.853117 47741112775552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
I0618 12:15:22.853201 47993270702976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.853215 47297439576960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.854091 47673148375936 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa9uxba7_
W0618 12:15:22.854214 47741112775552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0g9pv66r
I0618 12:15:22.855083 47673148375936 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa9uxba7_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5c0da94da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.855196 47741112775552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0g9pv66r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6be0a7ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.855483 47673148375936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.855587 47741112775552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.858629 47993270702976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.858672 47297439576960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.860159 47673148375936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.860240 47741112775552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881722.761878 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.762816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.763516 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.862869 47137834800000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.765949 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.766706 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.767370 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.863025 47513311638400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.863991 47137834800000 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdj9bn97v
W0618 12:15:22.864118 47513311638400 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpu4_ypn2d
I0618 12:15:22.865093 47137834800000 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdj9bn97v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf6a7c6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.865294 47513311638400 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpu4_ypn2d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36d6a64e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.865637 47137834800000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.865832 47513311638400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.870949 47137834800000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.871037 47513311638400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.874272 47737889530752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.874406 47813510738816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.878609 47737889530752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.878702 47813510738816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.879302 47741112775552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.879313 47673148375936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.880855 47297439576960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.881074 47993270702976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:22.883657 47737889530752 estimator.py:1111] Calling model_fn.
I0618 12:15:22.883734 47813510738816 estimator.py:1111] Calling model_fn.
W0618 12:15:22.883768 47737889530752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:22.883844 47813510738816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:22.885126 47737889530752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:22.885192 47813510738816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:22.888883 47477899064192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.892011 46937635328896 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.893092 47137834800000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.893181 47513311638400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.893230 47477899064192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.896353 46937635328896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:15:22.898316 47477899064192 estimator.py:1111] Calling model_fn.
W0618 12:15:22.898424 47477899064192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:22.899790 47477899064192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:15:22.901429 46937635328896 estimator.py:1111] Calling model_fn.
W0618 12:15:22.901541 46937635328896 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:22.902909 46937635328896 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881722.817813 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.818224 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.818585 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.903886 47914777875328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.816765 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.817174 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.817554 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.904426 47898237404032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.904907 47914777875328 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp03adm2pc
I0618 12:15:22.905889 47914777875328 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp03adm2pc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b944fe6de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:22.905416 47898237404032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2r7rqhfh
I0618 12:15:22.906283 47914777875328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.906389 47898237404032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2r7rqhfh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9076035e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.906790 47898237404032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.910925 47914777875328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.911365 47898237404032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881722.814575 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.815392 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.816214 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.914199 47477247419264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.816530 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.817245 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.817979 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.914355 47066969895808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.831125 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.831801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.832292 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.913986 47972610474880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.831104 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.831785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.832285 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.914791 47944127980416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.915280 47477247419264 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmveul0gf
W0618 12:15:22.915444 47066969895808 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpb471uj9s
I0618 12:15:22.916409 47477247419264 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmveul0gf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e710dfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.916573 47066969895808 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpb471uj9s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aceea9bae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:22.915013 47972610474880 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpj6fcc6q6
I0618 12:15:22.916870 47477247419264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.916005 47972610474880 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpj6fcc6q6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba1c6fe5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.917019 47066969895808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:22.916402 47972610474880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.915751 47944127980416 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkxwpsh0i
I0618 12:15:22.916718 47944127980416 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkxwpsh0i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b254dfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.917111 47944127980416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.921114 47972610474880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.922177 47477247419264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.922256 47066969895808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.921677 47944127980416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.926863 47741112775552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.926920 47673148375936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881722.846852 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.847655 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.848476 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.929116 47265443783552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881722.832243 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881722.833158 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881722.834039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:22.929292 47057428304768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:22.930137 47265443783552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbll8c61a
W0618 12:15:22.930297 47057428304768 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkraa08dp
I0618 12:15:22.931156 47265443783552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbll8c61a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd2092ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:22.931281 47057428304768 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkraa08dp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2accb1e2ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:22.929793 47914777875328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:22.931629 47265443783552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.930109 47297439576960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:15:22.931772 47057428304768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:22.930228 47993270702976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:22.930511 47898237404032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:22.931216 47741112775552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.931322 47673148375936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.934434 47297439576960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.934568 47993270702976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:22.936821 47057428304768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:22.936838 47265443783552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:15:22.936371 47741112775552 estimator.py:1111] Calling model_fn.
W0618 12:15:22.936480 47741112775552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:[2019-06-18 12:16:02] divide_golden_chunk finished: 3.386 seconds
[2019-06-18 12:16:02] generate golden chunk: 3.401 seconds
[2019-06-18 12:16:02] moving /lfs/lfs12/gma_akey/results/epb015/models/000020-000012.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000020-000013.meta
[2019-06-18 12:16:02] moving /lfs/lfs12/gma_akey/results/epb015/models/000020-000012.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000020-000013.data-00000-of-00001
[2019-06-18 12:16:02] moving /lfs/lfs12/gma_akey/results/epb015/models/000020-000012.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb
[2019-06-18 12:16:02] moving /lfs/lfs12/gma_akey/results/epb015/models/000020-000012.index --> /lfs/lfs12/gma_akey/results/epb015/models/000020-000013.index
[2019-06-18 12:16:02] iteration time 19: 49.209 seconds
2019-06-18 12:16:03.355807: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881762.210657 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:16:06] minmax time: 3.245 seconds
2019-06-18 12:16:06.610992: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:16:06.616428: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:16:06.620975: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881766.632370 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 19}}
[2019-06-18 12:16:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:16:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=21 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=1023779852 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=2047559683 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=3071339514 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=4095119345 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=5118899176 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=6142679007 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=7166458838 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=8190238669 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=9214018500 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=10237798331 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=11261578162 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=12285357993 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=13309137824 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=14332917655 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=15356697486 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=16380477317 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=17404257148 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=18428036979 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=19451816810 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000020-000013 --seed=20475596641 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:16:17] eval finished: 11.240 seconds
[2019-06-18 12:16:17] Win rate 000020-000013 vs 000019-000012: 0.470
:::MLL 1560881777.935207 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:16:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=22 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=1023779853 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=2047559684 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=3071339515 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=4095119346 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=5118899177 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=6142679008 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=7166458839 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=8190238670 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=9214018501 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=10237798332 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=11261578163 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=12285357994 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=13309137825 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=14332917656 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=15356697487 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=16380477318 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=17404257149 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000021-000012 --seed=18428036980 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:16:48] selfplay finished: 30.406 seconds
[2019-06-18 12:16:48] selfplay mn: 30.427 seconds
[2019-06-18 12:16:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-22-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779853 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559684 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339515 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119346 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899177 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679008 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458839 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238670 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018501 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798332 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578163 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357994 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137825 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917656 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697487 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477318 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257149 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036980 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816811 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596642 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376473 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156304 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:16:50] train finished: 43.969 seconds
:::MLL 1560881771.850494 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.851343 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.852218 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:11.953799 47530787713920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.859358 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.860035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.860716 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:11.954162 47050645021568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:11.954904 47530787713920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpibaccxk7
W0618 12:16:11.955204 47050645021568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1gryvg19
I0618 12:16:11.956006 47530787713920 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpibaccxk7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3ae84e0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:11.956298 47050645021568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1gryvg19', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb1d91eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:11.956456 47530787713920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:11.956750 47050645021568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:11.961763 47530787713920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:11.961872 47050645021568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881771.875409 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.876213 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.877002 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:11.976428 47063842628480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.876796 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.877608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.878265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:11.976582 47624732423040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:11.977513 47063842628480 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkpalfbzi
W0618 12:16:11.977654 47624732423040 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfs4of0kk
I0618 12:16:11.978569 47063842628480 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkpalfbzi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace30357da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:11.978770 47624732423040 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfs4of0kk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b50c7d88e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:11.979024 47063842628480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:11.979208 47624732423040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:11.984271 47063842628480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:11.983731 47530787713920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:11.984431 47624732423040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:11.984035 47050645021568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.005993 47063842628480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.006353 47624732423040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881771.915143 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.915835 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.916509 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.011399 47724308394880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.907472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.908362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.909207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.011894 47814960890752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.012531 47724308394880 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpid6nx8d0
I0618 12:16:12.013643 47724308394880 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpid6nx8d0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b67f7093e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.012984 47814960890752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpanr8cxl0
I0618 12:16:12.014091 47814960890752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpanr8cxl0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d12589e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.014101 47724308394880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.014548 47814960890752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.019406 47724308394880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.019837 47814960890752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881771.941604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.941986 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.942310 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.021549 47321060709248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.944108 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.944486 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.944812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.022215 47177193362304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.022603 47321060709248 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpspm277qp
I0618 12:16:12.023663 47321060709248 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpspm277qp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0a1399de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.023245 47177193362304 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkzzzmqey
I0618 12:16:12.024071 47321060709248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.024244 47177193362304 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkzzzmqey', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae894706da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.024649 47177193362304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.028717 47321060709248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.029290 47177193362304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.034340 47530787713920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.034999 47050645021568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.038648 47530787713920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.039305 47050645021568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.041890 47724308394880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.042501 47814960890752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:16:12.043704 47530787713920 estimator.py:1111] Calling model_fn.
W0618 12:16:12.043814 47530787713920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881771.962661 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.963106 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.963452 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.044198 47122868343680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.960140 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.960526 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.960853 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.044292 47068253434752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
I0618 12:16:12.044389 47050645021568 estimator.py:1111] Calling model_fn.
W0618 12:16:12.044502 47050645021568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:12.045190 47530787713920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:12.045210 47122868343680 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpt4hv5ri0
W0618 12:16:12.045299 47068253434752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc27qzgag
I0618 12:16:12.046206 47122868343680 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpt4hv5ri0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adbee6a5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.046273 47068253434752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc27qzgag', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf371d0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.045873 47050645021568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:16:12.046607 47122868343680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.046668 47068253434752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.047871 47321060709248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.048614 47177193362304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.051232 47122868343680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.051272 47068253434752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881771.967001 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.967746 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.968463 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.052479 47756885050240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.951284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.952176 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.953027 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.052682 47440035509120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.053548 47756885050240 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjw272l04
W0618 12:16:12.053727 47440035509120 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp396q3666
I0618 12:16:12.054674 47756885050240 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjw272l04', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f8cc19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.054830 47440035509120 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp396q3666', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b25c70d4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.055115 47756885050240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.055270 47440035509120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.055947 47063842628480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.056415 47624732423040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.059873 47756885050240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.060014 47440035509120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.060248 47063842628480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.060756 47624732423040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:16:12.065329 47063842628480 estimator.py:1111] Calling model_fn.
W0618 12:16:12.065438 47063842628480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:12.065848 47624732423040 estimator.py:1111] Calling model_fn.
W0618 12:16:12.065960 47624732423040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:12.066815 47063842628480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:12.067325 47624732423040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881771.981957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.982564 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.983018 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.069136 47713826751360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.986271 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.986672 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.987022 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.069347 47232307614592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.070202 47122868343680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.070408 47068253434752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.070163 47713826751360 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpm3i771ud
W0618 12:16:12.070361 47232307614592 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf171bqhi
I0618 12:16:12.071164 47713826751360 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpm3i771ud', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6586481e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.071385 47232307614592 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf171bqhi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af569812dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.071567 47713826751360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.071783 47232307614592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881771.913791 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.914729 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.915598 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.074712 47875528475520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.918470 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.919228 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.919896 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.075561 46941740483456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.076269 47713826751360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.076442 47232307614592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.075867 47875528475520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpui52j_wu
I0618 12:16:12.076974 47875528475520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpui52j_wu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b2c749e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.077435 47875528475520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.076679 46941740483456 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab1c25a6d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.079328 47756885050240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:16:12.077919 46941740483456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.079782 47440035509120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.082744 47875528475520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.083158 46941740483456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881771.977004 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.977861 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.978554 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.087025 47032321991552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
:::MLL 1560881771.976408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.977247 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.978025 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.087424 47348798940032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.088121 47032321991552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmcx10dc6
I0618 12:16:12.089241 47032321991552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmcx10dc6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac6d96ebda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.088491 47348798940032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0o58kkqj
I0618 12:16:12.089572 47348798940032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0o58kkqj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1088edbda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.089707 47032321991552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.090013 47348798940032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.092392 47724308394880 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.092417 47814960890752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.095024 47032321991552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.095253 47348798940032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:12.095463 47713826751360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.095696 47232307614592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.095676 47321060709248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.096736 47814960890752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.096712 47724308394880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.096203 47177193362304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:12.099994 47321060709248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:12.100558 47177193362304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:16:12.101796 47814960890752 estimator.py:1111] Calling model_fn.
I0618 12:16:12.101812 47724308394880 estimator.py:1111] Calling model_fn.
W0618 12:16:12.101907 47814960890752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:12.101924 47724308394880 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:12.103282 47814960890752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:12.103301 47724308394880 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:12.103661 47875528475520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:12.104115 46941740483456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:16:12.105061 47321060709248 estimator.py:1111] Calling model_fn.
W0618 12:16:12.105174 47321060709248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:12.105661 47177193362304 estimator.py:1111] Calling model_fn.
W0618 12:16:12.105772 47177193362304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881772.026643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881772.027166 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881772.027658 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.108139 47714368402304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.106533 47321060709248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881771.966851 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.967305 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.967660 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.106860 47210055844736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.107153 47177193362304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881771.969661 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881771.970081 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881771.970437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.107770 47837301252992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
W0618 12:16:12.109143 47714368402304 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkxqrp_5c
I0618 12:16:12.110131 47714368402304 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkxqrp_5c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b65a6910e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:12.107849 47210055844736 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_hhb0ex_
:::MLL 1560881772.033052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881772.033504 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881772.033884 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:12.110055 47433408390016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000011-000005.tfrecord.zz_0_0
I0618 12:16:12.108849 47210055844736 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_hhb0ex_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af03b321e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:12.110541 47714368402304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:12.109242 47210055844736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:12.108732 47837301252992 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpp0qldkbp
I0618 12:16:12.109735 47837301252992 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpp0qldkbp', '_tf_random_seed': None, '_save_summary_steps': 1[2019-06-18 12:16:51] divide_golden_chunk finished: 3.338 seconds
[2019-06-18 12:16:51] generate golden chunk: 3.352 seconds
[2019-06-18 12:16:51] iteration time 20: 49.506 seconds
2019-06-18 12:16:52.855283: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881811.716390 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:16:56] minmax time: 3.237 seconds
2019-06-18 12:16:56.102172: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:16:56.107490: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:16:56.112051: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881816.124980 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 20}}
[2019-06-18 12:16:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:16:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=22 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=1023779853 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=2047559684 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=3071339515 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=4095119346 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=5118899177 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=6142679008 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=7166458839 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=8190238670 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=9214018501 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=10237798332 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=11261578163 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=12285357994 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=13309137825 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=14332917656 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=15356697487 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=16380477318 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=17404257149 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=18428036980 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=19451816811 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000021-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000021-000013 --seed=20475596642 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:17:06] eval finished: 10.396 seconds
[2019-06-18 12:17:06] Win rate 000021-000013 vs 000019-000012: 0.380
:::MLL 1560881826.584491 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:17:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=23 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=1023779854 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=2047559685 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=3071339516 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=4095119347 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=5118899178 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=6142679009 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=7166458840 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=8190238671 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=9214018502 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=10237798333 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=11261578164 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=12285357995 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=13309137826 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=14332917657 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=15356697488 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=16380477319 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=17404257150 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000022-000012 --seed=18428036981 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:17:36] selfplay finished: 30.267 seconds
[2019-06-18 12:17:36] selfplay mn: 30.284 seconds
[2019-06-18 12:17:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-23-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=23 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779854 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559685 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339516 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119347 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899178 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679009 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458840 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238671 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018502 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798333 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578164 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357995 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137826 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917657 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697488 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477319 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257150 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036981 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816812 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596643 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376474 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156305 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000022-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:17:39] train finished: 43.699 seconds
:::MLL 1560881821.376038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.376896 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.377666 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.461754 47893769978752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.343482 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.344394 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.345219 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.461867 48004319904640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.359267 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.360002 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.360666 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.461803 47759938954112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.355696 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.356576 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.357242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.461921 47446110131072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.462915 47893769978752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptai8kg3x
W0618 12:17:01.462993 48004319904640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2_cvla2b
I0618 12:17:01.464015 47893769978752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptai8kg3x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8f6bbbde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.464082 48004319904640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2_cvla2b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba92905de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:01.462936 47759938954112 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpatsxpi15
I0618 12:17:01.464440 47893769978752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.463072 47446110131072 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnbmocrsd
I0618 12:17:01.464505 48004319904640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.464033 47759938954112 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpatsxpi15', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7042c87e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.464252 47446110131072 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnbmocrsd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2731207e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.464481 47759938954112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.464746 47446110131072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.469792 48004319904640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.469796 47893769978752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.469755 47759938954112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.470018 47446110131072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.491613 48004319904640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.491784 47893769978752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.491590 47759938954112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.492686 47446110131072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881821.446924 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.447324 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.447663 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.522286 47033467945856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.523421 47033467945856 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp1yhlw593
I0618 12:17:01.524478 47033467945856 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp1yhlw593', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac71dbc8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.524910 47033467945856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881821.434325 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.434740 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.435116 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.524127 46944671363968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.435534 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.435948 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.436342 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.524157 47880193377152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.525134 46944671363968 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjtuepvtv
W0618 12:17:01.525163 47880193377152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsnrg6ypk
I0618 12:17:01.526129 46944671363968 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjtuepvtv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2710c3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.526143 47880193377152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsnrg6ypk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c42815e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.526534 46944671363968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.526533 47880193377152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.529953 47033467945856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.531183 46944671363968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.531179 47880193377152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881821.447571 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.448185 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.448558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.532288 47692133917568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.533283 47692133917568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdz_i9pxc
I0618 12:17:01.534268 47692133917568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdz_i9pxc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6079498e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881821.430003 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.430899 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.431755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.534010 47451291530112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.436894 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.437636 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.438361 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.534136 47700452172672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
I0618 12:17:01.534668 47692133917568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.535183 47451291530112 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6tdjfnor
I0618 12:17:01.535286 47700452172672 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6269182d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.536232 47451291530112 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6tdjfnor', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2865f67e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.536522 47700452172672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.536684 47451291530112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.539273 47692133917568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.541389 47759938954112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.541918 47451291530112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.541960 47700452172672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.543126 48004319904640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.543240 47893769978752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.543098 47446110131072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.545667 47759938954112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.547428 48004319904640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.547533 47893769978752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.547463 47446110131072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.549360 47033467945856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.550305 47880193377152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881821.446105 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.447043 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.447893 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.550808 47975304430464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.550379 46944671363968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:01.550729 47759938954112 estimator.py:1111] Calling model_fn.
W0618 12:17:01.550838 47759938954112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881821.451495 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.452268 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.452932 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.551398 48004929844096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
I0618 12:17:01.552508 48004319904640 estimator.py:1111] Calling model_fn.
W0618 12:17:01.551942 47975304430464 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyatyyyog
W0618 12:17:01.552624 48004319904640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:01.552640 47893769978752 estimator.py:1111] Calling model_fn.
W0618 12:17:01.552747 47893769978752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:01.552203 47759938954112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:17:01.553080 47975304430464 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyatyyyog', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba26790ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.552621 47446110131072 estimator.py:1111] Calling model_fn.
W0618 12:17:01.552510 48004929844096 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjh1qzj94
W0618 12:17:01.552729 47446110131072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:01.553547 47975304430464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.553627 48004929844096 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjh1qzj94', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba94d60ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.554089 48004929844096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.553996 48004319904640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.554100 47893769978752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.554111 47446110131072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.558573 47692133917568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.558831 47975304430464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.559239 48004929844096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.564043 47451291530112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.564170 47700452172672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881821.475225 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.476152 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.477036 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.580996 47084181951360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.484803 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.485549 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.486211 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.581080 47469569143680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.580567 47975304430464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.580967 48004929844096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.582139 47084181951360 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpplml4pln
W0618 12:17:01.582178 47469569143680 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpezx5k8of
I0618 12:17:01.583264 47084181951360 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpplml4pln', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad2ec86ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.583281 47469569143680 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpezx5k8of', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ca764bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.583717 47084181951360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.583733 47469569143680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.589020 47469569143680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.589082 47084181951360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881821.494522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.494941 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.495296 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.587921 47783749272448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.491349 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.491834 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.492320 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.588164 47050426397568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.588950 47783749272448 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp817jn61s
W0618 12:17:01.589159 47050426397568 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbd3hu22e
I0618 12:17:01.589945 47783749272448 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp817jn61s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75cdfd1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.590142 47050426397568 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbd3hu22e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb108a0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.590343 47783749272448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.590533 47050426397568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881821.500714 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.501433 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.502116 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.592958 47399061468032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.488987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.489926 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.490817 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.592955 47111758611328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.593956 47399061468032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc3hiew1c
W0618 12:17:01.593985 47111758611328 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7mplzoss
I0618 12:17:01.594983 47399061468032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc3hiew1c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1c3ccf0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.594982 47111758611328 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7mplzoss', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad958395e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:01.595393 47399061468032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:01.595427 47111758611328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:01.594955 47783749272448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.595102 47050426397568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.597139 47033467945856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.597848 47880193377152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.598180 46944671363968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.600406 47111758611328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.600418 47399061468032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:01.601475 47033467945856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.602157 47880193377152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.602530 46944671363968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.606039 47692133917568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:17:01.606572 47033467945856 estimator.py:1111] Calling model_fn.
W0618 12:17:01.606688 47033467945856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:01.607249 47880193377152 estimator.py:1111] Calling model_fn.
W0618 12:17:01.607358 47880193377152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:01.608075 47033467945856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:17:01.607589 46944671363968 estimator.py:1111] Calling model_fn.
W0618 12:17:01.607698 46944671363968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:01.608738 47880193377152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.609052 46944671363968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.610827 47469569143680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.611040 47084181951360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.610375 47692133917568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:01.613222 47700452172672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.613514 47451291530112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:01.614120 47783749272448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:01.614149 47050426397568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:01.615479 47692133917568 estimator.py:1111] Calling model_fn.
W0618 12:17:01.615592 47692133917568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881821.535578 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.535970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.536553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.616148 47914535056256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
:::MLL 1560881821.536567 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881821.536996 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881821.537325 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:01.616588 46946868196224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000012-000005.tfrecord.zz_0_0
W0618 12:17:01.616961 47692133917568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:01.617174 47914535056256 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpiwmvg_r7
W0618 12:17:01.617518 47700452172672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:17:01.618164 47914535056256 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpiwmvg_r7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94416dce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:01.617850 47451291530112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly f[2019-06-18 12:17:40] divide_golden_chunk finished: 3.438 seconds
[2019-06-18 12:17:40] generate golden chunk: 3.453 seconds
[2019-06-18 12:17:40] iteration time 21: 48.607 seconds
2019-06-18 12:17:41.561902: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881860.323328 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:17:44] minmax time: 3.254 seconds
2019-06-18 12:17:44.826586: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:17:44.832392: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:17:44.837202: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881864.849961 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 21}}
[2019-06-18 12:17:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:17:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=23 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=1023779854 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=2047559685 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=3071339516 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=4095119347 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=5118899178 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=6142679009 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=7166458840 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=8190238671 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=9214018502 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=10237798333 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=11261578164 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=12285357995 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=13309137826 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=14332917657 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=15356697488 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=16380477319 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=17404257150 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=18428036981 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=19451816812 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000022-000013 --seed=20475596643 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:17:56] eval finished: 11.285 seconds
[2019-06-18 12:17:56] Win rate 000022-000013 vs 000019-000012: 0.610
:::MLL 1560881876.197209 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:17:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=24 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=1023779855 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=2047559686 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=3071339517 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=4095119348 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=5118899179 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=6142679010 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=7166458841 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=8190238672 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=9214018503 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=10237798334 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=11261578165 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=12285357996 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=13309137827 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=14332917658 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=15356697489 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=16380477320 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=17404257151 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000023-000012 --seed=18428036982 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:18:25] selfplay finished: 29.396 seconds
[2019-06-18 12:18:25] selfplay mn: 29.414 seconds
[2019-06-18 12:18:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-24-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=24 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779855 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559686 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339517 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119348 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899179 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679010 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458841 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238672 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018503 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798334 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578165 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357996 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137827 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917658 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697489 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477320 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257151 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036982 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816813 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596644 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376475 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156306 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:18:28] train finished: 44.003 seconds
:::MLL 1560881870.208501 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.209210 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.209895 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.305593 47068696560512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.196363 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.197284 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.198158 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.306078 47015789323136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.306760 47068696560512 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpuxlabxk1
I0618 12:17:50.307874 47068696560512 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpuxlabxk1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf51869e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:50.307233 47015789323136 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvupz03_n
I0618 12:17:50.308340 47068696560512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.308350 47015789323136 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvupz03_n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac300020e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.308791 47015789323136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.313554 47068696560512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.313827 47015789323136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.335761 47068696560512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.335867 47015789323136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881870.241113 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.241852 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.242513 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.339529 47994737124224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.231942 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.232903 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.233713 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.339651 48001902125952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.340578 47994737124224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvm4znp86
W0618 12:17:50.340649 48001902125952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp51shnp3l
I0618 12:17:50.341580 47994737124224 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvm4znp86', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6edd83e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881870.173947 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.174827 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.175605 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.340137 47693802820480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
I0618 12:17:50.341622 48001902125952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp51shnp3l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba898e97e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.341974 47994737124224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881870.174087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.174936 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.175741 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.340534 47125400044416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
I0618 12:17:50.342019 48001902125952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.341279 47693802820480 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpk1zxl6t_
I0618 12:17:50.342362 47693802820480 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpk1zxl6t_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b60dcc32e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:50.341654 47125400044416 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyow_caqo
I0618 12:17:50.342789 47125400044416 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyow_caqo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adc85511e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.342817 47693802820480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.343238 47125400044416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.346892 47994737124224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.346906 48001902125952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.348169 47693802820480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.348482 47125400044416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.366081 48001902125952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.366140 47994737124224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.368912 47693802820480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.369493 47125400044416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881870.226821 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.227225 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.227581 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.370416 47056381793152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.371443 47056381793152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfbhqgjgn
I0618 12:17:50.372435 47056381793152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfbhqgjgn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc73822e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.372833 47056381793152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881870.228731 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.229139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.229489 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.372861 47888263926656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.279685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.280174 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.280601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.372202 47416905474944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.285625 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.286000 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.286325 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.372315 47924170916736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.373839 47888263926656 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpg1ht8358
W0618 12:17:50.373231 47416905474944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmb5umpfz
W0618 12:17:50.373322 47924170916736 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpez3o96_e
I0618 12:17:50.374230 47416905474944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmb5umpfz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b206464fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.374809 47888263926656 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpg1ht8358', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e238c1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.374314 47924170916736 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpez3o96_e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b967fc54da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.374629 47416905474944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.375200 47888263926656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.374710 47924170916736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.377499 47056381793152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.379729 47888263926656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.379247 47416905474944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.379308 47924170916736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881870.242028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.242913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.243799 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.382093 47207992382336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.249112 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.249842 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.250527 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.382199 46981358961536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.383260 47207992382336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpnruv68n3
I0618 12:17:50.383322 46981358961536 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abafbcc9d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.384349 47207992382336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpnruv68n3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aefc0341e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.384579 46981358961536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.384795 47207992382336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.386141 47015789323136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.386623 47068696560512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.389945 46981358961536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.390016 47207992382336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881870.273570 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.274444 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.275271 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.390357 47664634049408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.273820 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.274722 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.275537 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.390665 47234815148928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.390429 47015789323136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:50.390979 47068696560512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:50.391485 47664634049408 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppwp5ogcd
W0618 12:17:50.391767 47234815148928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpyco2f87k
I0618 12:17:50.392568 47664634049408 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppwp5ogcd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5a122b0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.392855 47234815148928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpyco2f87k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af5fef71e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.393010 47664634049408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.393297 47234815148928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.395465 47015789323136 estimator.py:1111] Calling model_fn.
W0618 12:17:50.395572 47015789323136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:50.396115 47068696560512 estimator.py:1111] Calling model_fn.
W0618 12:17:50.396689 47056381793152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.396222 47068696560512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881870.314140 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.314672 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.315136 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.397911 47396963558272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.314684 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.315210 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.315623 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.398208 47133603046272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.396925 47015789323136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:50.397590 47068696560512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:50.398268 47664634049408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.398522 47234815148928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.398941 47396963558272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6qh_lepc
W0618 12:17:50.399192 47133603046272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5xd_kcje
I0618 12:17:50.399923 47396963558272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6qh_lepc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1bbfc37e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:50.398796 47888263926656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:50.400164 47133603046272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5xd_kcje', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade6e40fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:50.398324 47416905474944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.398481 47924170916736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:50.400317 47396963558272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.400571 47133603046272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.404906 47396963558272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.405073 47133603046272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.411465 46981358961536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.411644 47207992382336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.414081 47994737124224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.414099 48001902125952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.416512 47693802820480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.417179 47125400044416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.418397 48001902125952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:50.418370 47994737124224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:50.420337 47664634049408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.420594 47234815148928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.420804 47693802820480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:50.421507 47125400044416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:17:50.423419 48001902125952 estimator.py:1111] Calling model_fn.
I0618 12:17:50.423437 47994737124224 estimator.py:1111] Calling model_fn.
W0618 12:17:50.423524 48001902125952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:50.423545 47994737124224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:50.423987 47396963558272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.424028 47133603046272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:50.424876 48001902125952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:50.424907 47994737124224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:17:50.425853 47693802820480 estimator.py:1111] Calling model_fn.
W0618 12:17:50.425964 47693802820480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:50.426589 47125400044416 estimator.py:1111] Calling model_fn.
:::MLL 1560881870.306548 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.306961 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.307307 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.426154 47887962997632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.426701 47125400044416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881870.307936 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.308353 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.308707 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.426225 47705413886848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.427322 47693802820480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:50.428053 47125400044416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:50.427182 47887962997632 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8mdjgzlc
W0618 12:17:50.427230 47705413886848 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp993k_04l
I0618 12:17:50.428156 47887962997632 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8mdjgzlc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e119c5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.428207 47705413886848 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp993k_04l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6390d5ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.428555 47887962997632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881870.329829 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.330567 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.331235 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.429513 47441937417088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
I0618 12:17:50.428601 47705413886848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881870.322995 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.323935 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.324827 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.429567 47943462359936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.430704 47943462359936 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7t1b2cgb
W0618 12:17:50.430739 47441937417088 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpw1_5470w
I0618 12:17:50.431822 47943462359936 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7t1b2cgb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9afda16e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.431860 47441937417088 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpw1_5470w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b26386a1da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.432276 47943462359936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:50.432315 47441937417088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.433199 47887962997632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.433244 47705413886848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.437594 47943462359936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:50.437609 47441937417088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881870.348186 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.348720 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.349170 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.442645 47737423889280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881870.353956 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881870.354460 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881870.354816 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:50.443441 47867273286528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000022-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:17:50.444099 47056381793152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:50.443684 47737423889280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp055zgmss
I0618 12:17:50.444684 47737423889280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp055zgmss', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b04c7de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.445086 47737423889280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.444431 47867273286528 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpspme0xek
I0618 12:17:50.445405 47867273286528 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpspme0xek', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8940686e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:50.445802 47867273286528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:50.446135 47888263926656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions statef[2019-06-18 12:18:28] divide_golden_chunk finished: 3.383 seconds
[2019-06-18 12:18:29] generate golden chunk: 3.398 seconds
[2019-06-18 12:18:29] moving /lfs/lfs12/gma_akey/results/epb015/models/000023-000013.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000023-000014.meta
[2019-06-18 12:18:29] moving /lfs/lfs12/gma_akey/results/epb015/models/000023-000013.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb
[2019-06-18 12:18:29] moving /lfs/lfs12/gma_akey/results/epb015/models/000023-000013.index --> /lfs/lfs12/gma_akey/results/epb015/models/000023-000014.index
[2019-06-18 12:18:29] moving /lfs/lfs12/gma_akey/results/epb015/models/000023-000013.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000023-000014.data-00000-of-00001
[2019-06-18 12:18:29] iteration time 22: 48.729 seconds
2019-06-18 12:18:30.283666: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881909.052005 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:18:33] minmax time: 3.237 seconds
2019-06-18 12:18:33.530987: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:18:33.536486: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:18:33.541067: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881913.552636 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 22}}
[2019-06-18 12:18:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:18:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=24 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=1023779855 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=2047559686 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=3071339517 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=4095119348 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=5118899179 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=6142679010 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=7166458841 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=8190238672 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=9214018503 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=10237798334 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=11261578165 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=12285357996 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=13309137827 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=14332917658 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=15356697489 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=16380477320 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=17404257151 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=18428036982 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=19451816813 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000023-000014 --seed=20475596644 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:18:43] eval finished: 10.046 seconds
[2019-06-18 12:18:43] Win rate 000023-000014 vs 000022-000013: 0.560
:::MLL 1560881923.669760 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:18:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=25 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=1023779856 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=2047559687 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=3071339518 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=4095119349 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=5118899180 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=6142679011 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=7166458842 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=8190238673 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=9214018504 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=10237798335 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=11261578166 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=12285357997 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=13309137828 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=14332917659 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=15356697490 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=16380477321 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=17404257152 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000024-000013 --seed=18428036983 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:19:12] selfplay finished: 29.180 seconds
[2019-06-18 12:19:12] selfplay mn: 29.198 seconds
[2019-06-18 12:19:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-25-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=25 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779856 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559687 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339518 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119349 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899180 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679011 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458842 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238673 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018504 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798335 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578166 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357997 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137828 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917659 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697490 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477321 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257152 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036983 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816814 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596645 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376476 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156307 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:19:16] divide_golden_chunk finished: 3.466 seconds
[2019-06-18 12:19:16] generate golden chunk: 3.481 seconds
[2019-06-18 12:19:17] train finished: 43.815 seconds
:::MLL 1560881918.883244 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.883966 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.884734 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:38.993717 47670910817152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881918.884510 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.885253 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.885899 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:38.993751 47248228615040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:38.994889 47670910817152 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpchmk9y2k
W0618 12:18:38.994920 47248228615040 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7ty9rj1g
I0618 12:18:38.996016 47248228615040 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7ty9rj1g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af91e785e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:38.996014 47670910817152 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpchmk9y2k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b884ade10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:38.996474 47248228615040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:38.996501 47670910817152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.001819 47248228615040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.001937 47670910817152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.023803 47248228615040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.024451 47670910817152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881918.945922 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.946840 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.947710 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.057054 47789291922304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881918.958632 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.959376 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.960031 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.057206 47098584863616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.058174 47789291922304 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmbzq3o5e
W0618 12:18:39.058323 47098584863616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpswbekxc4
I0618 12:18:39.059255 47789291922304 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmbzq3o5e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b77185b3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.059413 47098584863616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpswbekxc4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad64701ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881918.973860 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.974348 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.974669 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.058758 47136705143680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
I0618 12:18:39.059708 47789291922304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.059865 47098584863616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.059802 47136705143680 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpdzeyb3xx
I0618 12:18:39.060800 47136705143680 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpdzeyb3xx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf27272e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.061208 47136705143680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.065028 47789291922304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.065140 47098584863616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881918.973505 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.973908 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.974343 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.065001 47971137172352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.065835 47136705143680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.065972 47971137172352 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpik0ij2u8
I0618 12:18:39.066960 47971137172352 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpik0ij2u8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba16f2d7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.067354 47971137172352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.071866 47971137172352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.075621 47248228615040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.075975 47670910817152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.079925 47248228615040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.080297 47670910817152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.085186 47136705143680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:18:39.085267 47248228615040 estimator.py:1111] Calling model_fn.
W0618 12:18:39.085377 47248228615040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:18:39.085618 47670910817152 estimator.py:1111] Calling model_fn.
W0618 12:18:39.085729 47670910817152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881918.969741 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.970514 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.971370 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.086673 47621642879872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881918.971512 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.972212 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.972919 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.086872 47981558117248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.087146 47789291922304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.086762 47248228615040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.087306 47098584863616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.087085 47670910817152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.087774 47621642879872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsd982jwj
W0618 12:18:39.087946 47981558117248 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbrbt74vb
I0618 12:18:39.088863 47621642879872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsd982jwj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b500fb1ddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.089047 47981558117248 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbrbt74vb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba3dc508e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.089319 47621642879872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.089509 47981558117248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.091252 47971137172352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.094560 47621642879872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.094749 47981558117248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881918.955286 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.956041 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.956747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.101630 47029168931712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.102775 47029168931712 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmphoca_ajj
:::MLL 1560881918.957725 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881918.958462 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881918.959137 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.103535 47898152153984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
I0618 12:18:39.103902 47029168931712 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmphoca_ajj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac61d7eddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.104369 47029168931712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.104656 47898152153984 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9070ee8d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.105914 47898152153984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.109714 47029168931712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.111171 47898152153984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.116582 47621642879872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.116891 47981558117248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881919.027752 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.028241 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.028630 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.116972 47715901870976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881919.030943 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.031361 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.031746 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.117366 47758481306496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.118000 47715901870976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa5nxitbr
I0618 12:18:39.118977 47715901870976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa5nxitbr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6601f7ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:39.118362 47758481306496 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpku1wante
I0618 12:18:39.119351 47758481306496 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpku1wante', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6febe67e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.119373 47715901870976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.119751 47758481306496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.123997 47715901870976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.124362 47758481306496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.131355 47029168931712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881919.009474 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.010379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.011234 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.133347 47539051066240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881919.017434 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.018148 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.018839 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.133778 47110930756480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.132988 47898152153984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.132794 47136705143680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.134499 47539051066240 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp91jysvbg
I0618 12:18:39.135599 47539051066240 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp91jysvbg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3cd4d6bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:39.134928 47110930756480 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8n9qakq2
I0618 12:18:39.136070 47539051066240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.136095 47110930756480 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8n9qakq2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad926e11e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.136563 47110930756480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.137114 47789291922304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.137553 47098584863616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.137124 47136705143680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.138587 47971137172352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.141321 47539051066240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.141883 47110930756480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.141399 47789291922304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.141873 47098584863616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:18:39.142237 47136705143680 estimator.py:1111] Calling model_fn.
W0618 12:18:39.142344 47136705143680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:39.143074 47715901870976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.142925 47971137172352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.143522 47758481306496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.143716 47136705143680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881919.057965 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.058444 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.058853 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.145172 47303675913088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881919.058648 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.059110 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.059489 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.145743 47917593723776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
I0618 12:18:39.146487 47789291922304 estimator.py:1111] Calling model_fn.
W0618 12:18:39.146601 47789291922304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:39.146221 47303675913088 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpod6hius4
I0618 12:18:39.146976 47098584863616 estimator.py:1111] Calling model_fn.
W0618 12:18:39.147085 47098584863616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:18:39.147213 47303675913088 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpod6hius4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b060762ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:39.146725 47917593723776 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpun5mw7jf
I0618 12:18:39.147613 47303675913088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.147701 47917593723776 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpun5mw7jf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94f7bd5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881919.010423 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.010948 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.011444 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.147166 46983465644928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881919.015181 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.015618 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.015999 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.147504 46929955054464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
I0618 12:18:39.148116 47917593723776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.147976 47789291922304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.148456 47098584863616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:18:39.147999 47971137172352 estimator.py:1111] Calling model_fn.
W0618 12:18:39.148113 47971137172352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:39.148181 46983465644928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpt281jl9w
I0618 12:18:39.149168 46983465644928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpt281jl9w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb795e0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:39.148492 46929955054464 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpavya5mxt
I0618 12:18:39.149465 46929955054464 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpavya5mxt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaf03e32e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:39.149564 46983465644928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:39.149867 46929955054464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:39.149465 47971137172352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.152197 47303675913088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.152641 47917593723776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.154178 46983465644928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.154429 46929955054464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:39.163448 47539051066240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.164449 47110930756480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.166561 47981558117248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.166666 47621642879872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.170845 47981558117248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.170981 47621642879872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.171302 47303675913088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.171756 47917593723776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.173272 46983465644928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:39.173448 46929955054464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:18:39.175942 47981558117248 estimator.py:1111] Calling model_fn.
W0618 12:18:39.176062 47981558117248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:18:39.176109 47621642879872 estimator.py:1111] Calling model_fn.
W0618 12:18:39.176218 47621642879872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:39.177422 47981558117248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.177589 47621642879872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:39.179807 47029168931712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:39.180962 47898152153984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881919.084175 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.084611 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.084977 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.184702 47158642148224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881919.085467 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881919.085872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881919.086225 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:39.184895 47636763829120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:18:39.184219 47029168931712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:39.185729 47158642148224 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpb5blrj7s
W0618 12:18:39.185287 47898152153984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimat[2019-06-18 12:19:17] moving /lfs/lfs12/gma_akey/results/epb015/models/000024-000014.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000024-000015.meta
[2019-06-18 12:19:17] moving /lfs/lfs12/gma_akey/results/epb015/models/000024-000014.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000024-000015.data-00000-of-00001
[2019-06-18 12:19:17] moving /lfs/lfs12/gma_akey/results/epb015/models/000024-000014.index --> /lfs/lfs12/gma_akey/results/epb015/models/000024-000015.index
[2019-06-18 12:19:17] moving /lfs/lfs12/gma_akey/results/epb015/models/000024-000014.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb
[2019-06-18 12:19:17] iteration time 23: 48.382 seconds
2019-06-18 12:19:18.724690: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881957.434280 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:19:21] minmax time: 3.213 seconds
2019-06-18 12:19:21.947727: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:19:21.953277: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:19:21.957912: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881961.969538 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 23}}
[2019-06-18 12:19:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:19:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=25 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=1023779856 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=2047559687 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=3071339518 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=4095119349 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=5118899180 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=6142679011 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=7166458842 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=8190238673 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=9214018504 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=10237798335 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=11261578166 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=12285357997 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=13309137828 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=14332917659 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=15356697490 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=16380477321 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=17404257152 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=18428036983 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=19451816814 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000024-000015 --seed=20475596645 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:19:32] eval finished: 10.277 seconds
[2019-06-18 12:19:32] Win rate 000024-000015 vs 000023-000014: 0.410
:::MLL 1560881972.306658 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:19:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=26 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=1023779857 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=2047559688 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=3071339519 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=4095119350 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=5118899181 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=6142679012 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=7166458843 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=8190238674 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=9214018505 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=10237798336 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=11261578167 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=12285357998 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=13309137829 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=14332917660 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=15356697491 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=16380477322 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=17404257153 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000025-000014 --seed=18428036984 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:20:02] selfplay finished: 29.693 seconds
[2019-06-18 12:20:02] selfplay mn: 29.714 seconds
[2019-06-18 12:20:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-26-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=26 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779857 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559688 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339519 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119350 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899181 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679012 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458843 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238674 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018505 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798336 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578167 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357998 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137829 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917660 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697491 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477322 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257153 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036984 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816815 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596646 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376477 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156308 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:20:05] divide_golden_chunk finished: 3.321 seconds
[2019-06-18 12:20:05] generate golden chunk: 3.335 seconds
[2019-06-18 12:20:05] train finished: 43.791 seconds
:::MLL 1560881967.284231 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.285175 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.285966 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.395377 47442542801792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.396525 47442542801792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpoa_i3wwx
I0618 12:19:27.397623 47442542801792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpoa_i3wwx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b265c7f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.398074 47442542801792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.403333 47442542801792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881967.321899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.322771 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.323552 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.408869 47462201246592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.409968 47462201246592 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjfqudzuz
I0618 12:19:27.411060 47462201246592 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjfqudzuz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2af03b9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.411523 47462201246592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.416778 47462201246592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881967.312307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.313160 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.313984 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.423251 47063948669824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.317169 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.317907 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.318565 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.423459 47046286222208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.424379 47063948669824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxhjdpa4w
W0618 12:19:27.424564 47046286222208 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmphnzzxxo2
I0618 12:19:27.425493 47063948669824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxhjdpa4w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace36878e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:27.425028 47442542801792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:19:27.425703 47046286222208 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmphnzzxxo2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca19c3fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.425947 47063948669824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.426158 47046286222208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.431190 47063948669824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.431384 47046286222208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.438932 47462201246592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.452877 47063948669824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.453391 47046286222208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881967.353397 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.354070 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.354747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.460324 47228229223296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.348436 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.349346 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.350149 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.460410 47191809721216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.461422 47228229223296 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6e049uoi
W0618 12:19:27.461488 47191809721216 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpy9xz7my4
I0618 12:19:27.462484 47228229223296 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6e049uoi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af47669de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.462549 47191809721216 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpy9xz7my4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aebfba45dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.462913 47228229223296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.462974 47191809721216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.468049 47228229223296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.468058 47191809721216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.476328 47442542801792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881967.385545 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.385956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.386574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.477564 47308322702208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.385412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.385810 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.386211 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.477608 47964243276672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.478600 47308322702208 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpodrpoqlt
W0618 12:19:27.478629 47964243276672 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpqacy8a4y
I0618 12:19:27.479604 47308322702208 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpodrpoqlt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b071c5b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.479622 47964243276672 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpqacy8a4y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9fd4450e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.480002 47308322702208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.480009 47964243276672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.480614 47442542801792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881967.388805 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.389219 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.389585 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.482066 47762690282368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.390558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.391010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.391384 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.482239 47984848581504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.483093 47762690282368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp57a0tpg9
W0618 12:19:27.483210 47984848581504 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_e1vjowu
I0618 12:19:27.484097 47762690282368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp57a0tpg9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70e6c66e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.484208 47984848581504 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_e1vjowu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba4a0710e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.484493 47762690282368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.484610 47984848581504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.484664 47964243276672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.484684 47308322702208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881967.388046 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.388742 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.389426 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.485608 47368925479808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.372832 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.373754 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.374639 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.485698 47231610041216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
I0618 12:19:27.485662 47442542801792 estimator.py:1111] Calling model_fn.
W0618 12:19:27.485770 47442542801792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.487135 47442542801792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:19:27.486744 47368925479808 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1538904d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:27.486814 47231610041216 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_hapgfr9
I0618 12:19:27.487938 47231610041216 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_hapgfr9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af53fed0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.488022 47368925479808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.488390 47231610041216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.488378 47462201246592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.489172 47762690282368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.489192 47984848581504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.489431 47228229223296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.489735 47191809721216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.492644 47462201246592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.493301 47368925479808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.493585 47231610041216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:27.497679 47462201246592 estimator.py:1111] Calling model_fn.
W0618 12:19:27.497786 47462201246592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.499152 47462201246592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.501983 47063948669824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.502259 47046286222208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.503562 47964243276672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.503642 47308322702208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.506273 47063948669824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.506566 47046286222208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.508162 47984848581504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.508206 47762690282368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:19:27.511312 47063948669824 estimator.py:1111] Calling model_fn.
W0618 12:19:27.511423 47063948669824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:27.511658 47046286222208 estimator.py:1111] Calling model_fn.
W0618 12:19:27.511766 47046286222208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.512783 47063948669824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.513139 47046286222208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.515053 47368925479808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.515325 47231610041216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881967.420472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.421398 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.422296 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.530374 47310690812800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.434339 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.435067 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.435770 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.531085 47809862366080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.531526 47310690812800 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpv5kthotq
I0618 12:19:27.532639 47310690812800 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpv5kthotq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b07a981de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:27.532227 47809862366080 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppowcj9ci
I0618 12:19:27.533091 47310690812800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.533316 47809862366080 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppowcj9ci', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7be2734e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.533767 47809862366080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.538353 47310690812800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.538893 47809862366080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881967.440454 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.440860 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.441213 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.538716 46976931582848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.440365 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.440769 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.441118 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.538738 47259296879488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.539769 46976931582848 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx3ewrrq5
W0618 12:19:27.539737 47259296879488 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprpsuv1qa
I0618 12:19:27.540746 47259296879488 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprpsuv1qa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afbb230ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.540779 46976931582848 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx3ewrrq5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab9f3e81e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.541148 47259296879488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.541138 47228229223296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:19:27.541180 46976931582848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.541516 47191809721216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881967.435449 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.435960 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.436322 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.542181 47863448802176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.438544 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.438973 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.439361 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.542499 47506926175104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.543213 47863448802176 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvwgu5irr
W0618 12:19:27.543469 47506926175104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkcvco8ky
I0618 12:19:27.544213 47863448802176 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvwgu5irr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b885c736e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.544462 47506926175104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkcvco8ky', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b355a0bddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:27.544614 47863448802176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.544865 47506926175104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:27.545444 47228229223296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.545824 47191809721216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.545807 46976931582848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.545787 47259296879488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.549269 47863448802176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:27.549409 47506926175104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:27.550700 47228229223296 estimator.py:1111] Calling model_fn.
W0618 12:19:27.550808 47228229223296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:27.551096 47191809721216 estimator.py:1111] Calling model_fn.
W0618 12:19:27.551206 47191809721216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.550810 47964243276672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.551099 47308322702208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.552183 47228229223296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.552569 47191809721216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.555114 47964243276672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.555675 47984848581504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.555405 47308322702208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.556066 47762690282368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.560098 47310690812800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.559965 47984848581504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.561090 47809862366080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.560412 47762690282368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:19:27.560159 47964243276672 estimator.py:1111] Calling model_fn.
W0618 12:19:27.560266 47964243276672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:27.560490 47308322702208 estimator.py:1111] Calling model_fn.
W0618 12:19:27.560600 47308322702208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.561619 47964243276672 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.561954 47308322702208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.564924 47259296879488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.565070 46976931582848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:19:27.565035 47984848581504 estimator.py:1111] Calling model_fn.
W0618 12:19:27.565143 47984848581504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:27.565520 47762690282368 estimator.py:1111] Calling model_fn.
W0618 12:19:27.565636 47762690282368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.565892 47368925479808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.565980 47231610041216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:27.566519 47984848581504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.566992 47762690282368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.568240 47863448802176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.568414 47506926175104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:27.570337 47368925479808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:27.570466 47231610041216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:19:27.575699 47368925479808 estimator.py:1111] Calling model_fn.
W0618 12:19:27.575819 47368925479808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:27.575803 47231610041216 estimator.py:1111] Calling model_fn.
W0618 12:19:27.575918 47231610041216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:27.577286 47368925479808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:27.577394 47231610041216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881967.506291 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.506673 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.506994 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.591089 47645698683776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881967.508139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881967.508512 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881967.508826 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:27.591798 47273072337792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:19:27.592130 47645698683776 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpcgzj_42k
I0618 12:19:27.593125 47645698683776 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpcgzj_42k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55a9884da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:27.592796 47273072337792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpiuxrkewa
I0618 12:19:27.593535 47645698683776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:27.593755 47273072337792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpiuxrkewa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afee7457e10>, '_task_type': 'worker', '_task_id': 0, '_glo[2019-06-18 12:20:05] iteration time 24: 48.348 seconds
2019-06-18 12:20:07.156028: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882005.782096 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:20:10] minmax time: 3.235 seconds
2019-06-18 12:20:10.401929: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:20:10.407539: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:20:10.412159: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882010.425286 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 24}}
[2019-06-18 12:20:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:20:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=26 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=1023779857 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=2047559688 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=3071339519 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=4095119350 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=5118899181 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=6142679012 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=7166458843 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=8190238674 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=9214018505 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=10237798336 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=11261578167 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=12285357998 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=13309137829 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=14332917660 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=15356697491 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=16380477322 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=17404257153 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=18428036984 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=19451816815 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000025-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000025-000015 --seed=20475596646 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:20:21] eval finished: 10.589 seconds
[2019-06-18 12:20:21] Win rate 000025-000015 vs 000023-000014: 0.370
:::MLL 1560882021.077461 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:20:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=27 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=1023779858 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=2047559689 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=3071339520 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=4095119351 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=5118899182 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=6142679013 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=7166458844 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=8190238675 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=9214018506 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=10237798337 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=11261578168 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=12285357999 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=13309137830 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=14332917661 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=15356697492 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=16380477323 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=17404257154 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000026-000014 --seed=18428036985 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:20:50] selfplay finished: 29.536 seconds
[2019-06-18 12:20:50] selfplay mn: 29.555 seconds
[2019-06-18 12:20:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-27-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=27 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779858 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559689 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339520 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119351 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899182 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679013 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458844 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238675 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018506 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798337 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578168 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285357999 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137830 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917661 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697492 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477323 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257154 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036985 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816816 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596647 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376478 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156309 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000026-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:20:53] divide_golden_chunk finished: 3.246 seconds
[2019-06-18 12:20:53] generate golden chunk: 3.261 seconds
[2019-06-18 12:20:54] train finished: 43.613 seconds
:::MLL 1560882015.703833 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.704663 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.705344 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.820382 47371204866944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.707316 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.708076 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.708720 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.820792 47245871743872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.821512 47371204866944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpu4ocguwq
I0618 12:20:15.822620 47371204866944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpu4ocguwq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b15c06cfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:20:15.821900 47245871743872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpus57lo63
I0618 12:20:15.823033 47245871743872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpus57lo63', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af891fd5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.823067 47371204866944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:15.823478 47245871743872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.828379 47371204866944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.828741 47245871743872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882015.690895 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.691743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.692558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.845649 47506559783808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.846786 47506559783808 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_qurmp_q
I0618 12:20:15.847858 47506559783808 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_qurmp_q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3544351e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.848322 47506559783808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.850266 47371204866944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.850821 47245871743872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882015.690895 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.691749 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.692564 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.850148 47103347757952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.851245 47103347757952 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpal2ks5hm
I0618 12:20:15.852337 47103347757952 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpal2ks5hm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad762e5de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.852786 47103347757952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.853663 47506559783808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.857977 47103347757952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.875590 47506559783808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882015.780458 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.780869 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.781234 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.879238 47260039254912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.879744 47103347757952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.880328 47260039254912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp938mq5j3
I0618 12:20:15.881316 47260039254912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp938mq5j3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afbde706e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.881724 47260039254912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882015.781983 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.782398 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.782766 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.884058 47556721787776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.885061 47556721787776 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsemnsqvk
I0618 12:20:15.886047 47556721787776 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsemnsqvk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40f2189da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.886438 47556721787776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.886483 47260039254912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882015.779104 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.779834 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.780536 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.886394 47200249725824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.771093 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.771989 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.772838 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.886641 47510272136064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.887547 47200249725824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp46qjbex2
W0618 12:20:15.887760 47510272136064 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp89czu4dy
I0618 12:20:15.888660 47200249725824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp46qjbex2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aedf2b48e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.888846 47510272136064 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp89czu4dy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36217b2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.889114 47200249725824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:15.889290 47510272136064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.891046 47556721787776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882015.757848 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.758345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.758778 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.891830 47250796684160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.761718 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.762159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.762538 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.892736 47488836539264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.892860 47250796684160 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkmdharsv
W0618 12:20:15.894467 47200249725824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:20:15.893862 47250796684160 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkmdharsv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9b789fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:20:15.894522 47510272136064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:20:15.894270 47250796684160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.893706 47488836539264 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa6pm3v0r
I0618 12:20:15.894704 47488836539264 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa6pm3v0r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3123d1ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.895111 47488836539264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.898940 47250796684160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.899750 47488836539264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.901355 47371204866944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.901433 47245871743872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.905497 47260039254912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.905725 47371204866944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.905793 47245871743872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.910245 47556721787776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:20:15.910834 47371204866944 estimator.py:1111] Calling model_fn.
I0618 12:20:15.910918 47245871743872 estimator.py:1111] Calling model_fn.
W0618 12:20:15.910944 47371204866944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.911024 47245871743872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.912390 47371204866944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.912466 47245871743872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.916398 47510272136064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.916489 47200249725824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.918631 47250796684160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.919147 47488836539264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.923604 47506559783808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.927501 47103347757952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.927905 47506559783808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.931828 47103347757952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:20:15.932977 47506559783808 estimator.py:1111] Calling model_fn.
W0618 12:20:15.933091 47506559783808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.934457 47506559783808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:20:15.936903 47103347757952 estimator.py:1111] Calling model_fn.
W0618 12:20:15.937009 47103347757952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.938378 47103347757952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882015.851911 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.852405 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.852843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.948519 47047675343744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.855815 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.856224 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.856572 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.948592 46928148202368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.949540 47047675343744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxcgbk3da
W0618 12:20:15.949596 46928148202368 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_yzjjol5
I0618 12:20:15.950539 47047675343744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxcgbk3da', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca6c904e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.950580 46928148202368 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_yzjjol5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae9830ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.950934 47047675343744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:15.950974 46928148202368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.952938 47260039254912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.955610 47047675343744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.955685 46928148202368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.957240 47260039254912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.957879 47556721787776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.962206 47556721787776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:20:15.962284 47260039254912 estimator.py:1111] Calling model_fn.
W0618 12:20:15.962392 47260039254912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.963756 47260039254912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.966494 47510272136064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.966748 47200249725824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.966434 47250796684160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:15.966550 47488836539264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:20:15.967289 47556721787776 estimator.py:1111] Calling model_fn.
W0618 12:20:15.967399 47556721787776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882015.800246 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.801180 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.801999 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.968957 47004700951424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.800167 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.801073 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.801909 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.969480 47872047838080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.968769 47556721787776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.970003 47004700951424 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpaj_23df6
I0618 12:20:15.970995 47004700951424 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpaj_23df6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac06b170e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:20:15.970471 47872047838080 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5lpc0xjl
I0618 12:20:15.971400 47004700951424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:15.971458 47872047838080 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5lpc0xjl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a5cfe4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.971853 47872047838080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.970815 47510272136064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.971103 47200249725824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.970784 47250796684160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:15.970880 47488836539264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882015.837846 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.838571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.839254 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.973555 47421014819712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560882015.836224 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.836962 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.837813 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.973630 47039706567552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.974707 47047675343744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.974729 47421014819712 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpj4t1xbp7
W0618 12:20:15.974762 47039706567552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxt36qcmk
W0618 12:20:15.974761 46928148202368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:20:15.975851 47421014819712 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpj4t1xbp7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2159549e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.975865 47039706567552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxt36qcmk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac891966e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:20:15.976300 47004700951424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:20:15.976314 47421014819712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:15.976309 47039706567552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.976599 47872047838080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:20:15.975887 47510272136064 estimator.py:1111] Calling model_fn.
W0618 12:20:15.975996 47510272136064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:20:15.976219 47200249725824 estimator.py:1111] Calling model_fn.
W0618 12:20:15.976327 47200249725824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:20:15.975942 47488836539264 estimator.py:1111] Calling model_fn.
I0618 12:20:15.975901 47250796684160 estimator.py:1111] Calling model_fn.
W0618 12:20:15.976012 47250796684160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.976050 47488836539264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:15.977349 47510272136064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.977705 47200249725824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.977409 47488836539264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:15.977388 47250796684160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882015.779551 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.780420 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.781262 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.977797 47800604816256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
I0618 12:20:15.978990 47800604816256 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b79baa84d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.980241 47800604816256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.981669 47421014819712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.981676 47039706567552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:15.985706 47800604816256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882015.780339 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882015.781241 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882015.781950 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:15.990089 47341802034048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:20:15.991214 47341802034048 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_h_7kujt
I0618 12:20:15.992296 47341802034048 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_h_7kujt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0ee7e16e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:15.992736 47341802034048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:15.995274 47004700951424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.996047 47872047838080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:15.997845 47341802034048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:16.003440 47421014819712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:16.003488 47039706567552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:16.008018 47800604816256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882015.858882 global_[2019-06-18 12:20:54] iteration time 25: 48.278 seconds
2019-06-18 12:20:55.455062: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882054.060083 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:20:58] minmax time: 3.240 seconds
2019-06-18 12:20:58.704992: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:20:58.710543: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:20:58.715141: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882058.728377 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 25}}
[2019-06-18 12:20:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000027-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:20:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=27 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=1023779858 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=2047559689 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=3071339520 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=4095119351 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=5118899182 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=6142679013 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=7166458844 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=8190238675 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=9214018506 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=10237798337 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=11261578168 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=12285357999 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=13309137830 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=14332917661 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=15356697492 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=16380477323 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=17404257154 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=18428036985 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=19451816816 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000026-000015 --seed=20475596647 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:21:09] eval finished: 11.181 seconds
[2019-06-18 12:21:09] Win rate 000026-000015 vs 000023-000014: 0.770
:::MLL 1560882069.971718 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:21:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=28 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=1023779859 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=2047559690 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=3071339521 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=4095119352 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=5118899183 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=6142679014 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=7166458845 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=8190238676 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=9214018507 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=10237798338 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=11261578169 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=12285358000 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=13309137831 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=14332917662 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=15356697493 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=16380477324 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=17404257155 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000027-000014 --seed=18428036986 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:21:40] selfplay finished: 30.574 seconds
[2019-06-18 12:21:40] selfplay mn: 30.593 seconds
[2019-06-18 12:21:40] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-28-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=28 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779859 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559690 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339521 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119352 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899183 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679014 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458845 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238676 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018507 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798338 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578169 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285358000 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137831 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917662 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697493 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477324 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257155 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036986 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816817 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596648 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376479 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156310 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000027-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:21:42] train finished: 43.648 seconds
:::MLL 1560882063.977767 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882063.978612 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882063.979272 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.093713 47085354640256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882063.980983 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882063.981718 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882063.982372 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.094010 47711729537920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.094814 47085354640256 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplia2kxdm
W0618 12:21:04.095125 47711729537920 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8yqxzfn3
I0618 12:21:04.095880 47085354640256 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplia2kxdm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad3326cada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.096159 47711729537920 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8yqxzfn3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6509471e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.096315 47085354640256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:04.096583 47711729537920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.101529 47085354640256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.101780 47711729537920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882063.996507 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882063.997471 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882063.998342 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.110743 46943144018816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.111845 46943144018816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpxix5lwhf
I0618 12:21:04.112883 46943144018816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpxix5lwhf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab21602de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.113314 46943144018816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882064.023328 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.024153 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.024918 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.114942 47656533762944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.115991 47656533762944 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbb3n1udd
I0618 12:21:04.117043 47656533762944 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbb3n1udd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b582f5a7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.117491 47656533762944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.118549 46943144018816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.122695 47656533762944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.123167 47085354640256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.123676 47711729537920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.140350 46943144018816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.144366 47656533762944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882064.062835 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.063263 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.063626 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.168470 47472606270336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882064.064472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.064895 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.065262 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.168497 47275543339904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.169550 47472606270336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfytaao6m
W0618 12:21:04.169514 47275543339904 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmnvjghkr
I0618 12:21:04.170506 47275543339904 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmnvjghkr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff7a8dfda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.170537 47472606270336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfytaao6m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2d5c6b9da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.170902 47275543339904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:04.170936 47472606270336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.173627 47085354640256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.174388 47711729537920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.175579 47472606270336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.175587 47275543339904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882064.056477 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.057188 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.057873 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.176558 47072374453120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882064.051572 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.052502 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.053395 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.176762 47117821424512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.177695 47072374453120 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppno3f4rq
W0618 12:21:04.177877 47117821424512 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpe6ug2pto
W0618 12:21:04.177938 47085354640256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:04.178787 47072374453120 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppno3f4rq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad02cbebe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.178969 47117821424512 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpe6ug2pto', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adac1987dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.179238 47072374453120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.178714 47711729537920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:04.179433 47117821424512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:04.182983 47085354640256 estimator.py:1111] Calling model_fn.
W0618 12:21:04.183097 47085354640256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:04.183782 47711729537920 estimator.py:1111] Calling model_fn.
W0618 12:21:04.183895 47711729537920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.184585 47072374453120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.184706 47117821424512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.184449 47085354640256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.185257 47711729537920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882064.094484 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.094915 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.095248 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.186457 47757207630720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882064.093831 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.094366 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.094741 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.186666 47552580060032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.187484 47757207630720 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa5umbjsx
W0618 12:21:04.187657 47552580060032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpi6o8rhgm
I0618 12:21:04.188462 47757207630720 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa5umbjsx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f9ffbbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.188644 47552580060032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpi6o8rhgm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3ffb3ade10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.188860 47757207630720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:04.189048 47552580060032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.191225 46943144018816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.193509 47757207630720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.193635 47552580060032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.194603 47656533762944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.194584 47472606270336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.194623 47275543339904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.195559 46943144018816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.198944 47656533762944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:04.200649 46943144018816 estimator.py:1111] Calling model_fn.
W0618 12:21:04.200761 46943144018816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.202145 46943144018816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:21:04.204041 47656533762944 estimator.py:1111] Calling model_fn.
W0618 12:21:04.204148 47656533762944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.205512 47656533762944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.206740 47072374453120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.206790 47117821424512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.212620 47757207630720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.212643 47552580060032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882064.042230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.042995 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.043712 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.224883 47209249661824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.226039 47209249661824 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp882xlzy9
I0618 12:21:04.227130 47209249661824 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp882xlzy9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af00b24be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.227581 47209249661824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.233052 47209249661824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882064.131602 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.132036 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.132456 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.239788 47461700780928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882064.131676 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.132125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.132533 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.241016 47369236468608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.240837 47461700780928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp9u625386
I0618 12:21:04.241858 47461700780928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp9u625386', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ad2671e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.242266 47461700780928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.241788 47472606270336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.242001 47369236468608 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpa9coiot1
W0618 12:21:04.242230 47275543339904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:21:04.242977 47369236468608 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpa9coiot1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b154b19add8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.243376 47369236468608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882064.044461 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.045203 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.045884 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.244113 47700586783616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
I0618 12:21:04.245259 47700586783616 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b62711e3cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.246559 47700586783616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.246076 47472606270336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.246993 47461700780928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.246536 47275543339904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.247906 47369236468608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:21:04.251117 47472606270336 estimator.py:1111] Calling model_fn.
W0618 12:21:04.251225 47472606270336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.251588 47700586783616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:21:04.251600 47275543339904 estimator.py:1111] Calling model_fn.
W0618 12:21:04.251709 47275543339904 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.252586 47472606270336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.253080 47275543339904 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.254865 47209249661824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.256755 47117821424512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.257188 47072374453120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.260092 47552580060032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.260628 47757207630720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:04.261072 47117821424512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.261513 47072374453120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.264389 47552580060032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.264976 47757207630720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:04.265868 47461700780928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:04.266167 47117821424512 estimator.py:1111] Calling model_fn.
W0618 12:21:04.266276 47117821424512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:04.266587 47072374453120 estimator.py:1111] Calling model_fn.
W0618 12:21:04.266697 47072374453120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.267026 47369236468608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.267646 47117821424512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.268070 47072374453120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:21:04.269434 47552580060032 estimator.py:1111] Calling model_fn.
W0618 12:21:04.269545 47552580060032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:04.270054 47757207630720 estimator.py:1111] Calling model_fn.
W0618 12:21:04.270167 47757207630720 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:04.270905 47552580060032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882064.098274 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.098714 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.099182 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.270890 47429056156544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.271524 47757207630720 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:04.271415 47700586783616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.271919 47429056156544 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp3sf81diw
I0618 12:21:04.272916 47429056156544 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp3sf81diw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2338a1ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560882064.097631 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.098091 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.098484 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.272965 47610755273600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
I0618 12:21:04.273316 47429056156544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.273974 47610755273600 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpjdx7vpwb
I0618 12:21:04.274954 47610755273600 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpjdx7vpwb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d86be2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.275349 47610755273600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.278022 47429056156544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:04.280230 47610755273600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882064.123719 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.124578 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.125327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.297264 46920970888064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560882064.123060 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.123915 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.124748 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.297366 47085836886912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.297076 47429056156544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:04.298413 46920970888064 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp45oeln4t
W0618 12:21:04.298452 47085836886912 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmphde14h0u
I0618 12:21:04.299545 47085836886912 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmphde14h0u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad34f2b0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.299546 46920970888064 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp45oeln4t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacec638e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:04.299985 47085836886912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:04.300013 46920970888064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:04.300854 47610755273600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882064.116966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882064.117829 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882064.118519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:04.303114 47875272893312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000026-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:21:04.303217 47209249661824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable,[2019-06-18 12:21:43] divide_golden_chunk finished: 3.292 seconds
[2019-06-18 12:21:43] generate golden chunk: 3.306 seconds
[2019-06-18 12:21:43] moving /lfs/lfs12/gma_akey/results/epb015/models/000027-000015.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000027-000016.meta
[2019-06-18 12:21:43] moving /lfs/lfs12/gma_akey/results/epb015/models/000027-000015.index --> /lfs/lfs12/gma_akey/results/epb015/models/000027-000016.index
[2019-06-18 12:21:43] moving /lfs/lfs12/gma_akey/results/epb015/models/000027-000015.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb
[2019-06-18 12:21:43] moving /lfs/lfs12/gma_akey/results/epb015/models/000027-000015.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000027-000016.data-00000-of-00001
[2019-06-18 12:21:43] iteration time 26: 49.854 seconds
2019-06-18 12:21:45.357508: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882103.914475 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:21:48] minmax time: 3.294 seconds
2019-06-18 12:21:48.661674: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:21:48.667226: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:21:48.671842: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882108.683489 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 26}}
[2019-06-18 12:21:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000028-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:21:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=28 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=1023779859 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=2047559690 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=3071339521 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=4095119352 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=5118899183 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=6142679014 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=7166458845 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=8190238676 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=9214018507 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=10237798338 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=11261578169 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=12285358000 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=13309137831 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=14332917662 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=15356697493 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=16380477324 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=17404257155 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=18428036986 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=19451816817 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000026-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000027-000016 --seed=20475596648 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:21:59] eval finished: 10.747 seconds
[2019-06-18 12:21:59] Win rate 000027-000016 vs 000026-000015: 0.500
:::MLL 1560882119.495002 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:21:59] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=29 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=1023779860 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=2047559691 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=3071339522 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=4095119353 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=5118899184 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=6142679015 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=7166458846 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=8190238677 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=9214018508 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=10237798339 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=11261578170 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=12285358001 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=13309137832 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=14332917663 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=15356697494 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=16380477325 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=17404257156 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000028-000015 --seed=18428036987 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:22:30] selfplay finished: 30.623 seconds
[2019-06-18 12:22:30] selfplay mn: 30.642 seconds
[2019-06-18 12:22:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-29-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=29 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779860 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559691 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339522 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119353 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899184 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679015 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458846 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238677 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018508 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798339 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578170 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285358001 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137832 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917663 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697494 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477325 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257156 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036987 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816818 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596649 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376480 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156311 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000028-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:22:32] train finished: 44.005 seconds
:::MLL 1560882113.929982 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882113.930732 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882113.931406 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.051371 47564166837120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882113.933246 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882113.933978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882113.934671 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.051805 47778294215552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.052544 47564166837120 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp2xdxnzh5
I0618 12:21:54.053640 47564166837120 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp2xdxnzh5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42addb0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:54.052955 47778294215552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpr9eplxhf
I0618 12:21:54.054050 47778294215552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpr9eplxhf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7488d78e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.054091 47564166837120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.054486 47778294215552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.059208 47564166837120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.059610 47778294215552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882113.936926 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882113.937813 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882113.938531 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.077891 46998306255744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882113.936096 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882113.936947 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882113.937771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.078268 47742097769344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.079012 46998306255744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprij3s276
I0618 12:21:54.080124 46998306255744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprij3s276', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abeedefbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:54.079377 47742097769344 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmploehjcii
I0618 12:21:54.080471 47742097769344 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmploehjcii', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c1b5d9da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.080566 46998306255744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.080935 47742097769344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.080808 47564166837120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.081937 47778294215552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.085873 46998306255744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.086308 47742097769344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.107666 46998306255744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.108542 47742097769344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882114.016012 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.016468 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.016876 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.115861 47940843258752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.116878 47940843258752 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzzcspvag
I0618 12:21:54.117865 47940843258752 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzzcspvag', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a61851e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.118269 47940843258752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.122966 47940843258752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882114.016292 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.016753 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.017135 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.124110 47756133696384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.125096 47756133696384 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8cywdrik
I0618 12:21:54.126073 47756133696384 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8cywdrik', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f5ff8de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.126462 47756133696384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.130689 47564166837120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.131002 47756133696384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.132169 47778294215552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.134963 47564166837120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:54.136482 47778294215552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882114.009790 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.010290 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.010668 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.137144 47063402279808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882114.012779 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.013201 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.013569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.137508 47849293886336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.138195 47063402279808 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpzav8zrcn
I0618 12:21:54.139195 47063402279808 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpzav8zrcn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace15f63e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:54.138506 47849293886336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5bywogec
I0618 12:21:54.139480 47849293886336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5bywogec', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8510c07e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.139597 47063402279808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.139884 47849293886336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.139991 47564166837120 estimator.py:1111] Calling model_fn.
W0618 12:21:54.140104 47564166837120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:54.141470 47564166837120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:21:54.141553 47778294215552 estimator.py:1111] Calling model_fn.
W0618 12:21:54.141664 47778294215552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:54.141963 47940843258752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.143022 47778294215552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.144248 47063402279808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.144497 47849293886336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.150059 47756133696384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.157330 46998306255744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.157969 47742097769344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.161650 46998306255744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:54.162304 47742097769344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:54.163192 47063402279808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.163775 47849293886336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:54.166715 46998306255744 estimator.py:1111] Calling model_fn.
W0618 12:21:54.166826 46998306255744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:54.167408 47742097769344 estimator.py:1111] Calling model_fn.
W0618 12:21:54.167518 47742097769344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882114.019440 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.020412 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.021293 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.167699 47232127177600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882114.039402 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.040104 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.040824 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.167942 47181152613248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.168191 46998306255744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.168892 47742097769344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.168850 47232127177600 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpp87_j58b
W0618 12:21:54.169034 47181152613248 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpil99_8uy
I0618 12:21:54.169914 47232127177600 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpp87_j58b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af55ebfee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.170092 47181152613248 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpil99_8uy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9806dce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.170441 47232127177600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.170639 47181152613248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882114.007944 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.008858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.009710 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.170325 47223854367616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.171467 47223854367616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsj5v34ka
I0618 12:21:54.172541 47223854367616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsj5v34ka', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af371a6ddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.173006 47223854367616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.175688 47232127177600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.175800 47181152613248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882114.022346 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.023105 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.023786 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.177978 47595121546112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.178378 47223854367616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:21:54.179116 47595121546112 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b49e2e65cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.180364 47595121546112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.185564 47595121546112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.189436 47940843258752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.193730 47940843258752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:54.197765 47232127177600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.197044 47756133696384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.197868 47181152613248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:54.198802 47940843258752 estimator.py:1111] Calling model_fn.
W0618 12:21:54.198913 47940843258752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:54.199833 47223854367616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.200278 47940843258752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.201331 47756133696384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:54.206340 47756133696384 estimator.py:1111] Calling model_fn.
W0618 12:21:54.206771 47595121546112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.206449 47756133696384 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882114.058412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.059146 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.059846 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.208455 47783990510464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882114.055073 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.055945 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.056647 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.208955 47277042844544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.207798 47756133696384 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.209597 47783990510464 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfaz44pcl
I0618 12:21:54.210690 47783990510464 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfaz44pcl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75dc5e1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:54.210026 47277042844544 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpkbvg35o9
I0618 12:21:54.211084 47277042844544 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpkbvg35o9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2affd3ee9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.211115 47783990510464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.211509 47277042844544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.211068 47063402279808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.211508 47849293886336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.215376 47063402279808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:54.216458 47783990510464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.216734 47277042844544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.215831 47849293886336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:54.220442 47063402279808 estimator.py:1111] Calling model_fn.
W0618 12:21:54.220551 47063402279808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:54.220933 47849293886336 estimator.py:1111] Calling model_fn.
W0618 12:21:54.221042 47849293886336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882114.071741 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.072163 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.072522 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.220819 47514110088064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.221921 47063402279808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.222411 47849293886336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:54.221842 47514110088064 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptu2lbghn
I0618 12:21:54.222832 47514110088064 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptu2lbghn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b37063dae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.223237 47514110088064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882114.068091 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.068577 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.069019 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.223175 47934923690880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882114.100810 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.101258 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.101638 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.224301 47687733625728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560882114.096837 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.097346 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.097762 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.224309 47179027039104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.224190 47934923690880 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfzo28g9m
I0618 12:21:54.225181 47934923690880 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfzo28g9m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9900afae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.225579 47934923690880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.225385 47687733625728 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5krnj3u7
W0618 12:21:54.225417 47179027039104 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5xhd05dz
I0618 12:21:54.226385 47687733625728 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5krnj3u7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f73029e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.226411 47179027039104 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5xhd05dz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae901bc1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.226785 47687733625728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:54.226810 47179027039104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882114.048616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.049571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.050444 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.228387 47760926667648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.227927 47514110088064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.229397 47760926667648 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprh8kyuno
I0618 12:21:54.230373 47760926667648 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprh8kyuno', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b707da7ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.230781 47760926667648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.230477 47934923690880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.231488 47687733625728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.231488 47179027039104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.235626 47760926667648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:54.238423 47783990510464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.238541 47277042844544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882114.056700 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882114.057436 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882114.058125 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:54.245472 47048585610112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000027-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:21:54.246419 47048585610112 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmputhhhqgx
I0618 12:21:54.247389 47048585610112 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmputhhhqgx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acaa2d1de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:54.247792 47048585610112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:54.246893 47514110088064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:54.248231 47223854367616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:54.249315 47181152613248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of [2019-06-18 12:22:33] divide_golden_chunk finished: 3.233 seconds
[2019-06-18 12:22:33] generate golden chunk: 3.247 seconds
[2019-06-18 12:22:33] moving /lfs/lfs12/gma_akey/results/epb015/models/000028-000016.index --> /lfs/lfs12/gma_akey/results/epb015/models/000028-000017.index
[2019-06-18 12:22:33] moving /lfs/lfs12/gma_akey/results/epb015/models/000028-000016.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000028-000017.meta
[2019-06-18 12:22:33] moving /lfs/lfs12/gma_akey/results/epb015/models/000028-000016.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000028-000017.data-00000-of-00001
[2019-06-18 12:22:33] moving /lfs/lfs12/gma_akey/results/epb015/models/000028-000016.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb
[2019-06-18 12:22:33] iteration time 27: 49.513 seconds
2019-06-18 12:22:34.913149: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882153.427934 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:22:38] minmax time: 3.247 seconds
2019-06-18 12:22:38.170182: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:22:38.175794: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:22:38.180334: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882158.191767 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 27}}
[2019-06-18 12:22:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000029-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:22:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=29 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=1023779860 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=2047559691 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=3071339522 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=4095119353 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=5118899184 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=6142679015 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=7166458846 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=8190238677 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=9214018508 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=10237798339 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=11261578170 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=12285358001 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=13309137832 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=14332917663 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=15356697494 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=16380477325 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=17404257156 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=18428036987 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=19451816818 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000027-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000028-000017 --seed=20475596649 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:22:49] eval finished: 11.309 seconds
[2019-06-18 12:22:49] Win rate 000028-000017 vs 000027-000016: 0.570
:::MLL 1560882169.562261 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:22:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=30 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=1023779861 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=2047559692 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=3071339523 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=4095119354 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=5118899185 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=6142679016 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=7166458847 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=8190238678 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=9214018509 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=10237798340 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=11261578171 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=12285358002 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=13309137833 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=14332917664 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=15356697495 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=16380477326 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=17404257157 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000029-000016 --seed=18428036988 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:23:19] selfplay finished: 29.841 seconds
[2019-06-18 12:23:19] selfplay mn: 29.859 seconds
[2019-06-18 12:23:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-30-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=30 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779861 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559692 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339523 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119354 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899185 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679016 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458847 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238678 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018509 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798340 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578171 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285358002 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137833 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917664 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697495 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477326 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257157 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036988 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816819 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596650 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376481 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156312 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000029-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:23:21] train finished: 43.791 seconds
:::MLL 1560882163.456633 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.457345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.458018 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.587262 47756880323456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.451840 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.452748 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.453606 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.587334 47049906484096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.588434 47756880323456 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpe4jzw13e
W0618 12:22:43.588466 47049906484096 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmplme_ugfh
I0618 12:22:43.589555 47756880323456 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpe4jzw13e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f8c797e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.589631 47049906484096 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmplme_ugfh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acaf18cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.590028 47756880323456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.590081 47049906484096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.595355 47049906484096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.595381 47756880323456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882163.469956 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.470683 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.471352 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.604112 47754357171072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.472230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.472961 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.473641 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.604450 47398978732928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.605257 47754357171072 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7ugukq6c
W0618 12:22:43.605544 47398978732928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpceaybu8q
I0618 12:22:43.606357 47754357171072 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7ugukq6c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ef6151da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.606631 47398978732928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpceaybu8q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1c37e06e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.606810 47754357171072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.607077 47398978732928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.612084 47754357171072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.612218 47398978732928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.617094 47049906484096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.617325 47756880323456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.633627 47754357171072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.634003 47398978732928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882163.517306 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.518151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.518838 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.642644 47154911110016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.520729 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.521487 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.522137 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.642959 47668443198336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.643761 47154911110016 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpr1kbp4hi
I0618 12:22:43.644875 47154911110016 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpr1kbp4hi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae364502e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:22:43.644099 47668443198336 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfz9evzuw
I0618 12:22:43.645225 47668443198336 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfz9evzuw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5af535fdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.645323 47154911110016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.645681 47668443198336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882163.534675 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.535085 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.535438 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.648795 47338772317056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.531774 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.532248 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.532607 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.648797 47159214146432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.650543 47154911110016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.649870 47338772317056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp34yl7s86
W0618 12:22:43.649901 47159214146432 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_p6na_l3
W0618 12:22:43.650913 47668443198336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:22:43.650868 47338772317056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp34yl7s86', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e334b8da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.650887 47159214146432 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_p6na_l3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae464cb5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.651272 47338772317056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.651276 47159214146432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.655894 47338772317056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.655945 47159214146432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882163.546589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.547009 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.547373 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.663439 47372007240576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.545899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.546364 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.546749 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.663440 47492261745536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.665572 47049906484096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.664479 47492261745536 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvoy5hj3d
W0618 12:22:43.664506 47372007240576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppctd9qjz
I0618 12:22:43.665483 47492261745536 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvoy5hj3d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b31effa6da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.665501 47372007240576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppctd9qjz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b15f0402da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:22:43.666132 47756880323456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:22:43.665877 47492261745536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.665899 47372007240576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.669861 47049906484096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.670450 47756880323456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.670523 47492261745536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.670564 47372007240576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.672244 47154911110016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.672996 47668443198336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.674719 47338772317056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:22:43.674892 47049906484096 estimator.py:1111] Calling model_fn.
W0618 12:22:43.675001 47049906484096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:43.675011 47159214146432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:22:43.675548 47756880323456 estimator.py:1111] Calling model_fn.
W0618 12:22:43.675664 47756880323456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:43.676347 47049906484096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.677026 47756880323456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.682915 47754357171072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.683319 47398978732928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.687247 47754357171072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.687656 47398978732928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.689502 47372007240576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.689491 47492261745536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:22:43.692324 47754357171072 estimator.py:1111] Calling model_fn.
W0618 12:22:43.692433 47754357171072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:43.692782 47398978732928 estimator.py:1111] Calling model_fn.
W0618 12:22:43.692897 47398978732928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:43.693897 47754357171072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.694318 47398978732928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882163.611638 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.612067 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.612440 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.713246 47213852013440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.601343 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.601852 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.602294 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.713464 47040165790592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.714289 47213852013440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp52fe2sje
W0618 12:22:43.714470 47040165790592 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpu1flepg7
I0618 12:22:43.715280 47213852013440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp52fe2sje', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af11d76fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.715459 47040165790592 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpu1flepg7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8acf58e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.715683 47213852013440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.715860 47040165790592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.720346 47213852013440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.720456 47040165790592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.722113 47338772317056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.722738 47154911110016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.722703 47159214146432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.723153 47668443198336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560882163.516559 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.517479 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.518339 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.724391 47638372524928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.524459 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.525212 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.525905 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.724607 47114868204416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.726412 47338772317056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.725558 47638372524928 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpvnbyd_ij
I0618 12:22:43.725717 47114868204416 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada1191fd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.726677 47638372524928 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpvnbyd_ij', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53f4dbfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:22:43.727060 47154911110016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.727065 47159214146432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:22:43.727000 47114868204416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.727161 47638372524928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.727462 47668443198336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:22:43.731442 47338772317056 estimator.py:1111] Calling model_fn.
W0618 12:22:43.731553 47338772317056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:43.732125 47154911110016 estimator.py:1111] Calling model_fn.
W0618 12:22:43.732231 47154911110016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:43.732158 47159214146432 estimator.py:1111] Calling model_fn.
W0618 12:22:43.732267 47159214146432 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:43.732519 47668443198336 estimator.py:1111] Calling model_fn.
W0618 12:22:43.732627 47668443198336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:43.732294 47114868204416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.732376 47638372524928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.732908 47338772317056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.733592 47154911110016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.733623 47159214146432 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.733992 47668443198336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.736758 47492261745536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.737132 47372007240576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.739410 47040165790592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.739429 47213852013440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.741040 47492261745536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.741445 47372007240576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:22:43.746104 47492261745536 estimator.py:1111] Calling model_fn.
W0618 12:22:43.746212 47492261745536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:43.746536 47372007240576 estimator.py:1111] Calling model_fn.
W0618 12:22:43.746646 47372007240576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:43.747573 47492261745536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.748023 47372007240576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:43.752396 47114868204416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.752504 47638372524928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882163.571281 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.571700 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.572144 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.756481 47116622975872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560882163.567485 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.567982 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.568402 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.756438 47401555153792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.757447 47401555153792 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp17cdha84
W0618 12:22:43.757511 47116622975872 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp07hgf37m
I0618 12:22:43.758449 47401555153792 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp17cdha84', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1cd171ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.758528 47116622975872 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp07hgf37m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada7a29ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.758861 47401555153792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:43.758955 47116622975872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.763526 47401555153792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.763548 47116622975872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882163.598498 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882163.599420 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882163.600314 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:43.767348 47409125725056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000028-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:22:43.768455 47409125725056 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpfhrnkk5o
I0618 12:22:43.769586 47409125725056 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpfhrnkk5o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1e94af5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:43.770042 47409125725056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:43.775343 47409125725056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:43.782507 47401555153792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.782747 47116622975872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:43.786942 47040165790592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.787062 47213852013440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:43.791250 47040165790592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:43.791358 47213852013440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterat[2019-06-18 12:23:22] divide_golden_chunk finished: 3.222 seconds
[2019-06-18 12:23:22] generate golden chunk: 3.235 seconds
[2019-06-18 12:23:22] moving /lfs/lfs12/gma_akey/results/epb015/models/000029-000017.index --> /lfs/lfs12/gma_akey/results/epb015/models/000029-000018.index
[2019-06-18 12:23:22] moving /lfs/lfs12/gma_akey/results/epb015/models/000029-000017.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000029-000018.data-00000-of-00001
[2019-06-18 12:23:22] moving /lfs/lfs12/gma_akey/results/epb015/models/000029-000017.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb
[2019-06-18 12:23:22] moving /lfs/lfs12/gma_akey/results/epb015/models/000029-000017.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000029-000018.meta
[2019-06-18 12:23:22] iteration time 28: 49.276 seconds
2019-06-18 12:23:24.205160: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882202.704297 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:23:27] minmax time: 3.204 seconds
2019-06-18 12:23:27.419497: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:23:27.424867: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:23:27.429510: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882207.441251 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 28}}
[2019-06-18 12:23:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:23:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=30 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=1023779861 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=2047559692 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=3071339523 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=4095119354 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=5118899185 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=6142679016 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=7166458847 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=8190238678 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=9214018509 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=10237798340 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=11261578171 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=12285358002 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=13309137833 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=14332917664 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=15356697495 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=16380477326 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=17404257157 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=18428036988 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=19451816819 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000029-000018 --seed=20475596650 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:23:38] eval finished: 11.040 seconds
[2019-06-18 12:23:38] Win rate 000029-000018 vs 000028-000017: 0.460
:::MLL 1560882218.545344 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:23:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=31 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=1023779862 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=2047559693 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=3071339524 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=4095119355 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=5118899186 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=6142679017 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=7166458848 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=8190238679 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=9214018510 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=10237798341 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=11261578172 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=12285358003 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=13309137834 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=14332917665 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=15356697496 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=16380477327 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=17404257158 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000030-000017 --seed=18428036989 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:24:08] selfplay finished: 30.448 seconds
[2019-06-18 12:24:09] selfplay mn: 30.464 seconds
[2019-06-18 12:24:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-31-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=31 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779862 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559693 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339524 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119355 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899186 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679017 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458848 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238679 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018510 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798341 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578172 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285358003 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137834 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917665 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697496 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477327 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257158 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036989 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816820 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596651 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376482 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156313 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:24:11] train finished: 43.979 seconds
:::MLL 1560882212.686757 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.687646 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.688500 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.810807 47749985313664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.700131 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.700872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.701553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.811891 47302318924672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:32.811952 47749985313664 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp30js4_j2
I0618 12:23:32.813043 47749985313664 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp30js4_j2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6df17ffda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.813502 47749985313664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.812980 47302318924672 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmlh219cv
I0618 12:23:32.814115 47302318924672 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmlh219cv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b05b680ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.814570 47302318924672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.818765 47749985313664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.819642 47302318924672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.840769 47749985313664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.841659 47302318924672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882212.700235 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.700939 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.701617 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.889297 47709111899008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.693999 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.694900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.695713 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.889333 47649227740032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:32.890494 47649227740032 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpo51wx0f0
W0618 12:23:32.890528 47709111899008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmph26nl4ho
I0618 12:23:32.891607 47649227740032 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpo51wx0f0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b567be16e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.891640 47709111899008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmph26nl4ho', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b646d412da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.892063 47649227740032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:32.892129 47709111899008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.891699 47749985313664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:32.892353 47302318924672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560882212.779848 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.780323 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.780682 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.892221 47417452155776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.779335 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.779850 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.780271 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.892399 47143078933376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:32.893285 47417452155776 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf3ikmaeq
W0618 12:23:32.893378 47143078933376 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptjere0e9
I0618 12:23:32.894277 47417452155776 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf3ikmaeq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2084faae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.894355 47143078933376 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptjere0e9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae0a30f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.894683 47417452155776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:32.894748 47143078933376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.896015 47749985313664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:32.896670 47302318924672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:32.897459 47649227740032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.897500 47709111899008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.899347 47417452155776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.899360 47143078933376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:23:32.901157 47749985313664 estimator.py:1111] Calling model_fn.
W0618 12:23:32.901265 47749985313664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:32.901803 47302318924672 estimator.py:1111] Calling model_fn.
W0618 12:23:32.901913 47302318924672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:32.902625 47749985313664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:32.903283 47302318924672 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:32.918687 47709111899008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.918770 47649227740032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.918399 47143078933376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.918454 47417452155776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882212.770197 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.770628 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.771023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.933454 47843175351168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.771081 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.771492 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.771850 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.933727 47764473435008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:32.934455 47843175351168 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmj3xo5z9
W0618 12:23:32.934707 47764473435008 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmppgb2d3l4
I0618 12:23:32.935447 47843175351168 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmj3xo5z9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b83a40f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.935675 47764473435008 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmppgb2d3l4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b71510f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.935844 47843175351168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:32.936067 47764473435008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.940576 47843175351168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.940823 47764473435008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.959713 47843175351168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.959928 47764473435008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:32.965944 47143078933376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:32.965995 47417452155776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:32.966596 47649227740032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:32.966837 47709111899008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:32.970301 47417452155776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:32.970269 47143078933376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:32.970897 47649227740032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:32.971153 47709111899008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:23:32.975393 47417452155776 estimator.py:1111] Calling model_fn.
I0618 12:23:32.975363 47143078933376 estimator.py:1111] Calling model_fn.
W0618 12:23:32.975473 47143078933376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:32.975503 47417452155776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:32.976005 47649227740032 estimator.py:1111] Calling model_fn.
W0618 12:23:32.976114 47649227740032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:32.976261 47709111899008 estimator.py:1111] Calling model_fn.
W0618 12:23:32.976368 47709111899008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:32.976851 47417452155776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:32.976827 47143078933376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:32.977491 47649227740032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:32.977742 47709111899008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882212.776244 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.777153 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.777991 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.981503 47071085716352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.782863 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.783608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.784332 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:32.981903 46932654674816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
I0618 12:23:32.982650 47071085716352 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acfdfee3d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:23:32.983021 46932654674816 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpsa4at_i4
I0618 12:23:32.983910 47071085716352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:32.984108 46932654674816 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpsa4at_i4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafa4cc2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:32.984548 46932654674816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:32.989163 47071085716352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:32.989675 46932654674816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.006836 47843175351168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.007343 47764473435008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.009579 47071085716352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.010021 46932654674816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.011129 47843175351168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.011698 47764473435008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882212.821000 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.821423 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.821781 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.014679 46993092281216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
I0618 12:23:33.016208 47843175351168 estimator.py:1111] Calling model_fn.
:::MLL 1560882212.814083 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.814973 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.815827 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.016360 47760670532480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:33.016319 47843175351168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.015733 46993092281216 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpx9cgmqw_
:::MLL 1560882212.822356 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.822766 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.823129 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.016481 47341724316544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
I0618 12:23:33.016799 47764473435008 estimator.py:1111] Calling model_fn.
I0618 12:23:33.016733 46993092281216 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpx9cgmqw_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abdb728ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:23:33.016910 47764473435008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:33.017143 46993092281216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.017680 47843175351168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.017520 47760670532480 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpq5siji8e
W0618 12:23:33.018257 47764473435008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.017470 47341724316544 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpf926q3k0
I0618 12:23:33.018640 47760670532480 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpq5siji8e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b706e636e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.018441 47341724316544 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpf926q3k0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0ee33f8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.019094 47760670532480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.018842 47341724316544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.021814 46993092281216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.023356 47341724316544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.024560 47760670532480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882212.814129 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.815049 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.815926 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.030617 47576813773696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:33.031722 47576813773696 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp6zp7k6g_
I0618 12:23:33.032829 47576813773696 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp6zp7k6g_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b459fabfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.033270 47576813773696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.034708 47749985313664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.
W0618 12:23:33.035479 47302318924672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.
W0618 12:23:33.038257 47576813773696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.040952 46993092281216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.042483 47341724316544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.046101 47760670532480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.057078 46932654674816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.057108 47071085716352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.058832 47576813773696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882212.876133 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.876606 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.876992 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.061338 47455396643712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.875860 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.876313 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.876724 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.061522 47248491352960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:33.061414 47071085716352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.061378 46932654674816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.062386 47455396643712 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpbimalxvq
W0618 12:23:33.062543 47248491352960 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpi8ns9s49
I0618 12:23:33.063390 47455396643712 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpbimalxvq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b295aa59e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.063530 47248491352960 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpi8ns9s49', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af92e216e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.063794 47455396643712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.063929 47248491352960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.066497 47071085716352 estimator.py:1111] Calling model_fn.
I0618 12:23:33.066469 46932654674816 estimator.py:1111] Calling model_fn.
W0618 12:23:33.066579 46932654674816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.066609 47071085716352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.067979 47071085716352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.067946 46932654674816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.068427 47455396643712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.068497 47248491352960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882212.819697 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.820477 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.821290 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.079608 47885261083520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:23:33.080660 47885261083520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmptk30gajc
:::MLL 1560882212.836808 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.837759 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.838496 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.081063 47909235016576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560882212.836048 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882212.836945 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882212.837830 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.081094 47413252383616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000029-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000020-000011.tfrecord.zz_0_0
I0618 12:23:33.081708 47885261083520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmptk30gajc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d70906e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.082128 47885261083520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.082234 47909235016576 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp8bm8mb83
W0618 12:23:33.082265 47413252383616 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpq1w526a3
I0618 12:23:33.083352 47909235016576 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp8bm8mb83', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9305859da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.083352 47413252383616 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpq1w526a3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1f8aa73e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.083803 47413252383616 train.py:201] Training, st[2019-06-18 12:24:12] divide_golden_chunk finished: 3.226 seconds
[2019-06-18 12:24:12] generate golden chunk: 3.241 seconds
[2019-06-18 12:24:12] iteration time 29: 49.548 seconds
2019-06-18 12:24:13.935835: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882252.252091 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:24:17] minmax time: 3.287 seconds
2019-06-18 12:24:17.232877: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:24:17.238467: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:24:17.243100: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882257.256799 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 29}}
[2019-06-18 12:24:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb025 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb026 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb015/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb015/models/000031-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb027 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:24:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-eval-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=31 : \
-host epb016 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=1023779862 : \
-host epb018 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=2047559693 : \
-host epb019 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=3071339524 : \
-host epb048 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=4095119355 : \
-host epb013 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=5118899186 : \
-host epb012 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=6142679017 : \
-host epb011 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=7166458848 : \
-host epb009 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=8190238679 : \
-host epb008 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=9214018510 : \
-host epb007 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=10237798341 : \
-host epb006 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=11261578172 : \
-host epb005 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=12285358003 : \
-host epb004 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=13309137834 : \
-host epb073 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=14332917665 : \
-host epb074 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=15356697496 : \
-host epb075 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=16380477327 : \
-host epb076 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=17404257158 : \
-host epb077 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=18428036989 : \
-host epb078 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=19451816820 : \
-host epb079 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/000028-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/000030-000018 --seed=20475596651 : \
-host epb100 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:24:28] eval finished: 10.789 seconds
[2019-06-18 12:24:28] Win rate 000030-000018 vs 000028-000017: 0.690
:::MLL 1560882268.107437 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:24:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-selfplay-32-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb015 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=32 : \
-host epb016 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=1023779863 : \
-host epb018 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=2047559694 : \
-host epb019 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=3071339525 : \
-host epb048 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=4095119356 : \
-host epb013 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=5118899187 : \
-host epb012 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=6142679018 : \
-host epb011 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=7166458849 : \
-host epb009 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=8190238680 : \
-host epb008 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=9214018511 : \
-host epb007 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=10237798342 : \
-host epb006 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=11261578173 : \
-host epb005 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=12285358004 : \
-host epb004 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=13309137835 : \
-host epb073 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=14332917666 : \
-host epb074 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=15356697497 : \
-host epb075 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=16380477328 : \
-host epb076 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=17404257159 : \
-host epb077 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb015/data/holdout/000031-000017 --seed=18428036990 : \
-host epb078 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb015/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000030-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb01
[2019-06-18 12:24:57] selfplay finished: 29.843 seconds
[2019-06-18 12:24:57] selfplay mn: 29.861 seconds
[2019-06-18 12:24:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb015/mpi/out-divide_golden_chunk-32-%r.txt \
-host epb015 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=32 : \
-host epb016 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=1023779863 : \
-host epb018 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=2047559694 : \
-host epb019 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=3071339525 : \
-host epb048 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=4095119356 : \
-host epb013 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=5118899187 : \
-host epb012 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=6142679018 : \
-host epb011 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=7166458849 : \
-host epb009 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=8190238680 : \
-host epb008 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=9214018511 : \
-host epb007 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=10237798342 : \
-host epb006 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=11261578173 : \
-host epb005 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=12285358004 : \
-host epb004 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=13309137835 : \
-host epb073 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=14332917666 : \
-host epb074 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=15356697497 : \
-host epb075 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=16380477328 : \
-host epb076 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=17404257159 : \
-host epb077 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=18428036990 : \
-host epb078 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=19451816821 : \
-host epb079 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=20475596652 : \
-host epb100 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=21499376483 : \
-host epb021 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000031-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb015 --seed=22523156314 : \
-host epb022 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb015/data/selfplay/000031-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb015/data/golde
[2019-06-18 12:25:01] train finished: 43.780 seconds
:::MLL 1560882262.541137 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.542022 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.542861 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.671136 47531045606272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882262.547688 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.548454 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.549123 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.671884 47538053915520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.672272 47531045606272 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp4vvmek7a
I0618 12:24:22.673390 47531045606272 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp4vvmek7a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3af7ad2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:22.672978 47538053915520 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpq2lllti3
I0618 12:24:22.673851 47531045606272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.674097 47538053915520 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpq2lllti3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c99677e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.674548 47538053915520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.679168 47531045606272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.679795 47538053915520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.700924 47531045606272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.701839 47538053915520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882262.632494 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.633003 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.633688 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.745610 47947036148608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882262.553647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.554407 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.555218 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.747525 47301728527232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882262.555136 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.555876 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.556576 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.747622 47178946925440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.746654 47947036148608 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpmfsywgue
I0618 12:24:22.747690 47947036148608 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpmfsywgue', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9bd2a51e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.748123 47947036148608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.748659 47301728527232 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp0slsq97u
W0618 12:24:22.748740 47178946925440 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpk2ze8a3q
I0618 12:24:22.749782 47301728527232 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp0slsq97u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0593503da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.749836 47178946925440 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpk2ze8a3q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8fcf5ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.750236 47301728527232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.750278 47178946925440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.752854 47947036148608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.753566 47531045606272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.754478 47538053915520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560882262.632539 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.633039 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.633731 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.754466 47629053903744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.755572 47301728527232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.755626 47178946925440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.755453 47629053903744 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpahx4hc7e
I0618 12:24:22.756448 47629053903744 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpahx4hc7e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b51c96d0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.756845 47629053903744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.758198 47531045606272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:22.759125 47538053915520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:22.761356 47629053903744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:24:22.763630 47531045606272 estimator.py:1111] Calling model_fn.
W0618 12:24:22.763745 47531045606272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:24:22.764574 47538053915520 estimator.py:1111] Calling model_fn.
W0618 12:24:22.764689 47538053915520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.765192 47531045606272 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.766136 47538053915520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.771862 47947036148608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.777075 47301728527232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.777167 47178946925440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.780299 47629053903744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882262.624813 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.625245 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.625621 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.791091 47613470602112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.792134 47613470602112 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_a5c291t
I0618 12:24:22.793143 47613470602112 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_a5c291t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4e2896ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.793545 47613470602112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882262.626439 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.626858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.627225 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.793480 47147322901376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.794476 47147322901376 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpj8rwb9v8
I0618 12:24:22.795461 47147322901376 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpj8rwb9v8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1a0054e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.795865 47147322901376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.798249 47613470602112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.800407 47147322901376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.817125 47613470602112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.819045 47947036148608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.819548 47147322901376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.823355 47947036148608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:22.824572 47301728527232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.824835 47178946925440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.826971 47629053903744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.828862 47301728527232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:24:22.828451 47947036148608 estimator.py:1111] Calling model_fn.
W0618 12:24:22.828564 47947036148608 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.829136 47178946925440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:22.829921 47947036148608 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.831265 47629053903744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:24:22.833887 47301728527232 estimator.py:1111] Calling model_fn.
W0618 12:24:22.833999 47301728527232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:24:22.834166 47178946925440 estimator.py:1111] Calling model_fn.
W0618 12:24:22.834275 47178946925440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.835358 47301728527232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.835635 47178946925440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882262.642398 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.643296 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.644167 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.836723 47354728711040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
I0618 12:24:22.836295 47629053903744 estimator.py:1111] Calling model_fn.
W0618 12:24:22.836404 47629053903744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.837754 47629053903744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.837905 47354728711040 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_2sww61w
I0618 12:24:22.839048 47354728711040 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_2sww61w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11ea5ebe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.839525 47354728711040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882262.652802 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.653538 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.654227 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.844425 47489165378432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.845009 47354728711040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.845506 47489165378432 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp7v71jjzi
I0618 12:24:22.846640 47489165378432 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp7v71jjzi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b31376b6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.847101 47489165378432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.852250 47489165378432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882262.629966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.630827 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.631651 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.858142 47820543124352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
I0618 12:24:22.859307 47820543124352 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb015/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7e5f129d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.860575 47820543124352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.864316 47613470602112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.866050 47820543124352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.866847 47354728711040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.866702 47147322901376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:22.868617 47613470602112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:22.871004 47147322901376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882262.630957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.631823 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.632511 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.873379 47718929351552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.873806 47489165378432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:24:22.873688 47613470602112 estimator.py:1111] Calling model_fn.
W0618 12:24:22.873800 47613470602112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.875172 47613470602112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:22.874465 47718929351552 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpudvokapy
I0618 12:24:22.875540 47718929351552 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpudvokapy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b66b66b8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560882262.660553 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.661473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.662380 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.876570 47186849710976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
I0618 12:24:22.875986 47718929351552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.876083 47147322901376 estimator.py:1111] Calling model_fn.
W0618 12:24:22.876191 47147322901376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:22.877675 47186849710976 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpspc_n5zd
I0618 12:24:22.878710 47186849710976 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpspc_n5zd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aead4008e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:22.877553 47147322901376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:24:22.879108 47186849710976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.880892 47718929351552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.883977 47186849710976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882262.715417 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.715835 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.716204 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.887019 47776667296640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.887077 47820543124352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882262.714233 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.714669 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.715068 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.887449 47656152765312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.888067 47776667296640 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp516ox7sz
I0618 12:24:22.889071 47776667296640 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp516ox7sz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7427debe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:22.888472 47656152765312 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpz3t_bwgu
I0618 12:24:22.889479 47776667296640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.889470 47656152765312 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpz3t_bwgu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5818a4ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.889864 47656152765312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.894193 47776667296640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.894467 47656152765312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882262.675562 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.676319 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.677003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.897452 46913551577984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882262.672509 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.673019 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.673382 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.896848 47307221726080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.898452 46913551577984 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp_zuqwlnt
I0618 12:24:22.899497 46913551577984 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp_zuqwlnt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab3229fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:22.897898 47307221726080 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmp5hgnbzqe
I0618 12:24:22.899927 46913551577984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.898910 47307221726080 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmp5hgnbzqe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b06dabbbdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.899318 47307221726080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882262.675556 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.675973 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.676339 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.899298 47210143888256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.900177 47718929351552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.900273 47210143888256 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmprrrxmxxh
I0618 12:24:22.901275 47210143888256 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmprrrxmxxh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af040717e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.901672 47210143888256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.903233 47186849710976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.904585 46913551577984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.904032 47307221726080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882262.669153 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.669926 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.670706 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.907228 47753683874688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.906421 47210143888256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882262.670955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882262.671671 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882262.672352 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:22.907508 47388247057280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:22.908370 47753683874688 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpg8mspvv_
W0618 12:24:22.908625 47388247057280 estimator.py:1760] Using temporary folder as model directory: /tmp/96735.tmpdir/tmpc36ipfnp
I0618 12:24:22.909510 47753683874688 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpg8mspvv_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ecdf37e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.909723 47388247057280 estimator.py:201] Using config: {'_model_dir': '/tmp/96735.tmpdir/tmpc36ipfnp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b19b8382e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:22.909970 47753683874688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:22.910167 47388247057280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:22.909529 47531045606272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.
W0618 12:24:22.910171 47538053915520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.
W0618 12:24:22.913086 47776667296640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.913490 47656152765312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:22.915236 47753683874688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.915296 47388247057280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:22.918083 47354728711040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays[2019-06-18 12:25:01] divide_golden_chunk finished: 3.199 seconds
[2019-06-18 12:25:01] generate golden chunk: 3.213 seconds
[2019-06-18 12:25:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000031-000018.index --> /lfs/lfs12/gma_akey/results/epb015/models/000031-000019.index
[2019-06-18 12:25:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000031-000018.pb --> /lfs/lfs12/gma_akey/results/epb015/models/000031-000019.pb
[2019-06-18 12:25:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000031-000018.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb015/models/000031-000019.data-00000-of-00001
[2019-06-18 12:25:01] moving /lfs/lfs12/gma_akey/results/epb015/models/000031-000018.meta --> /lfs/lfs12/gma_akey/results/epb015/models/000031-000019.meta
[2019-06-18 12:25:01] iteration time 30: 48.972 seconds
:::MLL 1560882301.224666 epoch_stop: {"value": null, "metadata": {'lineno': 737, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:25:01] Total time: 1705.967 seconds

numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000018-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000018-000012log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000019-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000019-000012log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000020-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000020-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000021-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000021-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000022-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000022-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000023-000014_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000023-000014log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000024-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000024-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000025-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000025-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000026-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000026-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000027-000016_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000027-000016log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000028-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000028-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000029-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000029-000018log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb015/models/000030-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb015/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb015/models/000030-000018log.txt
:::MLL 1560882303.871473 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
I0618 12:25:03.873263 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=1
I0618 12:25:30.513250 47128968758144 utils.py:86] eval finished: 26.638 seconds
I0618 12:25:30.517013 47128968758144 reference_implementation.py:563] Win rate 000001-000001 vs target: 0.060
:::MLL 1560882330.517963 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560882330.518297 eval_accuracy: {"value": 0.06, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560882330.518610 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
I0618 12:25:30.518918 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=2
I0618 12:25:55.079596 47128968758144 utils.py:86] eval finished: 24.560 seconds
I0618 12:25:55.082731 47128968758144 reference_implementation.py:563] Win rate 000002-000002 vs target: 0.170
:::MLL 1560882355.083431 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560882355.083758 eval_accuracy: {"value": 0.17, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560882355.084093 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
I0618 12:25:55.084399 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=3
I0618 12:26:20.612695 47128968758144 utils.py:86] eval finished: 25.528 seconds
I0618 12:26:20.615513 47128968758144 reference_implementation.py:563] Win rate 000003-000003 vs target: 0.030
:::MLL 1560882380.616205 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560882380.616528 eval_accuracy: {"value": 0.03, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560882380.616846 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
I0618 12:26:20.617159 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=4
I0618 12:26:47.631680 47128968758144 utils.py:86] eval finished: 27.014 seconds
I0618 12:26:47.634501 47128968758144 reference_implementation.py:563] Win rate 000004-000003 vs target: 0.040
:::MLL 1560882407.635408 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560882407.635743 eval_accuracy: {"value": 0.04, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560882407.636078 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
I0618 12:26:47.636403 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=5
I0618 12:27:13.427628 47128968758144 utils.py:86] eval finished: 25.791 seconds
I0618 12:27:13.430516 47128968758144 reference_implementation.py:563] Win rate 000005-000004 vs target: 0.100
:::MLL 1560882433.431211 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560882433.431544 eval_accuracy: {"value": 0.1, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560882433.431878 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
I0618 12:27:13.432193 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=6
I0618 12:27:37.805958 47128968758144 utils.py:86] eval finished: 24.374 seconds
I0618 12:27:37.808855 47128968758144 reference_implementation.py:563] Win rate 000006-000004 vs target: 0.130
:::MLL 1560882457.809543 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560882457.809866 eval_accuracy: {"value": 0.13, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560882457.810188 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
I0618 12:27:37.810494 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=7
I0618 12:28:01.758717 47128968758144 utils.py:86] eval finished: 23.948 seconds
I0618 12:28:01.761605 47128968758144 reference_implementation.py:563] Win rate 000007-000004 vs target: 0.200
:::MLL 1560882481.762658 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882481.763007 eval_accuracy: {"value": 0.2, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882481.763348 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
I0618 12:28:01.763668 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=8
I0618 12:28:26.657006 47128968758144 utils.py:86] eval finished: 24.893 seconds
I0618 12:28:26.659840 47128968758144 reference_implementation.py:563] Win rate 000008-000005 vs target: 0.210
:::MLL 1560882506.660545 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882506.660868 eval_accuracy: {"value": 0.21, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882506.661199 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
I0618 12:28:26.661489 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=9
I0618 12:28:51.632340 47128968758144 utils.py:86] eval finished: 24.971 seconds
I0618 12:28:51.635181 47128968758144 reference_implementation.py:563] Win rate 000009-000006 vs target: 0.150
:::MLL 1560882531.635864 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882531.636194 eval_accuracy: {"value": 0.15, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882531.636520 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
I0618 12:28:51.636820 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000010-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=10
I0618 12:29:16.601846 47128968758144 utils.py:86] eval finished: 24.965 seconds
I0618 12:29:16.604826 47128968758144 reference_implementation.py:563] Win rate 000010-000006 vs target: 0.180
:::MLL 1560882556.605695 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882556.606026 eval_accuracy: {"value": 0.18, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882556.606343 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
I0618 12:29:16.606646 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000011-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=11
I0618 12:29:39.847287 47128968758144 utils.py:86] eval finished: 23.240 seconds
I0618 12:29:39.850159 47128968758144 reference_implementation.py:563] Win rate 000011-000006 vs target: 0.310
:::MLL 1560882579.850829 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882579.851159 eval_accuracy: {"value": 0.31, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882579.851507 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
I0618 12:29:39.851817 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=12
I0618 12:30:03.447663 47128968758144 utils.py:86] eval finished: 23.596 seconds
I0618 12:30:03.450563 47128968758144 reference_implementation.py:563] Win rate 000012-000007 vs target: 0.320
:::MLL 1560882603.451519 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882603.451846 eval_accuracy: {"value": 0.32, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882603.452185 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
I0618 12:30:03.452504 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=13
I0618 12:30:27.375401 47128968758144 utils.py:86] eval finished: 23.923 seconds
I0618 12:30:27.378361 47128968758144 reference_implementation.py:563] Win rate 000013-000008 vs target: 0.420
:::MLL 1560882627.379238 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882627.379563 eval_accuracy: {"value": 0.42, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882627.379878 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
I0618 12:30:27.380229 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=14
I0618 12:30:52.739261 47128968758144 utils.py:86] eval finished: 25.359 seconds
I0618 12:30:52.742157 47128968758144 reference_implementation.py:563] Win rate 000014-000009 vs target: 0.210
:::MLL 1560882652.742839 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882652.743179 eval_accuracy: {"value": 0.21, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882652.743504 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
I0618 12:30:52.743814 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=15
I0618 12:31:15.734139 47128968758144 utils.py:86] eval finished: 22.990 seconds
I0618 12:31:15.737002 47128968758144 reference_implementation.py:563] Win rate 000015-000009 vs target: 0.400
:::MLL 1560882675.737675 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882675.738002 eval_accuracy: {"value": 0.4, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882675.738319 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
I0618 12:31:15.738626 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=16
I0618 12:31:36.549705 47128968758144 utils.py:86] eval finished: 20.811 seconds
I0618 12:31:36.552588 47128968758144 reference_implementation.py:563] Win rate 000016-000010 vs target: 0.420
:::MLL 1560882696.553571 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882696.553917 eval_accuracy: {"value": 0.42, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882696.554239 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
I0618 12:31:36.554552 47128968758144 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb015/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb015/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb015/sgf/eval/target --seed=17
I0618 12:31:58.305033 47128968758144 utils.py:86] eval finished: 21.750 seconds
I0618 12:31:58.307854 47128968758144 reference_implementation.py:563] Win rate 000017-000011 vs target: 0.510
:::MLL 1560882718.308548 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882718.308875 eval_accuracy: {"value": 0.51, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882718.309214 eval_result: {"value": null, "metadata": {'lineno': 52, 'file': 'ml_perf/eval_models.py', 'iteration': 16, 'timestamp': 848.149}}
:::MLL 1560882718.309529 run_stop: {"value": null, "metadata": {'lineno': 53, 'file': 'ml_perf/eval_models.py', 'status': 'success'}}
Model 000017-000011 beat target after 848.149s
~/submission/benchmarks/minigo/clx-8260l-2s-x32
