:::MLL 1560880069.803889849 submission_org: {"value": "Intel_Corp", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.805393395 submission_platform: {"value": "32xCLX-8260L_CPUs", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.806743794 submission_division: {"value": "closed", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.807968794 submission_status: {"value": "onprem", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.809251335 submission_benchmark: {"value": "minigo", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.810414858 submission_poc_name: {"value": "Guokai Ma, Letian Kang, Christine Cheng, Mingxiao Huang", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.811779984 submission_poc_email: {"value": "guokai.ma@intel.com, letian.kang@intel.com, christine.cheng@intel.com, mingxiao.huang@intel.com", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880069.813225978 submission_entry: {"value": {"framework": "TensorFlow 1.13.1", "power": "none", "notes": "none", "interconnect": "OPA", "os": "Oracle Linux Server 7.6", "libraries": "MKLDNN (v0.18), MKL (v2019.0.3.20190220), IntelMPI (2018.1.163)", "compilers": "GCC6.3", "nodes": [{"num_nodes": 32, "cpu": "Intel(R) Xeon(R) Platinum 8260L CPU @ 2.40GHz", "num_cores": 48, "num_vcpus": "NA", "accelerator": "NA", "num_accelerators": 0, "sys_mem_size": "192G", "sys_storage_type": "SSD", "sys_storage_size": "800G", "cpu_accel_interconnect": "100Gb OPA", "network_card": "100Gb OPA", "num_network_cards": 1, "notes": "NA"}]}, "metadata": {"lineno": 0, "file": "manual"}}
[mpiexec@epb162] control_cb (../../pm/pmiserv/pmiserv_cb.c:893): assert (!closed) failed
[mpiexec@epb162] HYDT_dmxu_poll_wait_for_event (../../tools/demux/demux_poll.c:76): callback returned error status
[mpiexec@epb162] HYD_pmci_wait_for_completion (../../pm/pmiserv/pmiserv_pmci.c:507): error waiting for event
[mpiexec@epb162] main (../../ui/mpich/mpiexec.c:1148): process manager error waiting for completion
:::MLL 1560880090.441963291 cache_clear: {value: true, metadata: {lineno: 0, file: manual}}
~/submission/benchmarks/minigo/implementations/tensorflow ~/submission/benchmarks/minigo/clx-8260l-2s-x32
Physical cores = 48
Virtual cores = 96
NUMA cores = 24
KMP_HW_SUBSET = 2T
Output to /lfs/lfs12/gma_akey
./run_mn.sh: line 20: ulimit: max user processes: cannot modify limit: Operation not permitted
Wiping dir /lfs/lfs12/gma_akey/results/epb162
:::MLL 1560880114.092101 init_start: {"value": null, "metadata": {'lineno': 742, 'file': 'ml_perf/reference_implementation.py'}}
Making dir /lfs/lfs12/gma_akey/results/epb162/models
Making dir /lfs/lfs12/gma_akey/results/epb162/data/selfplay
Making dir /lfs/lfs12/gma_akey/results/epb162/data/holdout
Making dir /lfs/lfs12/gma_akey/results/epb162/sgf/eval
Making dir /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks
Making dir /lfs/lfs12/gma_akey/results/epb162/work_dir
Making dir /lfs/lfs12/gma_akey/results/epb162/mpi
[2019-06-18 11:48:34] Selfplay nodes = ['epb162', 'epb326', 'epb161', 'epb283', 'epb325', 'epb327', 'epb163', 'epb324', 'epb118', 'epb148', 'epb320', 'epb246', 'epb329', 'epb299', 'epb298', 'epb242', 'epb244', 'epb245', 'epb243', 'epb240', 'epb248', 'epb249', 'epb247', 'epb241', 'epb160', 'epb323']
[2019-06-18 11:48:34] Train nodes = ['epb321', 'epb149', 'epb193', 'epb109', 'epb107', 'epb104']
[2019-06-18 11:48:34] Eval nodes = ['epb162', 'epb326', 'epb161', 'epb283', 'epb325', 'epb327', 'epb163', 'epb324', 'epb118', 'epb148', 'epb320', 'epb246', 'epb329', 'epb299', 'epb298', 'epb242', 'epb244', 'epb245', 'epb243', 'epb240', 'epb248', 'epb249', 'epb247', 'epb241', 'epb160', 'epb323']
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.41s/it]
[2019-06-18 11:51:30] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py:86: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
[2019-06-18 11:51:30] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
2019-06-18 11:51:30.502740: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-06-18 11:51:30.515508: I tensorflow/core/common_runtime/process_util.cc:113] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[2019-06-18 11:51:30] From ./quantize_graph.py:351: quantize_v2 (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.
Instructions for updating:
`tf.quantize_v2` is deprecated, please use `tf.quantization.quantize` instead.
2019-06-18 11:51:30.843474: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
[2019-06-18 11:51:30] From ./dual_net.py:679: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99']
Reading tf_records from 1 inputs
[2019-06-18 11:51:34] minmax time: 3.834 seconds
2019-06-18 11:51:34.688155: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:51:34.693873: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:51:34.698785: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880294.782189 init_stop: {"value": null, "metadata": {'lineno': 614, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880294.782562 run_start: {"value": null, "metadata": {'lineno': 615, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880294.782968 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:51:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:51:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=2 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=1023779833 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=2047559664 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=3071339495 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=4095119326 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=5118899157 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=6142678988 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=7166458819 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=8190238650 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=9214018481 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=10237798312 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=11261578143 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=12285357974 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=13309137805 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=14332917636 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=15356697467 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=16380477298 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=17404257129 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=18428036960 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000001-000000 --seed=19451816791 : \
-host epb2
[2019-06-18 11:52:08] selfplay finished: 33.674 seconds
[2019-06-18 11:52:08] selfplay mn: 33.697 seconds
[2019-06-18 11:52:08] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-2-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779833 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559664 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339495 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119326 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899157 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678988 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458819 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238650 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018481 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798312 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578143 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357974 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137805 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917636 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697467 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477298 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257129 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036960 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816791 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596622 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376453 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156284 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:52:24] divide_golden_chunk finished: 16.404 seconds
[2019-06-18 11:52:24] generate golden chunk: 16.421 seconds
[2019-06-18 11:52:30] train finished: 56.107 seconds
:::MLL 1560880310.429821 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.430656 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.431437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.024028 47525060367232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.510092 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.510529 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.510900 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.024525 48008943899520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.508129 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.508570 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.508955 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.024580 47917123695488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.429701 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.430533 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.431329 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.024601 47325685142400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:51:51.025112 47525060367232 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqasxsw0f
I0618 11:51:51.026208 47525060367232 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqasxsw0f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3992edae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.025614 48008943899520 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphuehx5f3
W0618 11:51:51.025644 47917123695488 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo80cft5g
W0618 11:51:51.025670 47325685142400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpak9v26cp
I0618 11:51:51.026647 47525060367232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.026691 48008943899520 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphuehx5f3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baa3ca25e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.026713 47917123695488 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo80cft5g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94dbb94e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.026744 47325685142400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpak9v26cp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b273d2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.027137 48008943899520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.027154 47917123695488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.027188 47325685142400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.035698 47917123695488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.035718 48008943899520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.035736 47325685142400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.035808 47525060367232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.057366 47325685142400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.057459 47525060367232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.057918 47917123695488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.058089 48008943899520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880310.485181 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.485952 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.486645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.063954 47554630579072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.487689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.488411 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.489073 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.063990 47747830686592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.568303 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.568717 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.569070 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.063991 46978451276672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.568392 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.568804 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.569152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.064023 47450343080832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:51:51.065067 47554630579072 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1hlfge04
W0618 11:51:51.065158 47747830686592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplm2laisj
W0618 11:51:51.065103 46978451276672 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyhi08jcm
W0618 11:51:51.065130 47450343080832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkqtrd4fh
I0618 11:51:51.066136 47554630579072 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1hlfge04', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4075734e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.066188 46978451276672 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyhi08jcm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba4e7cde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.066207 47450343080832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkqtrd4fh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b282d6e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.066220 47747830686592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplm2laisj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d7112de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.066575 47554630579072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.066633 46978451276672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.066636 47450343080832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.066653 47747830686592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.075458 47554630579072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.075424 46978451276672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.075435 47450343080832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.075500 47747830686592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.096852 46978451276672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.097126 47450343080832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.097720 47554630579072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.097798 47747830686592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880310.519878 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.520638 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.521321 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.105709 47733377942400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.504436 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.505320 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.506145 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.105796 47902120391552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.571095 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.571517 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.571877 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.105886 47648537146240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.568293 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.568715 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.569077 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.105875 47600480547712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:51:51.106945 47600480547712 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4b22524d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.107206 47733377942400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpar2jnyxk
I0618 11:51:51.108155 47600480547712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.108332 47733377942400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpar2jnyxk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a139f8e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.107699 47902120391552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpb3djdvl8
I0618 11:51:51.108792 47733377942400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.108810 47902120391552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpb3djdvl8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b915d750e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.108138 47648537146240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3ythk1dh
:::MLL 1560880310.517167 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.517989 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.518747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.108455 47045635363712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:51:51.109209 47648537146240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3ythk1dh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5652b7be80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880310.524602 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.525269 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.525973 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.108469 47841029092224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:51:51.109264 47902120391552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880310.565392 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.565858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.566263 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.108543 47158973145984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.570903 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.571330 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.571679 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.108592 47938770842496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
I0618 11:51:51.109625 47648537146240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.109586 47841029092224 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0o9yikp3
I0618 11:51:51.110306 47841029092224 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0o9yikp3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b832421be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.110631 47841029092224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.110003 47158973145984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3lh75lar
W0618 11:51:51.110500 47045635363712 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpaz2qg5cc
I0618 11:51:51.111099 47158973145984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3lh75lar', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae4566dee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.111223 47045635363712 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpaz2qg5cc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac9f2f89e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.110523 47938770842496 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz3tfnz6p
I0618 11:51:51.111540 47158973145984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.111559 47045635363712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.111604 47938770842496 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz3tfnz6p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b99e5fe8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.112048 47938770842496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.115811 47325685142400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.115984 47525060367232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.116882 47917123695488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.117008 48008943899520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.116865 47648537146240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.116917 47733377942400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.116914 47902120391552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.116923 47600480547712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.118471 47045635363712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.118682 47841029092224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.118849 47158973145984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.120139 47325685142400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:51.118897 47938770842496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.120361 47525060367232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:51.121727 47917123695488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:51.121926 48008943899520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:51:51.125215 47325685142400 estimator.py:1111] Calling model_fn.
W0618 11:51:51.125327 47325685142400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:51:51.125480 47525060367232 estimator.py:1111] Calling model_fn.
W0618 11:51:51.125594 47525060367232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:51.126742 47325685142400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:51.127011 47525060367232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:51:51.127445 47917123695488 estimator.py:1111] Calling model_fn.
W0618 11:51:51.127565 47917123695488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:51:51.127681 48008943899520 estimator.py:1111] Calling model_fn.
W0618 11:51:51.127806 48008943899520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:51.129138 47917123695488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:51.129397 48008943899520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:51.138258 47600480547712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.138322 47648537146240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.138162 47045635363712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.138298 47733377942400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.138422 47902120391552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.138307 47841029092224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.141063 47158973145984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:51.141489 47938770842496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880310.608872 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.609261 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.609586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.143377 47793850450816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.610056 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.610430 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.610748 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.143407 47191020807040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.548273 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.549125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.549819 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.143560 47620759049088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.547069 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.547917 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.548794 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.143566 47448681870208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.558908 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.559694 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.560334 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.146162 47176580674432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.555864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.556566 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.557266 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.146195 47348260242304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.616258 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.616698 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.617124 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.146234 47705694610304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880310.616101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880310.616532 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880310.616957 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:51.146251 47344261862272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:51:51.144909 47793850450816 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpynw388pb
I0618 11:51:51.146004 47793850450816 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpynw388pb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b782810ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.145361 47191020807040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4yicf40e
W0618 11:51:51.147238 47176580674432 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmyip1a90
W0618 11:51:51.147274 47348260242304 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxnnyhdi9
W0618 11:51:51.147308 47705694610304 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp84ja98bu
W0618 11:51:51.147333 47344261862272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpefuw0_cu
I0618 11:51:51.146432 47793850450816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.146438 47191020807040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4yicf40e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aebcc9e7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.148334 47348260242304 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxnnyhdi9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1068d1de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.148327 47176580674432 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmyip1a90', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae86feb8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.148381 47705694610304 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp84ja98bu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b63a1916e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.145775 47620759049088 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpm3ag76_3
I0618 11:51:51.148405 47344261862272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpefuw0_cu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0f7a7f5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:51.145805 47448681870208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplnk3zxpm
I0618 11:51:51.146876 47191020807040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.148767 47348260242304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.146897 47620759049088 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpm3ag76_3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4fdb03ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.148762 47176580674432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.146926 47448681870208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplnk3zxpm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b27ca6a4dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:51.148825 47705694610304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.148851 47344261862272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.147356 47620759049088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:51.147391 47448681870208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:51.155147 46978451276672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.155391 47554630579072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.156314 47450343080832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.156446 47747830686592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:51.155245 47191020807040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.155238 47793850450816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:51.155296 47448681870208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future vers[2019-06-18 11:52:30] iteration time 0: 56.135 seconds
2019-06-18 11:52:31.297437: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880350.919186 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 11:52:34] minmax time: 3.260 seconds
2019-06-18 11:52:34.567398: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:52:34.573075: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:52:34.577865: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880354.589298 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 0}}
[2019-06-18 11:52:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:52:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=2 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=1023779833 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=2047559664 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=3071339495 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=4095119326 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=5118899157 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=6142678988 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=7166458819 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=8190238650 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=9214018481 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=10237798312 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=11261578143 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=12285357974 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=13309137805 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=14332917636 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=15356697467 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=16380477298 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=17404257129 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=18428036960 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=19451816791 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000001-000001 --seed=20475596622 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_
[2019-06-18 11:52:47] eval finished: 13.271 seconds
[2019-06-18 11:52:47] Win rate 000001-000001 vs checkpoint: 0.770
:::MLL 1560880367.926518 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:52:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=3 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=1023779834 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=2047559665 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=3071339496 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=4095119327 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=5118899158 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=6142678989 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=7166458820 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=8190238651 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=9214018482 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=10237798313 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=11261578144 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=12285357975 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=13309137806 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=14332917637 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=15356697468 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=16380477299 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=17404257130 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000002-000000 --seed=18428036961 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:53:15] selfplay finished: 27.966 seconds
[2019-06-18 11:53:15] selfplay mn: 27.983 seconds
[2019-06-18 11:53:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-3-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779834 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559665 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339496 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119327 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899158 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678989 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458820 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238651 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018482 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798313 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578144 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357975 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137806 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917637 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697468 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477299 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257130 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036961 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816792 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596623 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376454 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156285 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:53:19] train finished: 44.467 seconds
:::MLL 1560880359.838675 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.839391 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.840078 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.907405 47967389299584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.840823 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.841557 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.842222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.907411 46987567195008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.908635 47967389299584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgn9wjj_k
W0618 11:52:39.908626 46987567195008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu04i1quj
I0618 11:52:39.909680 47967389299584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgn9wjj_k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba08fc98e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.909692 46987567195008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu04i1quj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc6dd6ae80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.910108 47967389299584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.910137 46987567195008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:39.915271 47967389299584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.915271 46987567195008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880359.885979 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.886478 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.886933 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.925076 47398411936640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.896522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.896999 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.897392 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.925254 47202306003840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.926106 47398411936640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_hqhf4cg
I0618 11:52:39.926305 47202306003840 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee6d44dd68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.927121 47398411936640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_hqhf4cg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1c1617fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.927478 47202306003840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.927532 47398411936640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:39.932496 47398411936640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.932516 47202306003840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.935528 47967389299584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.935496 46987567195008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.952141 47398411936640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.952304 47202306003840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880359.892904 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.893848 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.894670 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.961753 47450466874240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.914428 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.915274 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.916042 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.961872 47352699380608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.962783 47450466874240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpze8fcsym
W0618 11:52:39.962873 47352699380608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxfali9ni
I0618 11:52:39.963814 47450466874240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpze8fcsym', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2834cf4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.963898 47352699380608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxfali9ni', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b117169ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.964233 47450466874240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.964312 47352699380608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:39.969474 47450466874240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.969543 47352699380608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880359.899158 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.900030 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.900852 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.972132 47164435014528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.899574 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.900474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.901217 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.972159 47239343739776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.973244 47239343739776 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnn250fbr
W0618 11:52:39.973270 47164435014528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0_n8v4oo
I0618 11:52:39.974292 47239343739776 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnn250fbr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af70ce3ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.974301 47164435014528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0_n8v4oo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae59bfb6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.974710 47164435014528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.974708 47239343739776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:39.979664 47164435014528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.979662 47239343739776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.983403 46987567195008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:39.983495 47967389299584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:39.987712 46987567195008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:39.987807 47967389299584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880359.922192 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.922907 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.923616 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.987256 47316474422144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.915969 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.916901 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.917768 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.987270 47869739295616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.989466 47352699380608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.989474 47450466874240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.988341 47316474422144 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps5yx1s2c
W0618 11:52:39.988366 47869739295616 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptsbhv3le
I0618 11:52:39.989414 47316474422144 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps5yx1s2c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b09023c8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.989429 47869739295616 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptsbhv3le', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b89d364be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:39.989843 47316474422144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.989866 47869739295616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:39.992788 46987567195008 estimator.py:1111] Calling model_fn.
I0618 11:52:39.992866 47967389299584 estimator.py:1111] Calling model_fn.
W0618 11:52:39.992900 46987567195008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:39.992970 47967389299584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:39.994266 46987567195008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:39.994333 47967389299584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:39.995078 47869739295616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:39.995096 47316474422144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880359.971128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.971564 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.971944 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.999439 47571221840768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.999844 47164435014528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:39.999937 47239343739776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880359.928078 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.928878 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.929697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.998490 47070770688896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.929404 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.930222 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.930932 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:39.998607 47977283560320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:39.999415 47398411936640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:40.000267 47202306003840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880359.972934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.973371 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.973755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.000970 47384982995840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:40.000699 47571221840768 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1obu02pd
W0618 11:52:39.999552 47977283560320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnc50ips3
W0618 11:52:39.999494 47070770688896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4kn8ingn
I0618 11:52:40.001739 47571221840768 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1obu02pd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b44525dde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.000473 47070770688896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4kn8ingn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acfcd274e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.000521 47977283560320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnc50ips3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba2dd87fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.002164 47571221840768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.000869 47070770688896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.000911 47977283560320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.001951 47384982995840 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu0ts5l9y
I0618 11:52:40.002960 47384982995840 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu0ts5l9y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18f5aa9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.003376 47384982995840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.003704 47398411936640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:40.004597 47202306003840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:40.005542 47977283560320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.005563 47070770688896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.007131 47571221840768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.008229 47384982995840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880359.972472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.972909 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.973299 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.006348 47859909968768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.975690 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.976114 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.976472 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.006449 47006476034944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.940827 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.941713 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.942590 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.008824 47047718331264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.953180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.953872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.954601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.008859 47499281515392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
I0618 11:52:40.008771 47398411936640 estimator.py:1111] Calling model_fn.
W0618 11:52:40.008877 47398411936640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:40.007661 47859909968768 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcyq14gvl
W0618 11:52:40.007689 47006476034944 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpoqhjsgod
I0618 11:52:40.008639 47859909968768 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcyq14gvl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8789851e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.008661 47006476034944 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpoqhjsgod', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0d4e4be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:40.009798 47047718331264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp64hpkda5
W0618 11:52:40.009827 47499281515392 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnhd2s80t
I0618 11:52:40.009741 47202306003840 estimator.py:1111] Calling model_fn.
W0618 11:52:40.009846 47202306003840 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:40.010771 47047718331264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp64hpkda5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca6f202e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.009023 47859909968768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.010792 47499281515392 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnhd2s80t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3392638da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.009044 47006476034944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.011165 47047718331264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.010254 47398411936640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:40.011184 47499281515392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.011210 47202306003840 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:40.013740 47006476034944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.013761 47859909968768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.015927 47047718331264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.015926 47499281515392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.016230 47869739295616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.016232 47316474422144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880359.993020 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.993460 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.993840 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.017493 47152179929984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880359.992083 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.992457 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.992815 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.020085 47728384107392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:40.018476 47152179929984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmph0ja6x47
:::MLL 1560880359.993531 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.993956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.994284 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.020652 47264536777600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
I0618 11:52:40.019516 47152179929984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmph0ja6x47', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae2c1858e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.019957 47152179929984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.021487 47728384107392 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmwggqewt
W0618 11:52:40.021581 47264536777600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmply9uikjc
I0618 11:52:40.022464 47728384107392 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmwggqewt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68e9f7be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.022529 47264536777600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmply9uikjc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afcea831e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.022857 47728384107392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.022922 47264536777600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880359.999103 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880359.999518 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880359.999881 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.022735 47973786579840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:40.023766 47973786579840 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwnler3vp
W0618 11:52:40.024732 47152179929984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:52:40.024774 47973786579840 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwnler3vp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba20d184e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.025174 47973786579840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.025537 47977283560320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.025562 47070770688896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.027465 47728384107392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.027477 47264536777600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:40.027862 47571221840768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.028993 47384982995840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.029679 47973786579840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880360.002899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880360.003328 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880360.003685 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.031788 47416109478784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880360.003736 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880360.004112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880360.004430 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:40.031879 47935730324352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:52:40.033146 47416109478784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprv15os6q
W0618 11:52:40.033177 47935730324352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzipf7az3
I0618 11:52:40.034190 47416109478784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprv15os6q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2034f30e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.034211 47935730324352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzipf7az3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9930c3ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:40.034599 47416109478784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:40.034615 47935730324352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:40.033730 47006476034944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.033743 47859909968768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.035639 47499281515392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.035719 47047718331264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:40.037734 47450466874240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:40.037958 47352699380608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprec[2019-06-18 11:53:19] divide_golden_chunk finished: 3.305 seconds
[2019-06-18 11:53:19] generate golden chunk: 3.319 seconds
[2019-06-18 11:53:19] moving /lfs/lfs12/gma_akey/results/epb162/models/000002-000001.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000002-000002.meta
[2019-06-18 11:53:19] moving /lfs/lfs12/gma_akey/results/epb162/models/000002-000001.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000002-000002.data-00000-of-00001
[2019-06-18 11:53:19] moving /lfs/lfs12/gma_akey/results/epb162/models/000002-000001.index --> /lfs/lfs12/gma_akey/results/epb162/models/000002-000002.index
[2019-06-18 11:53:19] moving /lfs/lfs12/gma_akey/results/epb162/models/000002-000001.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb
[2019-06-18 11:53:19] iteration time 1: 48.354 seconds
2019-06-18 11:53:19.677328: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880399.272557 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 11:53:22] minmax time: 3.229 seconds
2019-06-18 11:53:22.916477: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:53:22.922110: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:53:22.926775: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880402.936952 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 1}}
[2019-06-18 11:53:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:53:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=3 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=1023779834 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=2047559665 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=3071339496 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=4095119327 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=5118899158 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=6142678989 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=7166458820 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=8190238651 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=9214018482 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=10237798313 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=11261578144 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=12285357975 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=13309137806 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=14332917637 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=15356697468 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=16380477299 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=17404257130 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=18428036961 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=19451816792 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000002-000002 --seed=20475596623 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:53:33] eval finished: 10.792 seconds
[2019-06-18 11:53:33] Win rate 000002-000002 vs 000001-000001: 0.520
:::MLL 1560880413.796428 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 11:53:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=4 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=1023779835 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=2047559666 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=3071339497 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=4095119328 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=5118899159 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=6142678990 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=7166458821 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=8190238652 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=9214018483 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=10237798314 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=11261578145 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=12285357976 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=13309137807 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=14332917638 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=15356697469 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=16380477300 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=17404257131 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000003-000001 --seed=18428036962 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:54:05] selfplay finished: 32.016 seconds
[2019-06-18 11:54:05] selfplay mn: 32.033 seconds
[2019-06-18 11:54:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-4-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779835 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559666 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339497 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119328 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899159 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678990 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458821 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238652 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018483 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798314 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578145 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357976 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137807 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917638 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697469 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477300 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257131 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036962 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816793 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596624 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376455 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156286 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:54:07] train finished: 44.349 seconds
:::MLL 1560880408.194138 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.194886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.195682 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.265433 47263629681536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.196047 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.196822 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.197516 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.265480 47363538678656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.266495 47263629681536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx87ogu2r
W0618 11:53:28.266526 47363538678656 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvwi0coeh
I0618 11:53:28.267592 47263629681536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx87ogu2r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afcb471fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.267615 47363538678656 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvwi0coeh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b13f77c4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.268033 47263629681536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.268061 47363538678656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.273358 47363538678656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.273371 47263629681536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880408.207800 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.208588 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.209265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.278879 47239517315968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.206289 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.207127 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.207955 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.278870 47553328366464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.279965 47553328366464 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk0nqlkm7
W0618 11:53:28.280000 47239517315968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8gp_lwyi
I0618 11:53:28.281024 47553328366464 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk0nqlkm7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4027d51e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.281068 47239517315968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8gp_lwyi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af7173c7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.281444 47553328366464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.281493 47239517315968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.286589 47553328366464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.286638 47239517315968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880408.258603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.259096 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.259478 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.293400 47142938620800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.294565 47263629681536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.294591 47363538678656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.294449 47142938620800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2bafvyfv
I0618 11:53:28.295528 47142938620800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2bafvyfv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae09ab28e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.295969 47142938620800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880408.259336 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.259781 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.260159 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.297467 47609429775232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
I0618 11:53:28.298557 47609429775232 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d37bcad68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.299782 47609429775232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.301117 47142938620800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.304818 47609429775232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.306709 47239517315968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.306702 47553328366464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880408.248409 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.249148 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.249815 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.315152 47359077020544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.241664 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.242557 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.243381 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.315186 47481228612480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.282898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.283353 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.283774 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.317708 46943618212736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.284343 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.284751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.285120 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.317742 47397063840640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.316177 47359077020544 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmg99rblv
W0618 11:53:28.316215 47481228612480 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppjqv8o6q
I0618 11:53:28.317277 47359077020544 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmg99rblv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b12ed8cce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.317336 47481228612480 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppjqv8o6q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2f5e5a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.317715 47359077020544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.317806 47481228612480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.318746 46943618212736 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplltsbqvf
W0618 11:53:28.318779 47397063840640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg50smf0n
I0618 11:53:28.319771 46943618212736 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplltsbqvf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab232467e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.319807 47397063840640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg50smf0n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1bc5bdae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.320195 46943618212736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.320231 47397063840640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.322483 47142938620800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.322483 47359077020544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.322652 47481228612480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.325179 46943618212736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.325187 47397063840640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.325848 47609429775232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880408.251380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.252286 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.253148 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.337555 46934459880320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.305631 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.306175 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.306642 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.337947 47591649330048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.259496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.260234 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.260907 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.337574 47168310834048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.282499 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.283309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.284027 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.339570 46974513873792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.261986 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.262909 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.263802 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.339652 47067281769344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.256615 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.257538 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.258403 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.340470 47977774441344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.262761 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.263454 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.264156 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.340499 47000966484864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.338977 47591649330048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpayhl2oca
W0618 11:53:28.340528 46974513873792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpveixq5cz
W0618 11:53:28.340586 47067281769344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy82u8wvu
W0618 11:53:28.338657 46934459880320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqg9cdf_l
W0618 11:53:28.338686 47168310834048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpe2p324tu
I0618 11:53:28.341506 46974513873792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpveixq5cz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab963ccce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.340044 47591649330048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpayhl2oca', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4913f08e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.341557 47067281769344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy82u8wvu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acefd329e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.339776 46934459880320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqg9cdf_l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab010656e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.339886 47168310834048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpe2p324tu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae682ffbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.341900 46974513873792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.341956 47067281769344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.340470 47591649330048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.340216 46934459880320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.340358 47168310834048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.341536 47977774441344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8gcxkhfn
W0618 11:53:28.341564 47000966484864 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc7ye7x_a
I0618 11:53:28.342568 47977774441344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8gcxkhfn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba2faca3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.342608 47000966484864 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc7ye7x_a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abf8c7fae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.342984 47977774441344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.343032 47000966484864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.342540 47359077020544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.343115 47363538678656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:28.343507 47263629681536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:28.343054 47481228612480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.344845 46943618212736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.344861 47397063840640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.346859 47067281769344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.346859 46974513873792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880408.313907 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.314360 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.314751 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.345453 47417043256192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.345624 47591649330048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.345560 46934459880320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.345749 47168310834048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.348208 47977774441344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.348358 47000966484864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.347529 47363538678656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:28.346488 47417043256192 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmlcb4bug
W0618 11:53:28.347943 47263629681536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:53:28.347497 47417043256192 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmlcb4bug', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b206c9b5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.347908 47417043256192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.352763 47363538678656 estimator.py:1111] Calling model_fn.
W0618 11:53:28.352890 47363538678656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:53:28.353203 47263629681536 estimator.py:1111] Calling model_fn.
W0618 11:53:28.352560 47417043256192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.353316 47263629681536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:28.354929 47553328366464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:28.354353 47363538678656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:28.355383 47239517315968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:28.354753 47263629681536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880408.301911 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.302301 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.302665 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.355584 47851230753664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.302979 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.303373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.303706 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.355649 47823791514496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.356670 47851230753664 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvect75mi
W0618 11:53:28.356701 47823791514496 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7xg9yd1s
I0618 11:53:28.357713 47851230753664 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvect75mi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b858432cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:53:28.359262 47553328366464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:53:28.357751 47823791514496 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7xg9yd1s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7f20b13dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.358132 47851230753664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:28.359755 47239517315968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:53:28.358172 47823791514496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:28.364405 47553328366464 estimator.py:1111] Calling model_fn.
W0618 11:53:28.364511 47553328366464 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:28.362974 47851230753664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:28.363001 47823791514496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:53:28.364896 47239517315968 estimator.py:1111] Calling model_fn.
W0618 11:53:28.365008 47239517315968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:28.365881 47553328366464 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:28.366383 47239517315968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:28.366825 46974513873792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.366923 47067281769344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.365620 47591649330048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.366786 46934459880320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.367230 47168310834048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.370790 47977774441344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.371390 47000966484864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:28.371490 47142938620800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:28.372205 47417043256192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880408.342532 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.342986 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.343365 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.375664 47367936611200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.341362 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.341817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.342251 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.375658 47131903218560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880408.342390 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.342784 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.343105 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.375692 47658761225088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.375004 47609429775232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880408.341522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880408.341910 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880408.342272 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:28.375764 47306901988224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:53:28.376144 47142938620800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:28.376706 47131903218560 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwsvx4ytp
W0618 11:53:28.376734 47367936611200 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprx9bffgi
W0618 11:53:28.376756 47658761225088 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4xlc5gof
W0618 11:53:28.376785 47306901988224 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpw7ebr54i
I0618 11:53:28.377740 47131903218560 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwsvx4ytp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade08ef9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.377763 47367936611200 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprx9bffgi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14fd9f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.377810 47658761225088 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4xlc5gof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b58b41ede10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:28.377814 47306901988224 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpw7ebr54i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b06c7acfda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas'[2019-06-18 11:54:09] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 11:54:09] generate golden chunk: 3.316 seconds
[2019-06-18 11:54:09] moving /lfs/lfs12/gma_akey/results/epb162/models/000003-000002.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000003-000003.data-00000-of-00001
[2019-06-18 11:54:09] moving /lfs/lfs12/gma_akey/results/epb162/models/000003-000002.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb
[2019-06-18 11:54:09] moving /lfs/lfs12/gma_akey/results/epb162/models/000003-000002.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000003-000003.meta
[2019-06-18 11:54:09] moving /lfs/lfs12/gma_akey/results/epb162/models/000003-000002.index --> /lfs/lfs12/gma_akey/results/epb162/models/000003-000003.index
[2019-06-18 11:54:09] iteration time 2: 49.915 seconds
2019-06-18 11:54:09.634227: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880449.188062 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 11:54:12] minmax time: 3.211 seconds
2019-06-18 11:54:12.855883: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:54:12.861602: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:54:12.866132: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880452.876654 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 2}}
[2019-06-18 11:54:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:54:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=4 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=1023779835 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=2047559666 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=3071339497 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=4095119328 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=5118899159 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=6142678990 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=7166458821 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=8190238652 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=9214018483 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=10237798314 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=11261578145 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=12285357976 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=13309137807 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=14332917638 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=15356697469 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=16380477300 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=17404257131 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=18428036962 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=19451816793 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000003-000003 --seed=20475596624 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:54:24] eval finished: 11.650 seconds
[2019-06-18 11:54:24] Win rate 000003-000003 vs 000002-000002: 0.290
:::MLL 1560880464.592878 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 11:54:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=5 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=1023779836 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=2047559667 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=3071339498 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=4095119329 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=5118899160 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=6142678991 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=7166458822 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=8190238653 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=9214018484 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=10237798315 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=11261578146 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=12285357977 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=13309137808 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=14332917639 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=15356697470 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=16380477301 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=17404257132 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000004-000002 --seed=18428036963 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:54:54] selfplay finished: 30.164 seconds
[2019-06-18 11:54:54] selfplay mn: 30.184 seconds
[2019-06-18 11:54:54] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-5-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779836 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559667 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339498 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119329 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899160 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678991 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458822 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238653 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018484 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798315 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578146 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357977 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137808 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917639 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697470 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477301 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257132 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036963 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816794 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596625 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376456 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156287 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:54:57] train finished: 44.350 seconds
:::MLL 1560880458.189250 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.189926 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.190628 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.257181 47660132680576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.186496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.187266 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.187953 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.257155 47626885251968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.258187 47626885251968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpktvjag0x
W0618 11:54:18.258218 47660132680576 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4xhf7vdr
I0618 11:54:18.259168 47626885251968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpktvjag0x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b514829fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.259228 47660132680576 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4xhf7vdr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5905dd9e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.259578 47626885251968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.259639 47660132680576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880458.203524 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.204300 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.205066 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.263498 47394418664320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.191684 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.192585 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.193361 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.263529 47141132776320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.264486 47141132776320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9g0c0ue7
W0618 11:54:18.264516 47394418664320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnt2l_lnr
W0618 11:54:18.264648 47660132680576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.264641 47626885251968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:54:18.265520 47141132776320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9g0c0ue7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae02f0f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.265536 47394418664320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnt2l_lnr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b28137e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.265973 47141132776320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.265981 47394418664320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.270902 47141132776320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.270961 47394418664320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.284965 47660132680576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.285020 47626885251968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.290963 47141132776320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.290976 47394418664320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880458.253058 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.253492 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.253875 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.294516 46994430342016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.295593 46994430342016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc6xdgzfw
I0618 11:54:18.296684 46994430342016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc6xdgzfw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abe06ea0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.297167 46994430342016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880458.254882 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.255336 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.255705 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.298818 47671094031232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 11:54:18.299868 47671094031232 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b93368d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.301075 47671094031232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880458.265453 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.265908 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.266297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.302218 47816801674112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.302239 46994430342016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.303216 47816801674112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyw7wmoda
I0618 11:54:18.304301 47816801674112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyw7wmoda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d8010be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880458.269170 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.269625 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.270036 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.304506 47968245576576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 11:54:18.304738 47816801674112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.305541 47968245576576 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa5ogi6k3
I0618 11:54:18.306490 47968245576576 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa5ogi6k3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0c2d34e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:54:18.305861 47671094031232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:54:18.306875 47968245576576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.309487 47816801674112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.311522 47968245576576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.322144 46994430342016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880458.250872 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.251716 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.252536 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.323945 47513781609344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.251577 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.252445 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.253161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.324039 47830391100288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.325452 47671094031232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.325136 47513781609344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpokolzm6x
W0618 11:54:18.325167 47830391100288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplbu255jh
I0618 11:54:18.326225 47513781609344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpokolzm6x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36f2a96e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.326264 47830391100288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplbu255jh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b80aa0eee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.326667 47513781609344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.326716 47830391100288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.328871 47816801674112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880458.260020 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.260759 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.261470 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.330623 47541084164992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.258326 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.259068 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.259847 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.330784 47098556949376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.331193 47968245576576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.331615 47541084164992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9ert2sj_
W0618 11:54:18.331747 47098556949376 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzvyn94wz
I0618 11:54:18.332598 47541084164992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9ert2sj_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3d4e055e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.332729 47098556949376 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzvyn94wz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad64557fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.333044 47541084164992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.333148 47098556949376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.331978 47830391100288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.332004 47513781609344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.333663 47660132680576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.333768 47626885251968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.338150 47541084164992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.338266 47098556949376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.337984 47660132680576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:18.338074 47626885251968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:18.339039 47141132776320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.339402 47394418664320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.343356 47141132776320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:18.343728 47394418664320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:54:18.343047 47660132680576 estimator.py:1111] Calling model_fn.
I0618 11:54:18.343107 47626885251968 estimator.py:1111] Calling model_fn.
W0618 11:54:18.343154 47660132680576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:18.343214 47626885251968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:18.344497 47660132680576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:18.344545 47626885251968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:54:18.348402 47141132776320 estimator.py:1111] Calling model_fn.
W0618 11:54:18.348510 47141132776320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:54:18.348808 47394418664320 estimator.py:1111] Calling model_fn.
W0618 11:54:18.348916 47394418664320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:18.349864 47141132776320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:18.350268 47394418664320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880458.306901 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.307340 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.307692 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.350274 47847903925120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.305836 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.306220 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.306593 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.350286 47361953620864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.351333 47847903925120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp44xh9jzu
W0618 11:54:18.351374 47361953620864 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpiq0a1oxu
I0618 11:54:18.352359 47847903925120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp44xh9jzu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b84bde75e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.352467 47361953620864 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpiq0a1oxu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1399023e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.352770 47847903925120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.352851 47361953620864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880458.275865 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.276764 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.277636 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.355065 47933948900224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.281593 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.282331 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.283035 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.355329 46937057756032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.354247 47513781609344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.354355 47830391100288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.356050 47933948900224 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphtnr4ydy
W0618 11:54:18.356283 46937057756032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp08_d8y73
I0618 11:54:18.357015 47933948900224 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphtnr4ydy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98c6958e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.357265 46937057756032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp08_d8y73', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab0ab3dce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.357406 47933948900224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.357676 46937057756032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.357456 47847903925120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.357528 47361953620864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.360713 47541084164992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.361274 47098556949376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.362328 47933948900224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.362443 46937057756032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880458.312725 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.313632 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.314373 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.368386 47197442937728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.299090 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.299987 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.300848 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.368460 47002181792640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.369747 46994430342016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.369373 47197442937728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfyi7nn24
W0618 11:54:18.369400 47002181792640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9kffn8ft
I0618 11:54:18.370347 47197442937728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfyi7nn24', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aed4b685dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.370351 47002181792640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9kffn8ft', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abfd4efce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.370733 47197442937728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.370738 47002181792640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880458.335058 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.335483 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.336011 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.372422 47258909483904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.336382 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.336769 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.337101 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.372509 47314411373440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.372774 47671094031232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.373392 47258909483904 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbr5n9jyt
W0618 11:54:18.373491 47314411373440 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfhfz582t
I0618 11:54:18.374363 47258909483904 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbr5n9jyt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afb9b197da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.374458 47314411373440 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfhfz582t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0887451e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.374752 47258909483904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.374850 47314411373440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.374072 46994430342016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:18.375699 47816801674112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880458.324120 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.324654 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.325081 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.375786 47810980451200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880458.327610 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880458.328088 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880458.328486 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:18.375819 47096848016256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:54:18.375483 47197442937728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.375480 47002181792640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.376759 47810980451200 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmpgfiujr
W0618 11:54:18.376785 47096848016256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpblaqmsa_
I0618 11:54:18.377715 47810980451200 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmpgfiujr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c2517ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:18.377776 47096848016256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpblaqmsa_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5df7bbda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:54:18.377096 47671094031232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:54:18.378153 47810980451200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:18.378195 47096848016256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:18.378245 47968245576576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:18.377112 47847903925120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.377120 47361953620864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.379338 47258909483904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:18.379401 47314411373440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:54:18.379135 46994430342016 estimator.py:1111] Calling model_fn.
W0618 11:54:18.379953 47816801674112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:18.379248 46994430342016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:18.380622 46994430342016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:18.382199 47933948900224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.382472 46937057756032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:18.382533 47968245576576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be remov[2019-06-18 11:54:58] divide_golden_chunk finished: 3.367 seconds
[2019-06-18 11:54:58] generate golden chunk: 3.382 seconds
[2019-06-18 11:54:58] iteration time 3: 48.973 seconds
2019-06-18 11:54:58.629379: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880498.161112 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 11:55:01] minmax time: 3.213 seconds
2019-06-18 11:55:01.852839: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:55:01.858499: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:55:01.862901: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880501.874994 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 3}}
[2019-06-18 11:55:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:55:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=5 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=1023779836 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=2047559667 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=3071339498 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=4095119329 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=5118899160 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=6142678991 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=7166458822 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=8190238653 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=9214018484 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=10237798315 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=11261578146 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=12285357977 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=13309137808 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=14332917639 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=15356697470 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=16380477301 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=17404257132 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=18428036963 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=19451816794 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000004-000003 --seed=20475596625 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:55:13] eval finished: 11.190 seconds
[2019-06-18 11:55:13] Win rate 000004-000003 vs 000002-000002: 0.720
:::MLL 1560880513.132215 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 11:55:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=6 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=1023779837 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=2047559668 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=3071339499 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=4095119330 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=5118899161 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=6142678992 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=7166458823 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=8190238654 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=9214018485 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=10237798316 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=11261578147 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=12285357978 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=13309137809 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=14332917640 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=15356697471 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=16380477302 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=17404257133 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000005-000002 --seed=18428036964 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:55:42] selfplay finished: 29.539 seconds
[2019-06-18 11:55:42] selfplay mn: 29.558 seconds
[2019-06-18 11:55:42] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-6-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779837 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559668 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339499 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119330 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899161 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678992 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458823 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238654 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018485 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798316 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578147 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357978 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137809 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917640 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697471 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477302 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257133 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036964 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816795 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596626 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376457 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156288 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:55:45] train finished: 43.783 seconds
:::MLL 1560880507.086935 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.087804 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.088654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.160385 47741711344512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.099339 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.100116 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.100863 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.160406 48006109684608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.161440 47741711344512 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcy1fu_ce
W0618 11:55:07.161468 48006109684608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_0kueh6z
I0618 11:55:07.162533 47741711344512 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcy1fu_ce', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c04551e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.162552 48006109684608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_0kueh6z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba993b3ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.162925 47741711344512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.162952 48006109684608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.167959 47741711344512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.167979 48006109684608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880507.098052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.098849 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.099654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.170197 47975557923712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.098653 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.099494 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.100185 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.170279 46926648968064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.171236 47975557923712 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpehfoomfp
W0618 11:55:07.171314 46926648968064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqqtfs6um
I0618 11:55:07.172284 47975557923712 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpehfoomfp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba276acde80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.172362 46926648968064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqqtfs6um', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae3ed45e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.172706 47975557923712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.172784 46926648968064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.177623 46926648968064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.177633 47975557923712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.187739 48006109684608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.187829 47741711344512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880507.123060 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.123937 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.124782 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.195166 46915032458112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.196247 46915032458112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpurcf157l
I0618 11:55:07.197381 46915032458112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpurcf157l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab8a6e7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.197865 46915032458112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880507.156341 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.157167 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.157922 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.197939 47660625351552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.197611 46926648968064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.197640 47975557923712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.198962 47660625351552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvesj102d
I0618 11:55:07.200007 47660625351552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvesj102d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b59233b1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.200425 47660625351552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.203213 46915032458112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880507.164453 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.164900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.165267 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.204028 47744103158656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.205715 47660625351552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880507.167206 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.167652 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.168036 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.204989 47702205666176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.205042 47744103158656 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvq65f89l
I0618 11:55:07.206040 47744103158656 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvq65f89l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c92e55e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.206465 47744103158656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.206019 47702205666176 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b62d19c5d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.207188 47702205666176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.211357 47744103158656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.211848 47702205666176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880507.175418 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.175790 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.176126 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.214423 47406874014592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.174106 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.174486 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.174803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.214418 47610856817536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.215522 47406874014592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdxd8jroa
W0618 11:55:07.215494 47610856817536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6g6w1g0t
I0618 11:55:07.216600 47406874014592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdxd8jroa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1e0e790e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.216600 47610856817536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6g6w1g0t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d8ccb8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.217039 47406874014592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.217042 47610856817536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.221926 47610856817536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.221939 47406874014592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.223121 46915032458112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.225402 47660625351552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.230852 47744103158656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.231421 47702205666176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.236208 48006109684608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.236459 47741711344512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.240559 48006109684608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.240832 47741711344512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.241739 47610856817536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.241859 47406874014592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:55:07.245631 48006109684608 estimator.py:1111] Calling model_fn.
W0618 11:55:07.245738 48006109684608 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:07.245956 47741711344512 estimator.py:1111] Calling model_fn.
W0618 11:55:07.246073 47741711344512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:07.245745 47975557923712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.247113 48006109684608 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.246404 46926648968064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.247455 47741711344512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.250049 47975557923712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.250761 46926648968064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:55:07.255116 47975557923712 estimator.py:1111] Calling model_fn.
W0618 11:55:07.255226 47975557923712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:07.255872 46926648968064 estimator.py:1111] Calling model_fn.
W0618 11:55:07.255981 46926648968064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880507.179552 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.180482 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.181327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.256947 46954817139584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.190255 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.191005 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.191721 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.257065 47459927225216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.256580 47975557923712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880507.198442 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.199253 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.200028 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.256117 47647798911872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.182549 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.183474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.184347 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.256280 47509963789184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.257350 46926648968064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.257971 46954817139584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppaikzhc7
W0618 11:55:07.258030 47459927225216 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqr4mpjtd
I0618 11:55:07.258954 46954817139584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppaikzhc7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab4cdc88da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.259010 47459927225216 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqr4mpjtd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2a68b0bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.259348 46954817139584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.259402 47459927225216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.257215 47647798911872 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp16yu61ub
W0618 11:55:07.257365 47509963789184 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmph9n1zb7r
I0618 11:55:07.258343 47647798911872 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp16yu61ub', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5626b72e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.258475 47509963789184 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmph9n1zb7r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b360f1a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.258787 47647798911872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.258938 47509963789184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.264359 47459927225216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.264449 46954817139584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.264152 47647798911872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.264199 47509963789184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880507.227625 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.228022 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.228354 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.266930 47510521447296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.228934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.229341 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.229668 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.266942 47241448096640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.267945 47241448096640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpammqo430
W0618 11:55:07.267974 47510521447296 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6s8_ybus
I0618 11:55:07.268990 47241448096640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpammqo430', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af78a51de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.269002 47510521447296 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6s8_ybus', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3630575e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.269410 47241448096640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.269417 47510521447296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.274412 47510521447296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.274428 47241448096640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.275546 46915032458112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.277775 47660625351552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.278692 47702205666176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.278746 47744103158656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.279844 46915032458112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.282201 47660625351552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880507.211860 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.212679 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.213392 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.281522 47476790317952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.210562 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.211442 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.212233 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.281492 47117937546112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.283011 47702205666176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.283053 47744103158656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.284247 46954817139584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.284426 47459927225216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880507.247141 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.247636 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.248118 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.284464 46968833708928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880507.243706 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.244194 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.244626 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.282803 47818584802176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.282530 47476790317952 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk02uod52
W0618 11:55:07.282561 47117937546112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp077vye4m
I0618 11:55:07.283600 47476790317952 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk02uod52', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e55cf2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.283615 47117937546112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp077vye4m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adac8844e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.285115 46915032458112 estimator.py:1111] Calling model_fn.
W0618 11:55:07.285230 46915032458112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:07.284024 47476790317952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.284039 47117937546112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.285532 46968833708928 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfv8tt10r
W0618 11:55:07.283878 47818584802176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgizfz5b_
W0618 11:55:07.286586 46915032458112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:55:07.286619 46968833708928 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfv8tt10r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab8113c6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.284929 47818584802176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgizfz5b_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7dea591e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.287060 46968833708928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.285350 47818584802176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.287367 47660625351552 estimator.py:1111] Calling model_fn.
W0618 11:55:07.287476 47660625351552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:07.286509 47647798911872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.286641 47509963789184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.288826 47660625351552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880507.249783 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.250296 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.250694 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.286933 47461428929408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
I0618 11:55:07.288061 47702205666176 estimator.py:1111] Calling model_fn.
I0618 11:55:07.288112 47744103158656 estimator.py:1111] Calling model_fn.
W0618 11:55:07.288168 47702205666176 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:07.288218 47744103158656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880507.251894 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880507.252329 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880507.252712 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:07.289368 47585715442560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:55:07.289465 47610856817536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.289659 47406874014592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:07.288927 47476790317952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.289546 47702205666176 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.288896 47117937546112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.289572 47744103158656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.287969 47461428929408 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpsx1xlt2_
I0618 11:55:07.289026 47461428929408 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpsx1xlt2_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ac232ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:55:07.290369 47585715442560 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5xbgeizu
I0618 11:55:07.289463 47461428929408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:07.291385 47585715442560 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5xbgeizu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47b240ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:07.291797 47585715442560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:07.292159 46968833708928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.290413 47818584802176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.293768 47610856817536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.293992 47510521447296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.293981 47406874014592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:07.294269 47241448096640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:07.294394 47461428929408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:07.296652 47585715442560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:07.298810 47610856817536 estimator.py:1111] Calling model_fn.
W0618 11:55:07.298919 47610856817536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:07.299050 47406874014592 estimator.py:1111] Calling model_fn.
W0618 11:55:07.299157 47406874014592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:07.300280 47610856817536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:07.300514 47406874014592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618[2019-06-18 11:55:46] divide_golden_chunk finished: 3.415 seconds
[2019-06-18 11:55:46] generate golden chunk: 3.430 seconds
[2019-06-18 11:55:46] moving /lfs/lfs12/gma_akey/results/epb162/models/000005-000003.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000005-000004.meta
[2019-06-18 11:55:46] moving /lfs/lfs12/gma_akey/results/epb162/models/000005-000003.index --> /lfs/lfs12/gma_akey/results/epb162/models/000005-000004.index
[2019-06-18 11:55:46] moving /lfs/lfs12/gma_akey/results/epb162/models/000005-000003.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000005-000004.data-00000-of-00001
[2019-06-18 11:55:46] moving /lfs/lfs12/gma_akey/results/epb162/models/000005-000003.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb
[2019-06-18 11:55:46] iteration time 4: 48.008 seconds
2019-06-18 11:55:46.687974: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880546.168839 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 11:55:49] minmax time: 3.220 seconds
2019-06-18 11:55:49.918462: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:55:49.924360: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:55:49.929128: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880549.939691 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 4}}
[2019-06-18 11:55:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:55:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=6 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=1023779837 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=2047559668 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=3071339499 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=4095119330 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=5118899161 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=6142678992 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=7166458823 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=8190238654 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=9214018485 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=10237798316 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=11261578147 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=12285357978 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=13309137809 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=14332917640 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=15356697471 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=16380477302 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=17404257133 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=18428036964 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=19451816795 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000005-000004 --seed=20475596626 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:56:01] eval finished: 11.322 seconds
[2019-06-18 11:56:01] Win rate 000005-000004 vs 000004-000003: 0.590
:::MLL 1560880561.326820 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 11:56:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=7 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=1023779838 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=2047559669 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=3071339500 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=4095119331 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=5118899162 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=6142678993 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=7166458824 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=8190238655 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=9214018486 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=10237798317 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=11261578148 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=12285357979 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=13309137810 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=14332917641 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=15356697472 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=16380477303 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=17404257134 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000006-000003 --seed=18428036965 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:56:30] selfplay finished: 28.918 seconds
[2019-06-18 11:56:30] selfplay mn: 28.936 seconds
[2019-06-18 11:56:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-7-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779838 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559669 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339500 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119331 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899162 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678993 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458824 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238655 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018486 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798317 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578148 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357979 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137810 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917641 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697472 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477303 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257134 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036965 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816796 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596627 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376458 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156289 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:56:33] divide_golden_chunk finished: 3.232 seconds
[2019-06-18 11:56:33] generate golden chunk: 3.248 seconds
[2019-06-18 11:56:33] train finished: 43.600 seconds
:::MLL 1560880555.200814 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.201553 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.202238 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.273729 47210693256064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.203825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.204547 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.205215 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.273903 47950736020352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.274807 47210693256064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2nwenzsb
W0618 11:55:55.274913 47950736020352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxoszyzpb
I0618 11:55:55.275894 47210693256064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2nwenzsb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af061303e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.275973 47950736020352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxoszyzpb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9caf2cae80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.276312 47210693256064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.276391 47950736020352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.281421 47210693256064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.281437 47950736020352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.302159 47210693256064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.302545 47950736020352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880555.231491 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.232229 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.232923 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.305310 47027951469440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.234402 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.235153 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.235836 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.305315 47369704510336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.306326 47027951469440 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu5vep1a7
W0618 11:55:55.306356 47369704510336 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgaz838r2
I0618 11:55:55.307317 47027951469440 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu5vep1a7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac5d4eddda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.307336 47369704510336 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgaz838r2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1566ff3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.307702 47027951469440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.307732 47369704510336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.312977 47369704510336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.313015 47027951469440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880555.266648 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.267087 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.267503 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.316030 47798199141248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.267685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.268124 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.268492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.316166 47205166904192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.317035 47798199141248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu5v8f9e8
I0618 11:55:55.317166 47205166904192 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aef17cacd68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.317986 47798199141248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu5v8f9e8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b792b449e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.318266 47205166904192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.318383 47798199141248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.322982 47205166904192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.323042 47798199141248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.332764 47369704510336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.332956 47027951469440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880555.264443 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.265200 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.265914 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.334614 47894735053696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.251783 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.252720 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.253553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.334665 47666829722496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.335748 47894735053696 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp10stsa9t
W0618 11:55:55.335780 47666829722496 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphilnfwj8
I0618 11:55:55.336818 47666829722496 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphilnfwj8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5a950a5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.336823 47894735053696 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp10stsa9t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8fa541be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.337248 47894735053696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.337245 47666829722496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880555.263035 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.263785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.264485 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.336130 47962883052416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.258603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.259540 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.260345 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.336381 47884659278720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.337136 47962883052416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp82m7qw0i
W0618 11:55:55.337395 47884659278720 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdkr16_0l
I0618 11:55:55.338172 47962883052416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp82m7qw0i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9f8331ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.338448 47884659278720 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdkr16_0l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d4cb19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.338587 47962883052416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.338900 47884659278720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.342375 47666829722496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.342467 47894735053696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880555.264408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.265106 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.265759 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.341183 47517297533824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.342561 47798199141248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.342579 47205166904192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880555.258887 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.259789 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.260659 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.341711 47476963767168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.342281 47517297533824 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp1iwrpfw
I0618 11:55:55.343365 47517297533824 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp1iwrpfw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b37c43a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:55:55.343753 47962883052416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.342807 47476963767168 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqx4zkevr
I0618 11:55:55.343797 47517297533824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.344157 47884659278720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:55.343907 47476963767168 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqx4zkevr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e6025ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.344359 47476963767168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880555.309544 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.310077 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.310672 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.348691 47918022132608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.349775 47918022132608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwbe_o8si
W0618 11:55:55.349058 47517297533824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:55.350871 47918022132608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwbe_o8si', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9511465da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.351315 47918022132608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.349700 47476963767168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.350951 47210693256064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.351244 47950736020352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.355270 47210693256064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:55.355592 47950736020352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:55.356537 47918022132608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880555.313629 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.314065 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.314448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.359106 47587442557824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.360047 47587442557824 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxanegish
I0618 11:55:55.361068 47587442557824 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxanegish', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4819324e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.360352 47210693256064 estimator.py:1111] Calling model_fn.
W0618 11:55:55.360463 47210693256064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:55.360677 47950736020352 estimator.py:1111] Calling model_fn.
I0618 11:55:55.361478 47587442557824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.360790 47950736020352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:55.361831 47210693256064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.362166 47950736020352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.364866 47666829722496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.363796 47962883052416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.365225 47894735053696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.364578 47884659278720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.366230 47587442557824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880555.322097 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.322555 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.322953 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.366049 47544530424704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.318345 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.318894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.319373 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.366247 47299636843392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.367102 47544530424704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfalpo425
W0618 11:55:55.367310 47299636843392 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnnpjrixv
I0618 11:55:55.368180 47544530424704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfalpo425', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3e1b6f2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.368381 47299636843392 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnnpjrixv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0516a39e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.368619 47544530424704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.368830 47299636843392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.371409 47517297533824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.372729 47476963767168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880555.329097 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.329501 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.329854 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.373178 47798275060608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.373708 47544530424704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.373895 47299636843392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880555.331480 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.331872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.332237 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.374127 47196777698176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.374185 47798275060608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy45qvpmx
W0618 11:55:55.376582 47918022132608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:55:55.375225 47798275060608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy45qvpmx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b792fcafe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.375640 47798275060608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.375124 47196777698176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxurkfgrr
I0618 11:55:55.376161 47196777698176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxurkfgrr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aed23c19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.376581 47196777698176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.380483 47369704510336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880555.340441 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.340828 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.341169 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.380771 47110545167232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.339622 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.340186 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.340528 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.380926 47338612937600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.381324 47027951469440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.380520 47798275060608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.381811 47110545167232 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6_n45trc
W0618 11:55:55.381929 47338612937600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzxvpp5uk
I0618 11:55:55.382849 47110545167232 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6_n45trc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad90fe5ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.382965 47338612937600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzxvpp5uk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e29cbae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:55:55.381352 47196777698176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:55.383266 47110545167232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.383383 47338612937600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.384778 47369704510336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880555.323640 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.324592 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.325377 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.384935 47684430738304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.311878 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.312793 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.313635 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.384998 47579116405632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.385700 47027951469440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:55.386033 47587442557824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.385975 47684430738304 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpslwxs1vf
W0618 11:55:55.386018 47579116405632 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprsddndeh
I0618 11:55:55.387031 47684430738304 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpslwxs1vf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5eae247da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.387053 47579116405632 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprsddndeh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4628eb3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.387454 47684430738304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:55.387480 47579116405632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:55.388093 47110545167232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.388189 47338612937600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:55.389801 47369704510336 estimator.py:1111] Calling model_fn.
W0618 11:55:55.389910 47369704510336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:55.390818 47027951469440 estimator.py:1111] Calling model_fn.
W0618 11:55:55.390927 47027951469440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:55.390149 47205166904192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.390346 47798199141248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.391254 47369704510336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.392295 47027951469440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.392536 47684430738304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.392515 47579116405632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:55.394449 47205166904192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:55.394653 47798199141248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:55.394846 47299636843392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.394939 47544530424704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:55:55.399508 47205166904192 estimator.py:1111] Calling model_fn.
W0618 11:55:55.399616 47205166904192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:55.399699 47798199141248 estimator.py:1111] Calling model_fn.
W0618 11:55:55.399806 47798199141248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:55.400974 47205166904192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.400192 47798275060608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.401163 47798199141248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:55.401051 47196777698176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.408329 47110545167232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.408461 47338612937600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.412788 47579116405632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.412958 47684430738304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:55.411791 47962883052416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.411796 47884659278720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880555.369576 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.370063 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.370519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.414616 47444901086080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880555.373052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880555.373528 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880555.373930 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:55.414643 47487106040704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:55:55.415025 47666829722496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.415848 47894735053696 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:55.415721 47487106040704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgfvp2bmr
W0618 11:55:55.415682 47444901086080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpr71m6sw7
I0618 11:55:55.416729 47444901086080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpr71m6sw7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b26e9101e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:55.416760 47487106040704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgfvp2bmr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': [2019-06-18 11:56:33] moving /lfs/lfs12/gma_akey/results/epb162/models/000006-000004.index --> /lfs/lfs12/gma_akey/results/epb162/models/000006-000005.index
[2019-06-18 11:56:33] moving /lfs/lfs12/gma_akey/results/epb162/models/000006-000004.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000006-000005.data-00000-of-00001
[2019-06-18 11:56:33] moving /lfs/lfs12/gma_akey/results/epb162/models/000006-000004.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb
[2019-06-18 11:56:33] moving /lfs/lfs12/gma_akey/results/epb162/models/000006-000004.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000006-000005.meta
[2019-06-18 11:56:33] iteration time 5: 47.442 seconds
2019-06-18 11:56:34.156402: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880593.611454 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 11:56:37] minmax time: 3.270 seconds
2019-06-18 11:56:37.436585: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:56:37.442035: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:56:37.446483: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880597.457052 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 5}}
[2019-06-18 11:56:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:56:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=7 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=1023779838 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=2047559669 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=3071339500 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=4095119331 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=5118899162 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=6142678993 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=7166458824 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=8190238655 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=9214018486 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=10237798317 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=11261578148 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=12285357979 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=13309137810 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=14332917641 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=15356697472 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=16380477303 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=17404257134 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=18428036965 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=19451816796 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000006-000005 --seed=20475596627 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:56:49] eval finished: 11.769 seconds
[2019-06-18 11:56:49] Win rate 000006-000005 vs 000005-000004: 0.420
:::MLL 1560880609.294029 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 11:56:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=8 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=1023779839 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=2047559670 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=3071339501 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=4095119332 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=5118899163 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=6142678994 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=7166458825 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=8190238656 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=9214018487 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=10237798318 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=11261578149 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=12285357980 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=13309137811 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=14332917642 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=15356697473 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=16380477304 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=17404257135 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000007-000004 --seed=18428036966 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:57:17] selfplay finished: 28.695 seconds
[2019-06-18 11:57:18] selfplay mn: 28.713 seconds
[2019-06-18 11:57:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-8-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779839 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559670 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339501 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119332 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899163 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678994 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458825 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238656 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018487 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798318 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578149 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357980 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137811 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917642 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697473 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477304 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257135 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036966 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816797 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596628 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376459 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156290 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:57:20] train finished: 43.423 seconds
:::MLL 1560880602.643581 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.644407 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.645195 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.720080 47779025290112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.644305 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.645196 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.645915 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.720070 47890159100800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.721158 47779025290112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmu8ljyjq
W0618 11:56:42.721188 47890159100800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnpv16u1q
I0618 11:56:42.722257 47779025290112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmu8ljyjq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b74b46ace80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.722279 47890159100800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnpv16u1q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e94823e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.722701 47779025290112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.722721 47890159100800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.728050 47890159100800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.728063 47779025290112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880602.675317 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.676023 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.676704 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.742211 47625834660736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.665946 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.666862 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.667684 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.742175 47292490744704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.743324 47625834660736 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpal0oryha
W0618 11:56:42.743290 47292490744704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmh02k9yb
I0618 11:56:42.744347 47292490744704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmh02k9yb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b036cb2de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.744356 47625834660736 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpal0oryha', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b51098b5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.744761 47292490744704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.744775 47625834660736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.748044 47890159100800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.748069 47779025290112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.749947 47625834660736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.749992 47292490744704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880602.703585 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.704068 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.704448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.750552 47664472748928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.751633 47664472748928 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa7o0w95w
I0618 11:56:42.752707 47664472748928 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa7o0w95w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5a088dbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.753163 47664472748928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880602.702842 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.703373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.703843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.754061 47298968499072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 11:56:42.755086 47298968499072 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04eecd8d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.756307 47298968499072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.758232 47664472748928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880602.685845 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.686592 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.687238 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.759945 47020803289984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.682656 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.683475 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.684140 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.760081 47461913944960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.761148 47298968499072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.761003 47020803289984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg9uj20pk
W0618 11:56:42.761099 47461913944960 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpctf_oe55
I0618 11:56:42.762082 47020803289984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg9uj20pk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac42add4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.762179 47461913944960 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpctf_oe55', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2adf1bbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.762535 47020803289984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.762610 47461913944960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.767568 47020803289984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.767614 47461913944960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.770146 47625834660736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.771121 47292490744704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.778014 47664472748928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880602.704408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.705320 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.706115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.780253 47101505610624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.704139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.704986 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.705804 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.780257 47996444447616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.780752 47298968499072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.781391 47996444447616 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvjit6tm1
W0618 11:56:42.781430 47101505610624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9dj0kh57
I0618 11:56:42.782478 47996444447616 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvjit6tm1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7539beda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.782550 47101505610624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9dj0kh57', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad6f518fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.782942 47996444447616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.783016 47101505610624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.788229 47996444447616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.788415 47101505610624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880602.745760 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.746172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.746526 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.790506 47725923443584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.748136 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.748602 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.748998 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.790795 47745218372480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.790043 47020803289984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.790537 47461913944960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.791560 47725923443584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3yon1l0h
W0618 11:56:42.791825 47745218372480 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphv80x94x
I0618 11:56:42.792525 47725923443584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3yon1l0h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68574cfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.792788 47745218372480 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphv80x94x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6cd55e3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.792911 47725923443584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.793179 47745218372480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.796061 47779025290112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.796561 47890159100800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.797620 47725923443584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.797857 47745218372480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.800360 47779025290112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:42.800901 47890159100800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:42.805440 47779025290112 estimator.py:1111] Calling model_fn.
W0618 11:56:42.805549 47779025290112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:42.805984 47890159100800 estimator.py:1111] Calling model_fn.
W0618 11:56:42.806093 47890159100800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:42.806913 47779025290112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.807459 47890159100800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880602.756934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.757446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.757832 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.808749 47466763445120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.756992 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.757517 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.757905 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.808960 47136194409344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.810876 47996444447616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.811318 47101505610624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.809783 47466763445120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8g3y7zkt
W0618 11:56:42.809982 47136194409344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmbdyppc7
I0618 11:56:42.810807 47466763445120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8g3y7zkt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c00293e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.811010 47136194409344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmbdyppc7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf08b60e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.811252 47466763445120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.811462 47136194409344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880602.771615 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.772055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.772432 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.816118 47314432160640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.817238 47725923443584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.817588 47745218372480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.816067 47466763445120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.816226 47136194409344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.817164 47314432160640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnpgx1x77
:::MLL 1560880602.775154 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.775567 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.775948 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.818028 47393064809344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 11:56:42.818204 47314432160640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnpgx1x77', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0888824e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.818640 47314432160640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.818742 47625834660736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.819125 47292490744704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.819040 47393064809344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpou_16906
I0618 11:56:42.820063 47393064809344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpou_16906', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ad7614e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.820444 47393064809344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.823106 47625834660736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:42.823394 47314432160640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.823445 47292490744704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:42.824999 47393064809344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.825927 47664472748928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:56:42.828215 47625834660736 estimator.py:1111] Calling model_fn.
W0618 11:56:42.828324 47625834660736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:42.828525 47292490744704 estimator.py:1111] Calling model_fn.
W0618 11:56:42.828635 47292490744704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:42.827912 47298968499072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.829685 47625834660736 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.829998 47292490744704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.830246 47664472748928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880602.747036 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.747942 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.748812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.829627 47259922965376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.759206 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.759931 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.760628 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.829979 47526085530496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.830678 47259922965376 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbb7ndxfo
W0618 11:56:42.832188 47298968499072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:42.831729 47259922965376 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbb7ndxfo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afbd781fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:42.831042 47526085530496 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa21pw4qd
I0618 11:56:42.832126 47526085530496 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa21pw4qd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39d0083e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.832158 47259922965376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.832562 47526085530496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880602.750274 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.751166 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.751935 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.834163 47468333921152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880602.749496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880602.750379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880602.751204 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:42.834285 47324650369920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:56:42.835236 47468333921152 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpl49frba2
W0618 11:56:42.835274 47324650369920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5kwa5xls
I0618 11:56:42.836263 47468333921152 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpl49frba2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c5dc4cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.836306 47324650369920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5kwa5xls', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0ae98fce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:42.835369 47664472748928 estimator.py:1111] Calling model_fn.
W0618 11:56:42.835476 47664472748928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:42.836674 47468333921152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:42.836715 47324650369920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:42.835718 47466763445120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.835775 47136194409344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.836842 47664472748928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:56:42.837224 47298968499072 estimator.py:1111] Calling model_fn.
W0618 11:56:42.837331 47298968499072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:42.837479 47259922965376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.837777 47526085530496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.838684 47298968499072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.842069 47324650369920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.842088 47468333921152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:42.842957 47314432160640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.844485 47393064809344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.843909 47020803289984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.844132 47461913944960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:42.848819 47020803289984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:42.849026 47461913944960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:42.854563 47020803289984 estimator.py:1111] Calling model_fn.
W0618 11:56:42.854680 47020803289984 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:42.854757 47461913944960 estimator.py:1111] Calling model_fn.
W0618 11:56:42.854878 47461913944960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:42.856216 47020803289984 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.856423 47461913944960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:42.858029 47259922965376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:42.858741 47526085530496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions [2019-06-18 11:57:21] divide_golden_chunk finished: 3.316 seconds
[2019-06-18 11:57:21] generate golden chunk: 3.330 seconds
[2019-06-18 11:57:21] iteration time 6: 47.727 seconds
2019-06-18 11:57:21.948899: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880641.338933 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 11:57:25] minmax time: 3.221 seconds
2019-06-18 11:57:25.180251: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:57:25.185626: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:57:25.190249: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880645.202650 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 6}}
[2019-06-18 11:57:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir 
[2019-06-18 11:57:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=8 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=1023779839 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=2047559670 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=3071339501 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=4095119332 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=5118899163 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=6142678994 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=7166458825 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=8190238656 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=9214018487 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=10237798318 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=11261578149 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=12285357980 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=13309137811 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=14332917642 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=15356697473 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=16380477304 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=17404257135 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=18428036966 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=19451816797 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000007-000005 --seed=20475596628 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:57:35] eval finished: 10.530 seconds
[2019-06-18 11:57:35] Win rate 000007-000005 vs 000005-000004: 0.590
:::MLL 1560880655.799291 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 11:57:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=9 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=1023779840 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=2047559671 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=3071339502 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=4095119333 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=5118899164 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=6142678995 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=7166458826 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=8190238657 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=9214018488 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=10237798319 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=11261578150 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=12285357981 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=13309137812 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=14332917643 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=15356697474 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=16380477305 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=17404257136 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000008-000004 --seed=18428036967 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/
[2019-06-18 11:58:05] selfplay finished: 29.774 seconds
[2019-06-18 11:58:05] selfplay mn: 29.796 seconds
[2019-06-18 11:58:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-9-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779840 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559671 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339502 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119333 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899164 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678995 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458826 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238657 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018488 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798319 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578150 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357981 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137812 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917643 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697474 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477305 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257136 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036967 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816798 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596629 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376460 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156291 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_
[2019-06-18 11:58:08] divide_golden_chunk finished: 3.316 seconds
[2019-06-18 11:58:08] generate golden chunk: 3.331 seconds
[2019-06-18 11:58:08] train finished: 43.749 seconds
:::MLL 1560880650.428401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.429302 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.430104 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.505862 47073152893824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.428472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.429363 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.430152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.505936 47248293774208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.506901 47073152893824 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8zb2oe93
W0618 11:57:30.506953 47248293774208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxszv7g6b
I0618 11:57:30.507886 47073152893824 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8zb2oe93', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad05b24de80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.507933 47248293774208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxszv7g6b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9225a9e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.508283 47073152893824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.508324 47248293774208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.513275 47073152893824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.513293 47248293774208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880650.467253 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.468078 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.468851 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.527939 47455102432128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.452420 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.453271 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.454121 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.528019 47164210996096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.528947 47455102432128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpybv45gjk
W0618 11:57:30.529003 47164210996096 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk45lmpqx
I0618 11:57:30.529942 47455102432128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpybv45gjk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b29491c3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.530001 47164210996096 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk45lmpqx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae58ea13e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.530346 47455102432128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.530400 47164210996096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.533101 47073152893824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.533156 47248293774208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.535521 47164210996096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.535544 47455102432128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880650.491473 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.492029 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.492499 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.536047 47474643813248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.537115 47474643813248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5s_8ugzi
I0618 11:57:30.538189 47474643813248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5s_8ugzi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2dd5de0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.538628 47474643813248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.543829 47474643813248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880650.498967 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.499455 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.499858 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.547799 47379352200064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
I0618 11:57:30.548791 47379352200064 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b17a60b6d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.549891 47379352200064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.554476 47379352200064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.555486 47455102432128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.555507 47164210996096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.563546 47474643813248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880650.502388 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.503135 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.503779 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.574150 47884360340352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.499637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.500390 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.501068 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.574208 47598493971328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.574093 47379352200064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.575183 47884360340352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqb7vyp0q
W0618 11:57:30.575212 47598493971328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3hijzugx
I0618 11:57:30.576198 47884360340352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqb7vyp0q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d3ae02e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.576212 47598493971328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3hijzugx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4aabe98e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.576597 47884360340352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.576608 47598493971328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.581805 47884360340352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.581863 47598493971328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.581450 47073152893824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.581909 47248293774208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.585789 47073152893824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880650.540499 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.540908 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.541276 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.586671 47254808859520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.539052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.539471 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.539820 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.586688 47912084358016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.586251 47248293774208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.587682 47254808859520 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7a2maali
W0618 11:57:30.587725 47912084358016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptivbb2hy
I0618 11:57:30.588723 47254808859520 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7a2maali', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afaa6aeeda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.588763 47912084358016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptivbb2hy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93af5b0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.589150 47254808859520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.589185 47912084358016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.590880 47073152893824 estimator.py:1111] Calling model_fn.
W0618 11:57:30.590988 47073152893824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:30.591364 47248293774208 estimator.py:1111] Calling model_fn.
W0618 11:57:30.591475 47248293774208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.592360 47073152893824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.592849 47248293774208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.594051 47912084358016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.594076 47254808859520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.602508 47884360340352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.602728 47598493971328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.603881 47164210996096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.604154 47455102432128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.608239 47164210996096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.608510 47455102432128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.611370 47474643813248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:57:30.613291 47164210996096 estimator.py:1111] Calling model_fn.
W0618 11:57:30.613398 47164210996096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.613574 47912084358016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:57:30.613593 47455102432128 estimator.py:1111] Calling model_fn.
W0618 11:57:30.613701 47455102432128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.613732 47254808859520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.614738 47164210996096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.615072 47455102432128 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.615680 47474643813248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:57:30.620762 47474643813248 estimator.py:1111] Calling model_fn.
W0618 11:57:30.620869 47474643813248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.621961 47379352200064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.622233 47474643813248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.626288 47379352200064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:57:30.631357 47379352200064 estimator.py:1111] Calling model_fn.
W0618 11:57:30.631465 47379352200064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.632846 47379352200064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880650.569008 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.569742 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.570422 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.635388 47308072145792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.560589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.561480 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.562341 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.635424 47942566253440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.636434 47308072145792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpevzn8ls8
W0618 11:57:30.636476 47942566253440 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdw6mlrs6
I0618 11:57:30.637458 47308072145792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpevzn8ls8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b070d6c2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.637501 47942566253440 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdw6mlrs6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ac837ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.637885 47308072145792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.637925 47942566253440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880650.594028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.594403 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.594727 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.639700 46985798755200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.593230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.593617 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.593988 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.639801 47326818526080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.640700 46985798755200 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpct01r1yy
W0618 11:57:30.640779 47326818526080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpda0ymvp5
I0618 11:57:30.641669 46985798755200 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpct01r1yy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc046e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.641730 47326818526080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpda0ymvp5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b6acb2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.642077 46985798755200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.642121 47326818526080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.642706 47942566253440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.642710 47308072145792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.646983 47326818526080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.646986 46985798755200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880650.567264 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.568203 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.569039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.648453 47181915657088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.571852 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.572619 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.573329 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.648555 47860973323136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.649515 47181915657088 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpaqa7z40n
W0618 11:57:30.649550 47860973323136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8h81sk03
I0618 11:57:30.650575 47181915657088 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpaqa7z40n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9ade8ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.650577 47860973323136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8h81sk03', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b87c8e68e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.651000 47181915657088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.651001 47860973323136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.654900 47884360340352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.656048 47181915657088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.656048 47860973323136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.655975 47598493971328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.659199 47884360340352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880650.562789 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.563729 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.564608 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.658430 47678307246976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.562789 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.563735 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.564599 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.658432 47300644848512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.660325 47598493971328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.661168 47912084358016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.661296 47254808859520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:30.659602 47300644848512 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg0krz8gv
W0618 11:57:30.659570 47678307246976 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps86y4vj2
I0618 11:57:30.660687 47300644848512 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg0krz8gv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0552b88e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.660710 47678307246976 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps86y4vj2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5d41276e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.661129 47300644848512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.661144 47678307246976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:30.662337 47308072145792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.662633 47942566253440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:57:30.664257 47884360340352 estimator.py:1111] Calling model_fn.
W0618 11:57:30.664363 47884360340352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:30.665426 47598493971328 estimator.py:1111] Calling model_fn.
W0618 11:57:30.665533 47598493971328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.665453 47912084358016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.665596 47254808859520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:30.665731 47884360340352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.666573 47326818526080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.666601 46985798755200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.666899 47598493971328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.666531 47300644848512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.666554 47678307246976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:57:30.670511 47912084358016 estimator.py:1111] Calling model_fn.
W0618 11:57:30.670617 47912084358016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:30.670664 47254808859520 estimator.py:1111] Calling model_fn.
W0618 11:57:30.670772 47254808859520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:30.671975 47912084358016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.672118 47254808859520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:30.676121 47860973323136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:30.676188 47181915657088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880650.624190 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.624746 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.625191 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.679919 47703909643136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.627919 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.628376 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.628760 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.680042 47638250546048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.680967 47703909643136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpeluys6lz
W0618 11:57:30.681100 47638250546048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3bi0ty3a
I0618 11:57:30.681986 47703909643136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpeluys6lz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b63372d0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.682131 47638250546048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3bi0ty3a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53ed96be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.682400 47703909643136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.682539 47638250546048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880650.634290 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.634722 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.635144 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.680935 47452450059136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.635558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.636000 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.636364 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.681085 47901167788928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.681981 47452450059136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxtm6p51e
W0618 11:57:30.682090 47901167788928 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9xx992h1
I0618 11:57:30.683023 47452450059136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxtm6p51e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b28ab044e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.683122 47901167788928 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9xx992h1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9124ad7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:30.683443 47452450059136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:30.683535 47901167788928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880650.615451 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.615913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.616323 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.684089 47049862763392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880650.614344 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880650.614763 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880650.615204 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:30.684117 46975875285888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:57:30.687451 47703909643136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:30.687506 47638250546048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-[2019-06-18 11:58:08] moving /lfs/lfs12/gma_akey/results/epb162/models/000008-000005.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000008-000006.meta
[2019-06-18 11:58:08] moving /lfs/lfs12/gma_akey/results/epb162/models/000008-000005.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb
[2019-06-18 11:58:08] moving /lfs/lfs12/gma_akey/results/epb162/models/000008-000005.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000008-000006.data-00000-of-00001
[2019-06-18 11:58:09] moving /lfs/lfs12/gma_akey/results/epb162/models/000008-000005.index --> /lfs/lfs12/gma_akey/results/epb162/models/000008-000006.index
[2019-06-18 11:58:09] iteration time 7: 47.682 seconds
2019-06-18 11:58:09.654795: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880689.021212 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 11:58:12] minmax time: 3.246 seconds
2019-06-18 11:58:12.910622: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:58:12.916308: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:58:12.921041: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880692.932202 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 7}}
[2019-06-18 11:58:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 11:58:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=9 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=1023779840 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=2047559671 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=3071339502 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=4095119333 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=5118899164 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=6142678995 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=7166458826 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=8190238657 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=9214018488 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=10237798319 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=11261578150 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=12285357981 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=13309137812 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=14332917643 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=15356697474 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=16380477305 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=17404257136 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=18428036967 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=19451816798 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000008-000006 --seed=20475596629 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:58:24] eval finished: 11.358 seconds
[2019-06-18 11:58:24] Win rate 000008-000006 vs 000007-000005: 0.650
:::MLL 1560880704.357538 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 11:58:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=10 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=1023779841 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=2047559672 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=3071339503 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=4095119334 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=5118899165 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=6142678996 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=7166458827 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=8190238658 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=9214018489 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=10237798320 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=11261578151 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=12285357982 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=13309137813 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=14332917644 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=15356697475 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=16380477306 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=17404257137 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000009-000005 --seed=18428036968 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 11:58:53] selfplay finished: 29.539 seconds
[2019-06-18 11:58:53] selfplay mn: 29.556 seconds
[2019-06-18 11:58:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-10-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779841 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559672 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339503 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119334 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899165 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678996 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458827 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238658 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018489 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798320 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578151 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357982 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137813 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917644 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697475 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477306 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257137 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036968 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816799 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596630 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376461 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156292 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 11:58:56] train finished: 43.762 seconds
:::MLL 1560880698.165673 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.166534 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.167327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.240934 47735275451264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.172542 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.173264 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.173916 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.241019 47219287888768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.242064 47735275451264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpccv13l6c
W0618 11:58:18.242114 47219287888768 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzwyq0ebs
I0618 11:58:18.243056 47735275451264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpccv13l6c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a84b94e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.243090 47219287888768 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzwyq0ebs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af26177de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.243448 47735275451264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.243479 47219287888768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880698.172807 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.173695 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.174545 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.248541 47708839674752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.248374 47735275451264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.248387 47219287888768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.249624 47708839674752 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpeiniey_i
I0618 11:58:18.250707 47708839674752 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpeiniey_i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b645d075e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.251131 47708839674752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880698.203108 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.203911 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.204692 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.251909 47234971190144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.252912 47234971190144 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6vboqk38
I0618 11:58:18.253907 47234971190144 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6vboqk38', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af608440e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.254319 47234971190144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.256233 47708839674752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.259364 47234971190144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.268470 47735275451264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.268675 47219287888768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.275916 47708839674752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880698.223738 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.224266 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.224745 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.275593 47854742012800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.276659 47854742012800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1nw6v6se
I0618 11:58:18.277746 47854742012800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1nw6v6se', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b86557c5e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.278167 47854742012800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880698.231007 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.231450 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.231834 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.278272 47512787698560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.279144 47234971190144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:58:18.279265 47512787698560 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36b76b9cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.280382 47512787698560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.282796 47854742012800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.284951 47512787698560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880698.215146 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.216101 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.216973 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.287804 47874802701184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.288856 47874802701184 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_k321kkq
I0618 11:58:18.289991 47874802701184 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_k321kkq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b01321e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.290456 47874802701184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.296183 47874802701184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.302574 47854742012800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.304515 47512787698560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880698.255561 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.256385 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.257147 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.306823 47200675259264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.307770 47200675259264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8549sys1
I0618 11:58:18.308748 47200675259264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8549sys1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee0c11ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.309150 47200675259264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.313971 47200675259264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.316405 47874802701184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.316549 47735275451264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:18.316787 47219287888768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880698.269912 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.270297 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.270619 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.318386 47507287831424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.268599 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.268980 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.269304 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.318453 47653412467584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.251768 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.252530 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.253217 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.318435 47385441043328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.242761 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.243684 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.244539 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.318490 47752091063168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.319389 47507287831424 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps7p0wngx
W0618 11:58:18.319420 47653412467584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvtt_id0m
I0618 11:58:18.320384 47507287831424 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps7p0wngx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b356f9a3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.320425 47653412467584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvtt_id0m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b57754f4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.320807 47507287831424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.320839 47653412467584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.319453 47385441043328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpeq7vboiv
W0618 11:58:18.319484 47752091063168 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqt0o2v32
I0618 11:58:18.320437 47385441043328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpeq7vboiv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1910f7de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.320859 47735275451264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:58:18.320466 47752091063168 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqt0o2v32', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6e6f032e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.321135 47219287888768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:58:18.320847 47385441043328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.320863 47752091063168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.324023 47708839674752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:18.325452 47507287831424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.325493 47653412467584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:18.325923 47735275451264 estimator.py:1111] Calling model_fn.
W0618 11:58:18.326030 47735275451264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:18.326257 47219287888768 estimator.py:1111] Calling model_fn.
W0618 11:58:18.325783 47385441043328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.326368 47219287888768 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.325789 47752091063168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.327264 47234971190144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:18.327395 47735275451264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880698.245107 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.245884 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.246571 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.326450 47317423432576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.328299 47708839674752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880698.247289 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.247969 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.248633 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.326575 47029411455872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.327751 47219287888768 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:18.327558 47317423432576 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0ybzhuri
W0618 11:58:18.327646 47029411455872 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzpshjw6d
I0618 11:58:18.328683 47317423432576 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0ybzhuri', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b093acd5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.328767 47029411455872 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzpshjw6d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac62bf36e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.329135 47317423432576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.329213 47029411455872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.331587 47234971190144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:58:18.333327 47708839674752 estimator.py:1111] Calling model_fn.
W0618 11:58:18.333437 47708839674752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.334780 47708839674752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:18.334939 47200675259264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.334387 47317423432576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.334412 47029411455872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:18.336651 47234971190144 estimator.py:1111] Calling model_fn.
W0618 11:58:18.336760 47234971190144 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.338120 47234971190144 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:18.344908 47507287831424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.345008 47653412467584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.345568 47385441043328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.345760 47752091063168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.350227 47854742012800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:18.351768 47512787698560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880698.299828 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.300416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.300926 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.352823 47018097820544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.305398 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.305832 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.306252 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.353360 47881020302208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.354548 47854742012800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:18.353831 47018097820544 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpirripka1
I0618 11:58:18.354811 47018097820544 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpirripka1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3899b1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.354324 47881020302208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppmb67s8s
I0618 11:58:18.355211 47018097820544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.355293 47881020302208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppmb67s8s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c73cb3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.356097 47512787698560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:58:18.355689 47881020302208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.356632 47317423432576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:18.356762 47029411455872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880698.288538 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.289303 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.290023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.359544 47861949584256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.285543 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.286345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.287073 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.359603 47418746844032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 11:58:18.359627 47854742012800 estimator.py:1111] Calling model_fn.
W0618 11:58:18.359740 47854742012800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.359818 47018097820544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.360574 47861949584256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgk4gl28j
W0618 11:58:18.360603 47418746844032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkeb3x57h
I0618 11:58:18.361585 47418746844032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkeb3x57h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20d2260e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.361583 47861949584256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgk4gl28j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8803170da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.360325 47881020302208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:18.361982 47861949584256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.361988 47418746844032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.361106 47854742012800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:58:18.361164 47512787698560 estimator.py:1111] Calling model_fn.
W0618 11:58:18.361277 47512787698560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.362658 47512787698560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880698.313977 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.314420 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.314765 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.363215 47378451133312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880698.315663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.316031 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.316357 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.363749 47203363787648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.364209 47378451133312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpn5ilgn6b
I0618 11:58:18.365250 47378451133312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpn5ilgn6b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1770564e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:18.366988 47418746844032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.366979 47861949584256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.364738 47203363787648 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpguse673v
I0618 11:58:18.365672 47378451133312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:18.365753 47203363787648 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpguse673v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeeac516dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.366145 47203363787648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.369554 47874802701184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880698.322685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.323135 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.323517 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.370919 47183928832896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:58:18.370355 47378451133312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.370680 47203363787648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:18.372001 47183928832896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjtak1s0w
:::MLL 1560880698.325901 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880698.326349 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880698.326735 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:18.372722 47913027588992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 11:58:18.372988 47183928832896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjtak1s0w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea25e78e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.373377 47183928832896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.374256 47874802701184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:18.373689 47913027588992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuu6b2lga
I0618 11:58:18.374659 47913027588992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuu6b2lga', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93e7938e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:18.375192 47913027588992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:18.378130 47183928832896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:18.379537 47874802701184 estimator.py:1111] Calling model_fn.
W0618 11:58:18.379670 47874802701184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:18.379816 47913027588992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatical[2019-06-18 11:58:57] divide_golden_chunk finished: 3.323 seconds
[2019-06-18 11:58:57] generate golden chunk: 3.338 seconds
[2019-06-18 11:58:57] moving /lfs/lfs12/gma_akey/results/epb162/models/000009-000006.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb
[2019-06-18 11:58:57] moving /lfs/lfs12/gma_akey/results/epb162/models/000009-000006.index --> /lfs/lfs12/gma_akey/results/epb162/models/000009-000007.index
[2019-06-18 11:58:57] moving /lfs/lfs12/gma_akey/results/epb162/models/000009-000006.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000009-000007.data-00000-of-00001
[2019-06-18 11:58:57] moving /lfs/lfs12/gma_akey/results/epb162/models/000009-000006.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000009-000007.meta
[2019-06-18 11:58:57] iteration time 8: 48.276 seconds
2019-06-18 11:58:57.981515: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880737.296865 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 11:59:01] minmax time: 3.227 seconds
2019-06-18 11:59:01.219555: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:59:01.224982: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:59:01.229598: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880741.240521 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 8}}
[2019-06-18 11:59:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 11:59:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=10 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=1023779841 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=2047559672 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=3071339503 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=4095119334 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=5118899165 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=6142678996 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=7166458827 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=8190238658 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=9214018489 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=10237798320 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=11261578151 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=12285357982 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=13309137813 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=14332917644 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=15356697475 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=16380477306 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=17404257137 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=18428036968 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=19451816799 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000009-000007 --seed=20475596630 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:59:11] eval finished: 10.413 seconds
[2019-06-18 11:59:11] Win rate 000009-000007 vs 000008-000006: 0.000
:::MLL 1560880751.718535 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 11:59:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=11 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=1023779842 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=2047559673 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=3071339504 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=4095119335 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=5118899166 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=6142678997 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=7166458828 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=8190238659 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=9214018490 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=10237798321 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=11261578152 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=12285357983 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=13309137814 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=14332917645 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=15356697476 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=16380477307 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=17404257138 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000010-000006 --seed=18428036969 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 11:59:41] selfplay finished: 29.572 seconds
[2019-06-18 11:59:41] selfplay mn: 29.592 seconds
[2019-06-18 11:59:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-11-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779842 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559673 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339504 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119335 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899166 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678997 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458828 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238659 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018490 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798321 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578152 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357983 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137814 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917645 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697476 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477307 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257138 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036969 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816800 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596631 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376462 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156293 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 11:59:44] divide_golden_chunk finished: 3.317 seconds
[2019-06-18 11:59:44] generate golden chunk: 3.332 seconds
[2019-06-18 11:59:44] train finished: 43.596 seconds
:::MLL 1560880746.459841 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.460740 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.461579 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.538811 47248746455936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.539937 47248746455936 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1rxeqr1x
I0618 11:59:06.541042 47248746455936 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1rxeqr1x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af93d55fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.541480 47248746455936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.546801 47248746455936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880746.490860 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.491635 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.492395 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.550732 47924420998016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.551692 47924420998016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppj2acsc3
I0618 11:59:06.552667 47924420998016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppj2acsc3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b968ead3e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.553094 47924420998016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.557673 47924420998016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.566560 47248746455936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.577458 47924420998016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880746.497955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.498818 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.499607 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.580397 47687626449792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.507498 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.508242 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.508941 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.580516 47839717299072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.581428 47687626449792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnytmb5w8
W0618 11:59:06.581468 47839717299072 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpn8m84tvr
I0618 11:59:06.582415 47687626449792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnytmb5w8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f6c9f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.582423 47839717299072 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpn8m84tvr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b82d5f14da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.582804 47687626449792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.582815 47839717299072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.587840 47687626449792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.587831 47839717299072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880746.545603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.546134 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.546542 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.603369 47165463921536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.529887 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.530594 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.531264 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.604204 47319267144576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.522109 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.523008 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.523891 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.604357 46973945308032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.604395 47165463921536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfu6qw6f9
I0618 11:59:06.605423 47165463921536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfu6qw6f9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5d94f4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:06.605284 47319267144576 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7eedisfv
W0618 11:59:06.605440 46973945308032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbz2dv7wt
I0618 11:59:06.606410 47319267144576 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7eedisfv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b09a8b21e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.606525 46973945308032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbz2dv7wt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab941e93e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.605860 47165463921536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.606867 47319267144576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.606973 46973945308032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.607758 47839717299072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.608427 47687626449792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880746.551705 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.552118 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.552465 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.607877 47379983815552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:59:06.609094 47379983815552 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b17cbb12cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.610296 47379983815552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.610624 47165463921536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.612104 47319267144576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.612144 46973945308032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880746.533973 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.534873 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.535706 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.615381 47134714405760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.614794 47248746455936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:06.615692 47379983815552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.616510 47134714405760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpw1l5_ewp
I0618 11:59:06.617658 47134714405760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpw1l5_ewp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adeb07efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.618129 47134714405760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.619098 47248746455936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.623686 47134714405760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880746.544015 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.544970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.545807 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.622796 47657223750528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.562014 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.562832 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.563605 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.622859 47032680498048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:59:06.624204 47248746455936 estimator.py:1111] Calling model_fn.
W0618 11:59:06.624315 47248746455936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.623880 47657223750528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6mm4msua
W0618 11:59:06.623914 47032680498048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6fq5rz67
W0618 11:59:06.625338 47924420998016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:59:06.624920 47657223750528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6mm4msua', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b58587ace48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.624973 47032680498048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6fq5rz67', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac6eecd1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:06.625697 47248746455936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:06.625344 47657223750528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.625393 47032680498048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.629628 47924420998016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.630116 47165463921536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.630442 47657223750528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.630453 47032680498048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880746.584633 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.585506 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.586311 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.634160 47467197838208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.634444 47319267144576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.634568 46973945308032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:59:06.634711 47924420998016 estimator.py:1111] Calling model_fn.
W0618 11:59:06.634829 47924420998016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.635135 47467197838208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3o2kjhic
I0618 11:59:06.636129 47467197838208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3o2kjhic', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c1a0d7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880746.579464 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.579928 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.580342 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.635877 47986604770176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.584685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.585130 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.585626 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.636024 46951246594944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:59:06.636518 47467197838208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.636196 47924420998016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:06.636873 47986604770176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpl3h7vb08
W0618 11:59:06.637007 46951246594944 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfjm0jagx
I0618 11:59:06.637865 47986604770176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpl3h7vb08', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5091e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.637974 46951246594944 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfjm0jagx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3f8f64e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.638276 47986604770176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.638380 46951246594944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.641269 47467197838208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.642978 47986604770176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.643064 46951246594944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.642663 47379983815552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.644986 47134714405760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880746.599730 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.600156 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.600530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.649322 47360335504256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.650377 47360335504256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx0qip_ks
I0618 11:59:06.651470 47360335504256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx0qip_ks', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b13388fbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.651913 47360335504256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.650705 47657223750528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.650844 47032680498048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880746.604616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.605063 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.605472 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.652662 47809055449984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.571963 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.572901 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.573761 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.651512 47526889268096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.577038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.577763 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.578488 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.651571 47150607741824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.653710 47809055449984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbf8_a1y6
I0618 11:59:06.654800 47809055449984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbf8_a1y6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7bb25abe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:06.652609 47526889268096 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcyrd9n9z
W0618 11:59:06.652644 47150607741824 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp23n1jtl
I0618 11:59:06.655238 47809055449984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.653706 47526889268096 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcyrd9n9z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39fff07dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.653730 47150607741824 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp23n1jtl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae263cffe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.654145 47526889268096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.654167 47150607741824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.656543 47687626449792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:06.656689 47839717299072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:06.657069 47360335504256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.660048 47809055449984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.660943 47467197838208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.660829 47687626449792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.661053 47839717299072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.659610 47150607741824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.659655 47526889268096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.662476 47986604770176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.662798 46951246594944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:59:06.665875 47687626449792 estimator.py:1111] Calling model_fn.
W0618 11:59:06.665984 47687626449792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:06.666163 47839717299072 estimator.py:1111] Calling model_fn.
W0618 11:59:06.666276 47839717299072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.667320 47687626449792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:06.667626 47839717299072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880746.617849 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.618276 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.618655 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.669132 47384843752320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.618352 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.618796 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.619152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.669291 46920613258112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.670134 47384843752320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmevhgtqr
W0618 11:59:06.670231 46920613258112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9xb2csxu
I0618 11:59:06.671103 47384843752320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmevhgtqr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18ed5dee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.671213 46920613258112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9xb2csxu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacd712be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.671494 47384843752320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:06.671610 46920613258112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880746.624910 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.625377 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.625787 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.675749 47149143380864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880746.625342 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.625818 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.626196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.676047 47065722200960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.676686 47360335504256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.676795 47149143380864 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptu9gx39u
W0618 11:59:06.676270 47384843752320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:06.677790 47149143380864 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptu9gx39u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae20c879e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:06.677095 47065722200960 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9zfudid4
W0618 11:59:06.676377 46920613258112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:06.678082 47065722200960 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9zfudid4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acea03d7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.678184 47149143380864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.677375 47165463921536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:59:06.678476 47065722200960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.679596 47809055449984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.681632 47165463921536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.682740 47149143380864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.682971 47065722200960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:06.682097 47150607741824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.682210 47526889268096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.684409 47319267144576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:06.684448 46973945308032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:59:06.686644 47165463921536 estimator.py:1111] Calling model_fn.
W0618 11:59:06.686755 47165463921536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.688704 47319267144576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.688735 46973945308032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.688110 47165463921536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880746.639499 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.639876 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.640211 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.690726 47175651652480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:59:06.693391 47134714405760 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880746.641030 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880746.641404 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880746.641726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:06.691673 47512558998400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:59:06.693744 47319267144576 estimator.py:1111] Calling model_fn.
I0618 11:59:06.693758 46973945308032 estimator.py:1111] Calling model_fn.
W0618 11:59:06.693854 47319267144576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.693864 46973945308032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:06.691689 47175651652480 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo8oyphot
I0618 11:59:06.692667 47175651652480 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo8oyphot', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8388bce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:06.693057 47175651652480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.692624 47512558998400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpj7u3_7lj
I0618 11:59:06.693610 47512558998400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpj7u3_7lj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36a9c9ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:06.695208 47319267144576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:06.695220 46973945308032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:06.693995 47512558998400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:06.695744 47384843752320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.695973 46920613258112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:06.697664 47134714405760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:06.697878 47175651652480 deprecation.py:[2019-06-18 11:59:44] iteration time 9: 47.562 seconds
2019-06-18 11:59:45.596227: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880784.858628 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 11:59:48] minmax time: 3.242 seconds
2019-06-18 11:59:48.848552: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:59:48.854192: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:59:48.858679: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880788.872077 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 9}}
[2019-06-18 11:59:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 11:59:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=11 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=1023779842 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=2047559673 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=3071339504 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=4095119335 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=5118899166 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=6142678997 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=7166458828 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=8190238659 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=9214018490 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=10237798321 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=11261578152 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=12285357983 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=13309137814 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=14332917645 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=15356697476 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=16380477307 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=17404257138 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=18428036969 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=19451816800 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000010-000007 --seed=20475596631 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:00:00] eval finished: 11.775 seconds
[2019-06-18 12:00:00] Win rate 000010-000007 vs 000008-000006: 0.300
:::MLL 1560880800.714797 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 12:00:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=12 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=1023779843 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=2047559674 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=3071339505 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=4095119336 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=5118899167 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=6142678998 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=7166458829 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=8190238660 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=9214018491 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=10237798322 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=11261578153 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=12285357984 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=13309137815 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=14332917646 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=15356697477 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=16380477308 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=17404257139 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000011-000006 --seed=18428036970 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:00:30] selfplay finished: 29.848 seconds
[2019-06-18 12:00:30] selfplay mn: 29.868 seconds
[2019-06-18 12:00:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-12-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779843 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559674 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339505 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119336 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899167 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678998 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458829 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238660 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018491 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798322 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578153 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357984 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137815 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917646 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697477 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477308 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257139 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036970 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816801 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596632 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376463 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156294 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:00:32] train finished: 43.737 seconds
:::MLL 1560880794.148726 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.149488 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.150185 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.222491 47021816693632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.137370 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.138265 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.139098 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.222578 47106924381056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.223528 47021816693632 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzs0r7d8k
W0618 11:59:54.223578 47106924381056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzzui3yn8
I0618 11:59:54.224567 47021816693632 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzs0r7d8k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac467449e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.224587 47106924381056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzzui3yn8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad83814de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880794.140619 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.141330 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.141963 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.223907 47833431241600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.137553 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.138237 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.138912 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.224002 47352364184448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 11:59:54.224993 47021816693632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.225009 47106924381056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.224936 47833431241600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcl4y8ukt
W0618 11:59:54.224967 47352364184448 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1zcibc3b
I0618 11:59:54.225935 47833431241600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcl4y8ukt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b815f43ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.225938 47352364184448 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1zcibc3b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b115d6f0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.226338 47833431241600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.226337 47352364184448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.229890 47106924381056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.229901 47021816693632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.231272 47833431241600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.231332 47352364184448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.249639 47021816693632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.249854 47106924381056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.251081 47833431241600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.251037 47352364184448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880794.198383 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.198869 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.199286 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.260299 47257768244096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.201414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.201872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.202302 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.261772 47430582735744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.261343 47257768244096 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8w4dm453
I0618 11:59:54.262315 47257768244096 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8w4dm453', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afb57138e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.262713 47257768244096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.262765 47430582735744 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b23939f6cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.263890 47430582735744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.267341 47257768244096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880794.215859 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.216385 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.216800 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.268546 47950098371456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.268379 47430582735744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.269628 47950098371456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppw6pns0i
I0618 11:59:54.270625 47950098371456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppw6pns0i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9c892aee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.271026 47950098371456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880794.220566 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.221018 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.221404 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.271334 47504721511296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.272332 47504721511296 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_h0i599n
I0618 11:59:54.273372 47504721511296 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_h0i599n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b34d6a35e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.273774 47504721511296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.275720 47950098371456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.278276 47504721511296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880794.202278 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.203205 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.203900 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.283673 47649493922688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.205958 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.206691 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.207385 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.283736 47739360478080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.284674 47649493922688 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6zrjd2k3
W0618 11:59:54.284702 47739360478080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgkphvah3
I0618 11:59:54.285683 47739360478080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgkphvah3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b7835de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.285682 47649493922688 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6zrjd2k3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b568bbf0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.286086 47649493922688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.286087 47739360478080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.286866 47257768244096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.287720 47430582735744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.290958 47649493922688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.290970 47739360478080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.295160 47950098371456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.297785 47504721511296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.297812 47021816693632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.297919 47106924381056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.299017 47833431241600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.299326 47352364184448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.302135 47021816693632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:54.302234 47106924381056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:54.303322 47833431241600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:54.303642 47352364184448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:54.307189 47021816693632 estimator.py:1111] Calling model_fn.
W0618 11:59:54.307296 47021816693632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:54.307295 47106924381056 estimator.py:1111] Calling model_fn.
W0618 11:59:54.307404 47106924381056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:54.308641 47021816693632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:54.308747 47106924381056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:54.308382 47833431241600 estimator.py:1111] Calling model_fn.
W0618 11:59:54.308488 47833431241600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:54.308758 47352364184448 estimator.py:1111] Calling model_fn.
W0618 11:59:54.308868 47352364184448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880794.231997 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.232706 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.233421 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.307997 47699089474432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.224896 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.225816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.226700 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.308105 47465914082176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.309848 47833431241600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:54.310239 47352364184448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:54.309164 47699089474432 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxd3o9mrh
W0618 11:59:54.309196 47465914082176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps7o5cnjy
I0618 11:59:54.310281 47465914082176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps7o5cnjy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2bcd88fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.310293 47699089474432 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxd3o9mrh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6217df0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.310719 47465914082176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.310749 47699089474432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.313062 47739360478080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.313456 47649493922688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.315968 47699089474432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.315976 47465914082176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880794.245826 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.246654 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.247350 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.332446 46933377598336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.248964 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.249723 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.250399 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.332593 47212420289408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.333455 46933377598336 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg7vm6xzc
W0618 11:59:54.333559 47212420289408 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8cnw3rq5
I0618 11:59:54.334551 46933377598336 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg7vm6xzc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafcfe31da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.334743 47212420289408 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8cnw3rq5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af0c820ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.334980 46933377598336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.335175 47212420289408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.334675 47257768244096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.335116 47430582735744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.339695 46933377598336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.339721 47212420289408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.338996 47257768244096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:54.338415 47699089474432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.338439 47465914082176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.339426 47430582735744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880794.242138 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.243050 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.243875 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.339965 47506920575872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880794.242023 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.242939 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.243771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.340106 47804374467456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.342638 47950098371456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.340981 47506920575872 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpejgkfrh2
W0618 11:59:54.341084 47804374467456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpewaths6i
I0618 11:59:54.341980 47506920575872 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpejgkfrh2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3559b66e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.342079 47804374467456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpewaths6i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7a9b589dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.342376 47506920575872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.342472 47804374467456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880794.290931 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.291314 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.291644 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.344451 47278400529280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 11:59:54.344079 47257768244096 estimator.py:1111] Calling model_fn.
W0618 11:59:54.344190 47257768244096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:54.344466 47430582735744 estimator.py:1111] Calling model_fn.
W0618 11:59:54.344575 47430582735744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:54.345208 47504721511296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880794.292871 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.293318 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.293679 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.345278 47274838696832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.345460 47278400529280 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjd240nh1
W0618 11:59:54.345562 47257768244096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:54.346452 47278400529280 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjd240nh1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0024db3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:54.345929 47430582735744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:54.346853 47278400529280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.346226 47274838696832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfxolgdud
W0618 11:59:54.346923 47950098371456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:54.347201 47274838696832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfxolgdud', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff508dee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.347591 47274838696832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.347282 47506920575872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.347306 47804374467456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.349531 47504721511296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:54.351602 47278400529280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:54.351979 47950098371456 estimator.py:1111] Calling model_fn.
W0618 11:59:54.352093 47950098371456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:54.352200 47274838696832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.353446 47950098371456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:54.354618 47504721511296 estimator.py:1111] Calling model_fn.
W0618 11:59:54.354727 47504721511296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:54.356083 47504721511296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880794.304497 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.304870 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.305191 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.357709 47000275395456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.359542 47212420289408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:54.359697 46933377598336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880794.305801 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880794.306189 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880794.306529 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:54.357827 47548525552512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:59:54.358692 47000275395456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0aa2fomq
W0618 11:59:54.358790 47548525552512 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp925saw4w
I0618 11:59:54.359655 47000275395456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0aa2fomq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abf634e6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.359764 47548525552512 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp925saw4w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f098fee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:54.360041 47000275395456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:54.360154 47548525552512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:54.363849 47739360478080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.363964 47649493922688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:54.364651 47000275395456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:54.364666 47548525552512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instru[2019-06-18 12:00:33] divide_golden_chunk finished: 3.336 seconds
[2019-06-18 12:00:33] generate golden chunk: 3.351 seconds
[2019-06-18 12:00:33] iteration time 10: 49.078 seconds
2019-06-18 12:00:34.702403: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880833.936337 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:00:37] minmax time: 3.217 seconds
2019-06-18 12:00:37.929671: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:00:37.934984: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:00:37.939528: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880837.952194 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 10}}
[2019-06-18 12:00:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:00:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=12 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=1023779843 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=2047559674 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=3071339505 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=4095119336 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=5118899167 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=6142678998 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=7166458829 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=8190238660 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=9214018491 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=10237798322 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=11261578153 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=12285357984 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=13309137815 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=14332917646 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=15356697477 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=16380477308 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=17404257139 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=18428036970 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=19451816801 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000011-000007 --seed=20475596632 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:00:49] eval finished: 11.377 seconds
[2019-06-18 12:00:49] Win rate 000011-000007 vs 000008-000006: 0.360
:::MLL 1560880849.395094 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 12:00:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=13 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=1023779844 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=2047559675 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=3071339506 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=4095119337 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=5118899168 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=6142678999 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=7166458830 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=8190238661 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=9214018492 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=10237798323 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=11261578154 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=12285357985 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=13309137816 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=14332917647 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=15356697478 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=16380477309 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=17404257140 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000012-000006 --seed=18428036971 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:01:19] selfplay finished: 29.720 seconds
[2019-06-18 12:01:19] selfplay mn: 29.738 seconds
[2019-06-18 12:01:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-13-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779844 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559675 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339506 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119337 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899168 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142678999 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458830 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238661 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018492 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798323 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578154 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357985 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137816 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917647 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697478 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477309 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257140 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036971 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816802 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596633 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376464 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156295 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000012-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:01:21] train finished: 43.972 seconds
:::MLL 1560880843.190939 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.191840 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.192655 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.274541 47236390343552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.198061 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.198792 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.199440 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.274619 47807941583744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.275617 47236390343552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpprimh1v9
W0618 12:00:43.275705 47807941583744 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpce__2tz3
I0618 12:00:43.276675 47236390343552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpprimh1v9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af65cda9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.276762 47807941583744 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpce__2tz3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7b6ff67e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.277106 47236390343552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.277219 47807941583744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.281887 47236390343552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.281998 47807941583744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.301839 47236390343552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.302578 47807941583744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880843.230501 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.231257 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.231941 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.314435 47502435591040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.232991 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.233761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.234397 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.314434 47182783308672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.315486 47182783308672 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7jb0_upl
W0618 12:00:43.315514 47502435591040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnf7r8deb
I0618 12:00:43.316502 47502435591040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnf7r8deb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b344e62fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.316499 47182783308672 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7jb0_upl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9e1a02e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.316910 47502435591040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.316912 47182783308672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.321913 47502435591040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.321959 47182783308672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880843.234326 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.235191 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.236006 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.325695 47725883507584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.234312 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.235162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.235952 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.326071 47671941624704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.265128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.265604 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.265981 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.327013 47322308744064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.264815 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.265272 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.265681 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.327011 47051636282240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.326755 47725883507584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2ag8kqfp
I0618 12:00:43.327834 47725883507584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2ag8kqfp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6854eb8e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:43.327092 47671941624704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuj8h8fso
I0618 12:00:43.328160 47671941624704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuj8h8fso', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5bc5bbbe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:43.328091 47322308744064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2fivlfv4
W0618 12:00:43.328063 47051636282240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprssv26qj
I0618 12:00:43.328261 47725883507584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.329038 47051636282240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprssv26qj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb58a76e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.329070 47322308744064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2fivlfv4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0a5dfd6da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.328572 47671941624704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.329433 47051636282240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.329469 47322308744064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880843.255218 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.255969 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.256632 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.332617 47376008827776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.248594 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.249480 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.250374 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.332709 47872854373248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.333497 47725883507584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.334241 47322308744064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.333505 47671941624704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.334204 47051636282240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.333645 47376008827776 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpiswd227s
W0618 12:00:43.333671 47872854373248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwjdi7vbt
I0618 12:00:43.334650 47376008827776 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpiswd227s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b16dec3add8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.334667 47872854373248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwjdi7vbt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a8d110e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.335056 47376008827776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.335072 47872854373248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.339930 47872854373248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.339940 47376008827776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880843.269015 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.269760 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.270455 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.344217 47018530341760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.344163 47502435591040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880843.262753 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.263599 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.264426 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.344363 48001388069760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.344273 47182783308672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.345242 47018530341760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo2jw0ws0
W0618 12:00:43.345342 48001388069760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphdgjzkih
I0618 12:00:43.346243 47018530341760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo2jw0ws0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3a362de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.346325 48001388069760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphdgjzkih', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba87a459e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.346642 47018530341760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.346721 48001388069760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.350713 47236390343552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.350847 47807941583744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.351814 47018530341760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.351811 48001388069760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.353460 47051636282240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.353570 47322308744064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.353333 47725883507584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.353345 47671941624704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.355038 47236390343552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:43.355170 47807941583744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:43.360131 47236390343552 estimator.py:1111] Calling model_fn.
W0618 12:00:43.360244 47236390343552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:43.360287 47807941583744 estimator.py:1111] Calling model_fn.
W0618 12:00:43.360395 47807941583744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:43.359415 47376008827776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.359454 47872854373248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.361598 47236390343552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:43.361745 47807941583744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880843.297996 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.298546 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.298987 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.361876 47360785920896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.297435 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.298000 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.298455 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.362013 47184637047680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.362913 47360785920896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptksve2__
I0618 12:00:43.363049 47184637047680 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea501e0d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.363929 47360785920896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptksve2__', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1353687e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.364237 47184637047680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.364335 47360785920896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.368939 47360785920896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.368985 47184637047680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880843.285569 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.286468 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.287273 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.368757 47732375499648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.317146 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.317672 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.318075 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.370420 48006969611136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.285562 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.286473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.287297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.369204 47777402962816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.371425 48001388069760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.371481 47018530341760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.369899 47732375499648 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnpwuamkf
W0618 12:00:43.371568 48006969611136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpd6newywi
I0618 12:00:43.370997 47732375499648 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnpwuamkf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b69d7df8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:43.370270 47777402962816 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpsqmvhzzt
I0618 12:00:43.372661 48006969611136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpd6newywi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba9c6f52e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.371340 47777402962816 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpsqmvhzzt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7453b81e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.371460 47732375499648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.373099 48006969611136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.371783 47777402962816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880843.315331 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.315886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.316367 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.372250 47765369574272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.373274 47765369574272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdbo933y1
I0618 12:00:43.374262 47765369574272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdbo933y1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7186791e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.374659 47765369574272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880843.321253 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.321704 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.322119 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.374897 47824562750336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880843.325670 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.326096 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.326452 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.376733 47345495024512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.375878 47824562750336 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxggdrtev
I0618 12:00:43.376852 47824562750336 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxggdrtev', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7f4ea95e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:43.378124 48006969611136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.376804 47732375499648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.376937 47777402962816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.377720 47345495024512 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc94dtshr
I0618 12:00:43.377252 47824562750336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.378709 47345495024512 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc94dtshr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0fc3ffee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.379102 47345495024512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.379300 47765369574272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.381824 47824562750336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880843.328219 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.328629 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.328987 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.383661 47850996749184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.383652 47345495024512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880843.326325 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880843.326810 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880843.327223 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:43.384012 47552500511616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:00:43.384723 47850996749184 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpikxp5u7_
W0618 12:00:43.385060 47552500511616 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprxaao4pn
I0618 12:00:43.385835 47850996749184 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpikxp5u7_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8576402e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.386135 47552500511616 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprxaao4pn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3ff67d0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:43.386278 47850996749184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:43.386551 47552500511616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:43.388149 47360785920896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.388586 47184637047680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.391187 47850996749184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.391425 47552500511616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:43.395387 47502435591040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.395637 47182783308672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.397527 48006969611136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.399680 47502435591040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:43.398576 47765369574272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.399928 47182783308672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:43.398961 47732375499648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.399080 47777402962816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.401026 47051636282240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.401467 47322308744064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.401093 47824562750336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:43.401773 47725883507584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.401827 47671941624704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:43.403248 47345495024512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:43.404796 47502435591040 estimator.py:1111] Calling model_fn.
W0618 12:00:43.404909 47502435591040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:43.404985 47182783308672 estimator.py:1111] Calling model_fn.
W0618 12:00:43.405093 47182783308672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:43.405343 47051636282240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:43.405794 47322308744064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructi[2019-06-18 12:01:22] divide_golden_chunk finished: 3.291 seconds
[2019-06-18 12:01:22] generate golden chunk: 3.306 seconds
[2019-06-18 12:01:22] iteration time 11: 48.504 seconds
2019-06-18 12:01:23.293102: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880882.440881 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:01:26] minmax time: 3.247 seconds
2019-06-18 12:01:26.550912: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:01:26.556474: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:01:26.561036: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880886.574262 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 11}}
[2019-06-18 12:01:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000013-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:01:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=13 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=1023779844 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=2047559675 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=3071339506 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=4095119337 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=5118899168 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=6142678999 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=7166458830 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=8190238661 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=9214018492 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=10237798323 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=11261578154 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=12285357985 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=13309137816 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=14332917647 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=15356697478 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=16380477309 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=17404257140 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=18428036971 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=19451816802 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000012-000007 --seed=20475596633 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:01:39] eval finished: 12.438 seconds
[2019-06-18 12:01:39] Win rate 000012-000007 vs 000008-000006: 0.490
:::MLL 1560880899.080542 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:01:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=14 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=1023779845 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=2047559676 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=3071339507 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=4095119338 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=5118899169 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=6142679000 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=7166458831 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=8190238662 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=9214018493 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=10237798324 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=11261578155 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=12285357986 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=13309137817 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=14332917648 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=15356697479 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=16380477310 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=17404257141 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000013-000006 --seed=18428036972 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:02:10] selfplay finished: 31.491 seconds
[2019-06-18 12:02:10] selfplay mn: 31.512 seconds
[2019-06-18 12:02:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-14-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779845 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559676 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339507 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119338 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899169 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679000 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458831 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238662 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018493 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798324 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578155 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357986 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137817 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917648 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697479 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477310 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257141 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036972 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816803 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596634 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376465 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156296 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000013-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:02:10] train finished: 44.053 seconds
:::MLL 1560880891.934894 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880891.935750 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880891.936558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.019385 47419171808128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880891.951893 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880891.952729 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880891.953519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.020312 47186418320256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.020447 47419171808128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpep9qw5af
I0618 12:01:32.021456 47419171808128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpep9qw5af', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20eb7a7e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.021859 47419171808128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.021283 47186418320256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplz8pa587
I0618 12:01:32.022248 47186418320256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplz8pa587', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeaba4a0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.022647 47186418320256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.026865 47419171808128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.027521 47186418320256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880891.958405 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880891.959260 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880891.960100 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.042731 47798894965632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880891.967260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880891.968004 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880891.968658 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.042765 47869055107968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.043757 47798894965632 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8tce66e_
W0618 12:01:32.043789 47869055107968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppfpvpiqw
I0618 12:01:32.044809 47798894965632 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8tce66e_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7954be0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.044836 47869055107968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppfpvpiqw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b89aa9cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.045256 47798894965632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.045283 47869055107968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.046454 47419171808128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.047612 47186418320256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.050405 47869055107968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.050410 47798894965632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.070303 47798894965632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.070574 47869055107968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880892.024809 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.025284 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.025654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.080371 47034319733632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.081475 47034319733632 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkusklch0
I0618 12:01:32.082570 47034319733632 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkusklch0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac75081de80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.083023 47034319733632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.088234 47034319733632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880892.035431 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.035871 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.036225 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.091827 47713003512704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.026751 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.027158 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.027523 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.091562 47883459343232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.092952 47713003512704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp691vauwu
I0618 12:01:32.092543 47883459343232 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d052c0d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.094054 47713003512704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp691vauwu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6555367e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.093785 47883459343232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.094508 47713003512704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.094862 47419171808128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:32.096055 47186418320256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880892.016969 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.017716 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.018379 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.097928 47775911670656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.012180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.013081 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.013907 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.097916 47279914464128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.037817 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.038264 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.038645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.098142 47230436922240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.098496 47883459343232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.099390 47713003512704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.099093 47230436922240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4mhjl_f2
I0618 12:01:32.100045 47230436922240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4mhjl_f2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af4fa009e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.099315 47419171808128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:32.100441 47230436922240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.100549 47186418320256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:32.100952 47279914464128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjqv1x8h3
W0618 12:01:32.105004 47230436922240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:32.104437 47419171808128 estimator.py:1111] Calling model_fn.
W0618 12:01:32.104546 47419171808128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:32.105799 47186418320256 estimator.py:1111] Calling model_fn.
W0618 12:01:32.105923 47186418320256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:32.105908 47419171808128 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:01:32.106923 47279914464128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjqv1x8h3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b007f180e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.107534 47186418320256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:32.108114 47034319733632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:32.110922 47279914464128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.102519 47775911670656 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6kk7belm
I0618 12:01:32.115159 47775911670656 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6kk7belm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b73fad4be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.115631 47775911670656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880892.035685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.036406 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.037131 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.116852 47066622268288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.024073 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.025003 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.025892 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.116933 47676084835200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.117932 47883459343232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.118760 47798894965632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:32.118939 47713003512704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880892.047414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.048294 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.049100 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.118935 47562498478976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.035725 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.036638 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.037517 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.118899 47676142523264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.119462 47869055107968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:32.119781 47279914464128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.117968 47066622268288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnzihyj8g
W0618 12:01:32.118002 47676084835200 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdie0n4_4
I0618 12:01:32.119083 47066622268288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnzihyj8g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aced5e35e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.119091 47676084835200 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdie0n4_4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5cbcb02e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.120020 47676142523264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzehyqkd0
W0618 12:01:32.120063 47562498478976 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa7z22k58
I0618 12:01:32.121066 47676142523264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzehyqkd0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5cc0206da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.121124 47562498478976 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa7z22k58', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b424a69eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.120927 47775911670656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:32.119534 47676084835200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.119539 47066622268288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.121484 47676142523264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.121543 47562498478976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.123080 47798894965632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:32.123778 47869055107968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:32.124321 47230436922240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880892.013029 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.013783 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.014467 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.124684 47780544562048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.010759 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.011495 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.012213 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.124874 47440705524608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.124938 47676084835200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.126728 47676142523264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.124949 47066622268288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.126740 47562498478976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.125754 47780544562048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxrn0jyrc
W0618 12:01:32.125920 47440705524608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp652gxd4r
I0618 12:01:32.126830 47780544562048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxrn0jyrc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b750ef91e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.126961 47440705524608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp652gxd4r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b25eefcee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.128215 47798894965632 estimator.py:1111] Calling model_fn.
W0618 12:01:32.128324 47798894965632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:32.127257 47780544562048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.127375 47440705524608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.128888 47869055107968 estimator.py:1111] Calling model_fn.
W0618 12:01:32.129006 47869055107968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:32.129684 47798894965632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:32.130364 47869055107968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:32.132075 47440705524608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.132067 47780544562048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.146755 47676142523264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.146846 47562498478976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.147259 47676084835200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.147385 47066622268288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880892.053287 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.053832 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.054333 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.150252 47059068965760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.064739 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.065192 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.065586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.150837 46965691224960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.151860 47440705524608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.151853 47780544562048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.151311 47059068965760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpajllm2k4
I0618 12:01:32.152288 47059068965760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpajllm2k4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acd13ad2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.151833 46965691224960 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3vdj6v5b
I0618 12:01:32.152688 47059068965760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.152808 46965691224960 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3vdj6v5b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab755edee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.153207 46965691224960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.156133 47279914464128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:32.155966 47034319733632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:32.157409 47059068965760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.157812 46965691224960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880892.094316 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.094825 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.095278 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.157473 46987169592192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.159089 47775911670656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880892.101650 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.102031 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.102379 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.159031 47014568072064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.158544 46987169592192 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpms8l5qnt
I0618 12:01:32.159530 46987169592192 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpms8l5qnt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc5623ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.160268 47034319733632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:32.159925 46987169592192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.159993 47014568072064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpe8q6liui
I0618 12:01:32.160965 47014568072064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpe8q6liui', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac2b7375e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.161364 47014568072064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:32.165105 47883459343232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:01:32.165385 47034319733632 estimator.py:1111] Calling model_fn.
W0618 12:01:32.165494 47034319733632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:32.164671 46987169592192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880892.098361 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.098761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.099134 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.166369 47054907892608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880892.098690 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.099120 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.099446 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.166872 47069907260288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.167129 47713003512704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:32.165940 47014568072064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:32.166870 47034319733632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:32.167410 47054907892608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_0fd3efq
:::MLL 1560880892.109793 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.110231 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.110603 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.168329 47171018441600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:01:32.168431 47054907892608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_0fd3efq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc1ba82e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:32.167872 47069907260288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpe5wtk87j
I0618 12:01:32.168853 47054907892608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:32.168896 47069907260288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpe5wtk87j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf99b06e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:32.169323 47069907260288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880892.112266 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880892.112705 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880892.113092 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:32.169429 47814948762496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:01:32.169385 47171018441600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpm5raqmor
W0618 12:01:32.169391 47883459343232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:32.170431 47171018441600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpm5raqmor', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_o[2019-06-18 12:02:13] divide_golden_chunk finished: 3.323 seconds
[2019-06-18 12:02:13] generate golden chunk: 3.338 seconds
[2019-06-18 12:02:13] moving /lfs/lfs12/gma_akey/results/epb162/models/000013-000007.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000013-000008.meta
[2019-06-18 12:02:13] moving /lfs/lfs12/gma_akey/results/epb162/models/000013-000007.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb
[2019-06-18 12:02:13] moving /lfs/lfs12/gma_akey/results/epb162/models/000013-000007.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000013-000008.data-00000-of-00001
[2019-06-18 12:02:13] moving /lfs/lfs12/gma_akey/results/epb162/models/000013-000007.index --> /lfs/lfs12/gma_akey/results/epb162/models/000013-000008.index
[2019-06-18 12:02:13] iteration time 12: 51.534 seconds
2019-06-18 12:02:14.815423: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880933.974866 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:02:18] minmax time: 3.247 seconds
2019-06-18 12:02:18.072510: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:02:18.078110: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:02:18.082837: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880938.094368 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 12}}
[2019-06-18 12:02:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000014-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:02:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=14 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=1023779845 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=2047559676 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=3071339507 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=4095119338 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=5118899169 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=6142679000 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=7166458831 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=8190238662 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=9214018493 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=10237798324 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=11261578155 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=12285357986 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=13309137817 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=14332917648 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=15356697479 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=16380477310 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=17404257141 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=18428036972 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=19451816803 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000013-000008 --seed=20475596634 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:02:30] eval finished: 11.960 seconds
[2019-06-18 12:02:30] Win rate 000013-000008 vs 000012-000007: 0.670
:::MLL 1560880950.123761 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:02:30] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=15 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=1023779846 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=2047559677 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=3071339508 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=4095119339 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=5118899170 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=6142679001 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=7166458832 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=8190238663 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=9214018494 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=10237798325 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=11261578156 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=12285357987 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=13309137818 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=14332917649 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=15356697480 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=16380477311 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=17404257142 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000014-000007 --seed=18428036973 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:03:01] selfplay finished: 30.982 seconds
[2019-06-18 12:03:01] selfplay mn: 31.003 seconds
[2019-06-18 12:03:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-15-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779846 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559677 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339508 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119339 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899170 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679001 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458832 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238663 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018494 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798325 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578156 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357987 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137818 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917649 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697480 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477311 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257142 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036973 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816804 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596635 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376466 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156297 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000014-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:03:02] train finished: 43.924 seconds
:::MLL 1560880943.330014 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.330915 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.331739 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.421350 47239694611328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.339955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.340693 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.341368 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.421434 47965180564352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.422340 47239694611328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpb988asti
W0618 12:02:23.422421 47965180564352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8y1z0cn9
I0618 12:02:23.423418 47239694611328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpb988asti', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af721cdce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.423604 47965180564352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8y1z0cn9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba00c22de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.423861 47239694611328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.424060 47965180564352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.428695 47239694611328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.428780 47965180564352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880943.343101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.343857 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.344548 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.432851 47430085632896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.346348 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.347044 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.347701 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.433282 47972462261120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.433902 47430085632896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps_ummce2
I0618 12:02:23.434960 47430085632896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps_ummce2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2375fe3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:23.434297 47972462261120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0ecrfxpr
I0618 12:02:23.435335 47972462261120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0ecrfxpr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba1be28ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.435389 47430085632896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.435758 47972462261120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.440382 47430085632896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.440643 47972462261120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.448356 47239694611328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.448450 47965180564352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.460676 47430085632896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.460878 47972462261120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880943.411037 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.411541 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.411975 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.472143 47357141644160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.473164 47357141644160 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpppf6qp41
I0618 12:02:23.474210 47357141644160 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpppf6qp41', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b127a314e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.474633 47357141644160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880943.419455 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.419903 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.420286 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.477128 47998944797568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.478168 47998944797568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz_7aacdx
I0618 12:02:23.479196 47998944797568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz_7aacdx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7e8a43e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.479609 47998944797568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.479600 47357141644160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.484452 47998944797568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880943.422232 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.422697 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.423105 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.488620 46973095539584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.489727 46973095539584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmfb3m2hu
I0618 12:02:23.490761 46973095539584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmfb3m2hu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab90f42be80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.491182 46973095539584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.496477 47965180564352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.496493 47239694611328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.496239 46973095539584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880943.426603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.427011 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.427359 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.496778 46938374230912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
I0618 12:02:23.497841 46938374230912 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab0f9b5ad68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:23.499371 47357141644160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:02:23.498953 46938374230912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.500801 47965180564352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:23.500827 47239694611328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:23.503784 47998944797568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.503533 46938374230912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:23.505830 47965180564352 estimator.py:1111] Calling model_fn.
I0618 12:02:23.505886 47239694611328 estimator.py:1111] Calling model_fn.
W0618 12:02:23.505938 47965180564352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:23.506005 47239694611328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:23.507274 47965180564352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.507368 47239694611328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.509519 47430085632896 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.509703 47972462261120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880943.422941 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.423684 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.424367 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.512957 47226221745024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.513877 47430085632896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:23.514058 47972462261120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880943.420439 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.421133 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.421837 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.513554 47107167683456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.514070 47226221745024 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5y_spt3x
W0618 12:02:23.515808 46973095539584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:02:23.515187 47226221745024 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5y_spt3x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af3fec22e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:23.514661 47107167683456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpv0ieh8wx
I0618 12:02:23.515638 47226221745024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.515761 47107167683456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpv0ieh8wx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad846955e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.516222 47107167683456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.518982 47430085632896 estimator.py:1111] Calling model_fn.
W0618 12:02:23.519094 47430085632896 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:23.519130 47972462261120 estimator.py:1111] Calling model_fn.
W0618 12:02:23.519238 47972462261120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:23.520465 47430085632896 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.520600 47972462261120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.520942 47226221745024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.521427 47107167683456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880943.425309 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.426189 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.427008 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.522891 47617534313344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.426268 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.427154 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.427860 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.522900 47431635137408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.522953 46938374230912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.524025 47431635137408 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk2g4mewa
W0618 12:02:23.524055 47617534313344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpi6yfv1hg
I0618 12:02:23.525119 47431635137408 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk2g4mewa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b23d259cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.525139 47617534313344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpi6yfv1hg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f1ace1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.525559 47431635137408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.525595 47617534313344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.530984 47617534313344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.531021 47431635137408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.543162 47226221745024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.543913 47107167683456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.546683 47357141644160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.550844 47998944797568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.551015 47357141644160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:23.553248 47617534313344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.553260 47431635137408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.555170 47998944797568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:02:23.556066 47357141644160 estimator.py:1111] Calling model_fn.
W0618 12:02:23.556172 47357141644160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880943.485475 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.485931 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.486326 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.554652 47115588834176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.485967 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.486414 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.486763 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.554699 47435167232896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.557512 47357141644160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.555670 47115588834176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkv1n8evc
W0618 12:02:23.555701 47435167232896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp80jf94kg
I0618 12:02:23.556653 47115588834176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkv1n8evc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada3c85edd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.556687 47435167232896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp80jf94kg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b24a4e14dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.557043 47115588834176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.557080 47435167232896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.560223 47998944797568 estimator.py:1111] Calling model_fn.
:::MLL 1560880943.470110 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.470876 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.471578 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.560348 47410831139712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.560333 47998944797568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880943.463086 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.464018 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.464910 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.560637 47114221556608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.561686 47998944797568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.561466 47410831139712 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp70hthvl8
W0618 12:02:23.561720 47114221556608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyuikwrnh
I0618 12:02:23.562466 47410831139712 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp70hthvl8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1efa55eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.562693 47114221556608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyuikwrnh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad9eb06ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.562861 47410831139712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.563092 47114221556608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:23.561733 47435167232896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.561761 47115588834176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.563374 46973095539584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.567606 47410831139712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.567721 47114221556608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.567681 46973095539584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880943.433867 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.434605 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.435325 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.567637 47023989928832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.436468 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.437157 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.437810 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.567783 46985043153792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.508907 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.509308 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.509643 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.569262 47040154481536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.508664 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.509060 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.509409 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.569212 47864727819136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.568610 47023989928832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpys4xfrdb
W0618 12:02:23.568742 46985043153792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpett6pf_n
I0618 12:02:23.569633 47023989928832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpys4xfrdb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4e8cd8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:23.570123 46938374230912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:02:23.569791 46985043153792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpett6pf_n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abbd764ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:23.570199 47864727819136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz7vjc1bj
W0618 12:02:23.570233 47040154481536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg8vs5zk7
I0618 12:02:23.570060 47023989928832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.571197 47864727819136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz7vjc1bj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b88a8afae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.571332 47040154481536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg8vs5zk7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8ac490e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.570222 46985043153792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.571610 47864727819136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.571749 47040154481536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.572752 46973095539584 estimator.py:1111] Calling model_fn.
W0618 12:02:23.572864 46973095539584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:23.574243 46973095539584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.574430 46938374230912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:23.574780 47023989928832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.574840 46985043153792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.576310 47864727819136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:23.576341 47040154481536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:23.579504 46938374230912 estimator.py:1111] Calling model_fn.
W0618 12:02:23.579610 46938374230912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:23.580972 46938374230912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:23.581096 47115588834176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.581144 47435167232896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.587564 47410831139712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.587589 47114221556608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.594434 47023989928832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.594392 46985043153792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.595739 47040154481536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:23.595809 47864727819136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880943.487317 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.487871 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.488297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.594646 47476467377024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.486903 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.487395 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.487861 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.594686 47201946448768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.594674 47226221745024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.595552 47107167683456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:23.595644 47476467377024 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa6f64_hz
W0618 12:02:23.595677 47201946448768 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpluut8pgm
I0618 12:02:23.596640 47476467377024 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa6f64_hz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e428f7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.596649 47201946448768 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpluut8pgm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee57d67dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:23.597077 47476467377024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:23.597080 47201946448768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880943.523162 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.523644 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.524053 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.599482 47162720633728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560880943.525787 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880943.526241 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880943.526638 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:23.599610 47763298472832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:02:23.599211 47226221745024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_itera[2019-06-18 12:03:04] divide_golden_chunk finished: 3.312 seconds
[2019-06-18 12:03:04] generate golden chunk: 3.327 seconds
[2019-06-18 12:03:04] moving /lfs/lfs12/gma_akey/results/epb162/models/000014-000008.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb
[2019-06-18 12:03:04] moving /lfs/lfs12/gma_akey/results/epb162/models/000014-000008.index --> /lfs/lfs12/gma_akey/results/epb162/models/000014-000009.index
[2019-06-18 12:03:04] moving /lfs/lfs12/gma_akey/results/epb162/models/000014-000008.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000014-000009.meta
[2019-06-18 12:03:04] moving /lfs/lfs12/gma_akey/results/epb162/models/000014-000008.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000014-000009.data-00000-of-00001
[2019-06-18 12:03:04] iteration time 13: 50.522 seconds
2019-06-18 12:03:05.363269: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880984.496681 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:03:08] minmax time: 3.248 seconds
2019-06-18 12:03:08.621311: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:08.626897: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:08.631575: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880988.643283 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 13}}
[2019-06-18 12:03:08] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:03:08] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=15 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=1023779846 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=2047559677 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=3071339508 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=4095119339 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=5118899170 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=6142679001 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=7166458832 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=8190238663 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=9214018494 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=10237798325 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=11261578156 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=12285357987 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=13309137818 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=14332917649 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=15356697480 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=16380477311 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=17404257142 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=18428036973 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=19451816804 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000014-000009 --seed=20475596635 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:03:20] eval finished: 11.567 seconds
[2019-06-18 12:03:20] Win rate 000014-000009 vs 000013-000008: 0.470
:::MLL 1560881000.278748 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:03:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=16 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=1023779847 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=2047559678 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=3071339509 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=4095119340 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=5118899171 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=6142679002 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=7166458833 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=8190238664 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=9214018495 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=10237798326 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=11261578157 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=12285357988 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=13309137819 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=14332917650 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=15356697481 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=16380477312 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=17404257143 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000015-000008 --seed=18428036974 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:03:50] selfplay finished: 30.340 seconds
[2019-06-18 12:03:50] selfplay mn: 30.360 seconds
[2019-06-18 12:03:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-16-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779847 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559678 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339509 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119340 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899171 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679002 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458833 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238664 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018495 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798326 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578157 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357988 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137819 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917650 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697481 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477312 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257143 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036974 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816805 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596636 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376467 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156298 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000015-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:03:52] train finished: 43.861 seconds
:::MLL 1560880993.858885 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.859728 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.860544 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:13.947840 47422937084800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880993.867347 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.868086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.868722 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:13.947860 47476451591040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:13.948854 47422937084800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuqv5ah93
W0618 12:03:13.948901 47476451591040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjegroc9p
I0618 12:03:13.949956 47422937084800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuqv5ah93', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b21cbe80e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:13.949986 47476451591040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjegroc9p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e419e9e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:13.950403 47422937084800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:13.950438 47476451591040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:13.955180 47476451591040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:13.955203 47422937084800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:13.974735 47476451591040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:13.974982 47422937084800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880993.914209 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.915079 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.915928 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.002079 47176443638656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880993.926596 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.927287 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.927977 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.002449 47017483920256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.003096 47176443638656 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnonn_sxk
I0618 12:03:14.004113 47176443638656 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnonn_sxk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae867c08e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:14.003537 47017483920256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpe02tl3m6
I0618 12:03:14.004534 47176443638656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.004568 47017483920256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpe02tl3m6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac36503be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.004971 47017483920256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.009342 47176443638656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.009655 47017483920256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880993.946783 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.947201 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.947559 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.009765 47760390648704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880993.948045 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.948453 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.948806 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.009880 47636620661632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.010781 47760390648704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjrnfxfel
I0618 12:03:14.010875 47636620661632 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b538c709d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.011751 47760390648704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjrnfxfel', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b705db4be80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.011988 47636620661632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.012154 47760390648704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.016633 47636620661632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.016772 47760390648704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.023099 47476451591040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.023460 47422937084800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.027407 47476451591040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.027797 47422937084800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.029141 47176443638656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.029355 47017483920256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:14.032461 47476451591040 estimator.py:1111] Calling model_fn.
W0618 12:03:14.032572 47476451591040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:14.032886 47422937084800 estimator.py:1111] Calling model_fn.
W0618 12:03:14.032994 47422937084800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:14.033929 47476451591040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:14.034352 47422937084800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:14.035912 47636620661632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.036110 47760390648704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880993.996440 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.996943 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.997392 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.073320 47381246169984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.002649 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.003059 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.003507 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.073674 47641617171328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.074295 47381246169984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5ojq6vgg
I0618 12:03:14.075255 47381246169984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5ojq6vgg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1816ef2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:14.074648 47641617171328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0kkv8qe6
I0618 12:03:14.075623 47641617171328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0kkv8qe6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54b6415da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.075651 47381246169984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.076025 47641617171328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880993.996159 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.996913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.997637 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.075344 47080839893888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880993.985331 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.986235 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.987071 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.075545 47917270090624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.077638 47176443638656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.077950 47017483920256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.076422 47080839893888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2oy5sky_
W0618 12:03:14.076591 47917270090624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmjx_sh3c
I0618 12:03:14.077536 47080839893888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2oy5sky_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad225531e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.077706 47917270090624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmjx_sh3c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94e4731e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.077991 47080839893888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.078170 47917270090624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.080287 47381246169984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.080610 47641617171328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.082040 47176443638656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.082342 47017483920256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.082914 47080839893888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.082912 47917270090624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.083602 47636620661632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.083915 47760390648704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:03:14.087135 47176443638656 estimator.py:1111] Calling model_fn.
W0618 12:03:14.087258 47176443638656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:14.087458 47017483920256 estimator.py:1111] Calling model_fn.
W0618 12:03:14.087567 47017483920256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:14.088635 47176443638656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:14.087919 47636620661632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.088241 47760390648704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.088949 47017483920256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880993.995993 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.996866 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.997591 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.090459 47231989969792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.000215 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.000976 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.001696 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.090736 47200509154176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.091540 47231989969792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7tbkbv8s
I0618 12:03:14.092638 47231989969792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7tbkbv8s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af556923e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:14.091830 47200509154176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3b_z0zj2
I0618 12:03:14.092955 47200509154176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3b_z0zj2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee022b2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.093091 47231989969792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.093402 47200509154176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.092944 47636620661632 estimator.py:1111] Calling model_fn.
W0618 12:03:14.093053 47636620661632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:14.093322 47760390648704 estimator.py:1111] Calling model_fn.
W0618 12:03:14.093430 47760390648704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:14.094409 47636620661632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:14.094792 47760390648704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:14.099605 47381246169984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.099962 47641617171328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.098648 47231989969792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.103030 47080839893888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.103196 47917270090624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880994.015449 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.016318 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.017133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.104978 47131691668352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.014985 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.015807 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.016645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.105133 47836245644160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.106074 47131691668352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3cxscc_s
W0618 12:03:14.106195 47836245644160 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjhkzhbqr
I0618 12:03:14.107194 47131691668352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3cxscc_s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2addfc53ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.107294 47836245644160 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjhkzhbqr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8207042da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.107597 47131691668352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.107688 47836245644160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.106275 47200509154176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.112373 47131691668352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.112384 47836245644160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880993.996743 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.997487 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.998179 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.112842 47424397214592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880993.994705 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880993.995517 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880993.996222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.113130 47709273031552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.113953 47424397214592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpynwnwmrq
W0618 12:03:14.114198 47709273031552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxhrc78m4
I0618 12:03:14.115066 47424397214592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpynwnwmrq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2222efce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.115278 47709273031552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxhrc78m4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6476dbce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.115515 47424397214592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.115731 47709273031552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.121035 47709273031552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.121052 47424397214592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880994.056760 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.057279 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.057726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.121649 47297682822016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.056659 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.057189 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.057632 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.121681 47218243527552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.122680 47297682822016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6_ur7rnk
W0618 12:03:14.122710 47218243527552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9zaqfcka
I0618 12:03:14.123644 47297682822016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6_ur7rnk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04a22bae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.123694 47218243527552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9zaqfcka', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af223383dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.124045 47297682822016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.124094 47218243527552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.128670 47297682822016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.128702 47218243527552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.131638 47131691668352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.131787 47836245644160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.135704 47200509154176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.139478 47231989969792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.142368 47709273031552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:14.142517 47424397214592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880994.048160 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.048570 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.048923 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.145229 46985388831616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.044150 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.044655 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.045075 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.145261 47799783670656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.146974 47381246169984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.147202 47641617171328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.146336 46985388831616 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1jmnrkld
W0618 12:03:14.146308 47799783670656 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpq9zijz6o
I0618 12:03:14.147301 47799783670656 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpq9zijz6o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7989b68e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.147311 46985388831616 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1jmnrkld', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abbebff8dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:14.147833 47297682822016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:14.147698 46985388831616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.147699 47799783670656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.147998 47218243527552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880994.080932 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.081305 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.081643 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.149150 47254368084864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.081917 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.082292 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.082609 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.149501 47569892615040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.076463 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.076981 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.077441 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.149886 47954681598848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880994.076480 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880994.076995 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880994.077451 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:14.150120 47297731388288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:03:14.150150 47254368084864 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcea05q67
I0618 12:03:14.151131 47254368084864 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcea05q67', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa8c692e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:14.150488 47569892615040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcq7je1do
W0618 12:03:14.151262 47381246169984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.150894 47954681598848 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3xi2z1by
W0618 12:03:14.151507 47641617171328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:14.151080 47297731388288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8k6k85yy
I0618 12:03:14.151453 47569892615040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcq7je1do', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4403237da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.151522 47254368084864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.151859 47954681598848 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3xi2z1by', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d9a596e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.152055 47297731388288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8k6k85yy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04a510ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:14.151855 47569892615040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.152262 47954681598848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:14.152443 47297731388288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:14.151880 47080839893888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.151886 47917270090624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:14.152378 46985388831616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.152384 47799783670656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:14.156133 47254368084864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:14.156314 47381246169984 estimator.py:1111] Calling model_fn.
W0618 12:03:14.156418 47381246169984 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:14.156331 47569892615040 deprecation.py:323] From /home/gm[2019-06-18 12:03:53] divide_golden_chunk finished: 3.311 seconds
[2019-06-18 12:03:53] generate golden chunk: 3.326 seconds
[2019-06-18 12:03:53] iteration time 14: 49.469 seconds
2019-06-18 12:03:54.863819: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881033.966069 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:03:58] minmax time: 3.231 seconds
2019-06-18 12:03:58.105543: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:58.110953: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:58.115457: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881038.128681 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 14}}
[2019-06-18 12:03:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000016-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:03:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=16 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=1023779847 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=2047559678 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=3071339509 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=4095119340 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=5118899171 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=6142679002 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=7166458833 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=8190238664 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=9214018495 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=10237798326 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=11261578157 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=12285357988 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=13309137819 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=14332917650 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=15356697481 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=16380477312 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=17404257143 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=18428036974 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=19451816805 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000015-000009 --seed=20475596636 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:04:09] eval finished: 11.398 seconds
[2019-06-18 12:04:09] Win rate 000015-000009 vs 000013-000008: 0.760
:::MLL 1560881049.592896 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:04:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=17 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=1023779848 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=2047559679 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=3071339510 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=4095119341 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=5118899172 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=6142679003 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=7166458834 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=8190238665 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=9214018496 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=10237798327 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=11261578158 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=12285357989 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=13309137820 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=14332917651 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=15356697482 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=16380477313 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=17404257144 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000016-000008 --seed=18428036975 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:04:39] selfplay finished: 29.849 seconds
[2019-06-18 12:04:39] selfplay mn: 29.867 seconds
[2019-06-18 12:04:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-17-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779848 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559679 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339510 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119341 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899172 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679003 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458834 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238665 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018496 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798327 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578158 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357989 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137820 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917651 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697482 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477313 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257144 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036975 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816806 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596637 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376468 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156299 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000016-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:04:42] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 12:04:42] generate golden chunk: 3.317 seconds
[2019-06-18 12:04:45] train finished: 46.994 seconds
:::MLL 1560881043.327486 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.328373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.329216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.410023 47321335976832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.332284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.333010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.333662 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.410341 47896821162880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.411161 47321335976832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp26mfubhw
W0618 12:04:03.411407 47896821162880 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpr4fmvyq1
I0618 12:04:03.412259 47321335976832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp26mfubhw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0a24021e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.412477 47896821162880 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpr4fmvyq1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9021992e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.412686 47321335976832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.412907 47896821162880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.417574 47321335976832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.417672 47896821162880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.437206 47321335976832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.437354 47896821162880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881043.371001 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.371721 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.372362 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.453789 47108756046720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.364566 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.365468 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.366277 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.453989 47695975601024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.454868 47108756046720 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyfz65phz
W0618 12:04:03.455034 47695975601024 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp53i599bs
I0618 12:04:03.455946 47108756046720 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyfz65phz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8a541ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.456101 47695975601024 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp53i599bs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b615e452e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.456395 47108756046720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.456527 47695975601024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.461373 47108756046720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.461397 47695975601024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881043.400604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.401095 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.401503 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.464907 46983734616960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.465968 46983734616960 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpziilo4i3
I0618 12:04:03.467020 46983734616960 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpziilo4i3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb89663e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.467451 46983734616960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881043.408431 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.408874 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.409261 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.471302 47732111299456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.472259 46983734616960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.472297 47732111299456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9p6_umgh
I0618 12:04:03.473273 47732111299456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9p6_umgh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b69c8202e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.473670 47732111299456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.478255 47732111299456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.480895 47108756046720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.481175 47695975601024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.485748 47321335976832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.486246 47896821162880 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.490052 47321335976832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.490598 47896821162880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.491691 46983734616960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:03.495114 47321335976832 estimator.py:1111] Calling model_fn.
W0618 12:04:03.495225 47321335976832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:03.495711 47896821162880 estimator.py:1111] Calling model_fn.
W0618 12:04:03.495821 47896821162880 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.496588 47321335976832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.497196 47896821162880 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.497641 47732111299456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881043.412027 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.412896 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.413716 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.502182 47558910780288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.417407 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.418164 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.418885 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.502716 47142541632384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.503228 47558910780288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcsr12uv5
I0618 12:04:03.504280 47558910780288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcsr12uv5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b417491fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:03.503762 47142541632384 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptfrqvgtz
I0618 12:04:03.504708 47558910780288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.504798 47142541632384 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptfrqvgtz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae08308ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.505219 47142541632384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881043.439507 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.440029 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.440449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.505386 47669920457600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.506484 47669920457600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpaufy4ha5
I0618 12:04:03.507571 47669920457600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpaufy4ha5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b4d432e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.507996 47669920457600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.512714 47669920457600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881043.443399 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.443841 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.444217 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.513062 47174664094592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
I0618 12:04:03.514103 47174664094592 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae7fdaedd68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.515284 47174664094592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.519828 47174664094592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.524472 47558910780288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.524937 47142541632384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.529196 47108756046720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.529243 47695975601024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.532188 47669920457600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.533506 47108756046720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.533521 47695975601024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881043.431678 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.432398 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.433094 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.536531 48000825258880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.429683 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.430465 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.431218 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.536569 47726056117120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.539163 46983734616960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:03.538586 47108756046720 estimator.py:1111] Calling model_fn.
I0618 12:04:03.538558 47695975601024 estimator.py:1111] Calling model_fn.
W0618 12:04:03.538668 47695975601024 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.538696 47108756046720 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.537660 48000825258880 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7170iux5
W0618 12:04:03.537690 47726056117120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8zn4e1xd
W0618 12:04:03.539284 47174664094592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:03.538752 48000825258880 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7170iux5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba858b9be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.538781 47726056117120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8zn4e1xd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b685f355dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.539208 48000825258880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.539223 47726056117120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.540058 47108756046720 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.540023 47695975601024 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881043.454940 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.455818 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.456654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.542849 46971545269120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.455677 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.456577 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.457350 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.542848 47499624211328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.543468 46983734616960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.543979 47499624211328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfgk062p6
W0618 12:04:03.544020 46971545269120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1nojeege
I0618 12:04:03.544996 47499624211328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfgk062p6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b33a6d0ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.545014 46971545269120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1nojeege', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab8b2db5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:03.544992 47732111299456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:03.545384 47499624211328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.545411 46971545269120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.544476 48000825258880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.544533 47726056117120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.546310 47558910780288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.546866 47142541632384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:04:03.548537 46983734616960 estimator.py:1111] Calling model_fn.
W0618 12:04:03.548649 46983734616960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.549294 47732111299456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.550019 46983734616960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.550180 46971545269120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.550188 47499624211328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:03.554351 47732111299456 estimator.py:1111] Calling model_fn.
W0618 12:04:03.554459 47732111299456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.555823 47732111299456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.566650 48000825258880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.566790 47726056117120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.569666 47499624211328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.569843 46971545269120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881043.440890 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.441643 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.442327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.568279 47115265733504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.439012 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.439814 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.440645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.568451 47008949646208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.569306 47115265733504 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp07yy48e9
W0618 12:04:03.569421 47008949646208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpamplnoip
I0618 12:04:03.570299 47115265733504 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp07yy48e9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada2943bdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.570407 47008949646208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpamplnoip', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac168550e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.570701 47115265733504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.570806 47008949646208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881043.505399 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.505855 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.506223 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.576273 47631818769280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.505480 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.505934 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.506296 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.576287 47648951518080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.575522 47115265733504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.575569 47008949646208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881043.486571 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.487057 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.487432 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.575846 46938758067072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.489653 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.490055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.490407 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.575978 47241518183296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.577274 47648951518080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp339e38n2
W0618 12:04:03.577302 47631818769280 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy5gt7vxs
I0618 12:04:03.578327 47648951518080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp339e38n2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b566b6a9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.578366 47631818769280 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy5gt7vxs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b526e399e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.578749 47648951518080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.578794 47631818769280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.576846 46938758067072 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp84i5kqky
W0618 12:04:03.576950 47241518183296 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7wtnvmhr
I0618 12:04:03.577830 46938758067072 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp84i5kqky', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab110968e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.577945 47241518183296 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7wtnvmhr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af78e7f4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.578233 46938758067072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.578350 47241518183296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.579726 47669920457600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.583403 47648951518080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.583444 47631818769280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.582790 46938758067072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.582916 47241518183296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.584037 47669920457600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881043.511913 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.512453 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.512929 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.587614 46927329395584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.587224 47174664094592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881043.516449 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.516886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.517274 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.588080 47236603765632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.588695 46927329395584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo7ff3hkn
I0618 12:04:03.589743 46927329395584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo7ff3hkn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae6762ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:03.589136 47236603765632 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpq_d338db
I0618 12:04:03.589098 47669920457600 estimator.py:1111] Calling model_fn.
W0618 12:04:03.589208 47669920457600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:03.590164 46927329395584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:03.590169 47236603765632 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpq_d338db', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af669933da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:03.590582 47236603765632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:03.590573 47669920457600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:03.591566 47174664094592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.595054 46927329395584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.595397 47236603765632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:03.594938 47115265733504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.594938 47008949646208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:03.596361 47558910780288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:03.596722 47142541632384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:03.596679 47174664094592 estimator.py:1111] Calling model_fn.
W0618 12:04:03.596793 47174664094592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:03.598172 47174664094592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881043.484027 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.484589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.485092 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.598442 47676593529728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881043.494942 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881043.495431 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881043.495830 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:03.598497 47275569542016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:04:03.600619 47558910780288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.600994 47142541632384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:03.599471 47676593529728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptnd1ufgd
W0618 12:04:03.599501 47275569542016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcmjk_oz4
I0618 12:04:03.600446 47676593529728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tm[2019-06-18 12:04:45] moving /lfs/lfs12/gma_akey/results/epb162/models/000016-000009.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000016-000010.data-00000-of-00001
[2019-06-18 12:04:45] moving /lfs/lfs12/gma_akey/results/epb162/models/000016-000009.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000016-000010.meta
[2019-06-18 12:04:45] moving /lfs/lfs12/gma_akey/results/epb162/models/000016-000009.index --> /lfs/lfs12/gma_akey/results/epb162/models/000016-000010.index
[2019-06-18 12:04:45] moving /lfs/lfs12/gma_akey/results/epb162/models/000016-000009.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb
[2019-06-18 12:04:45] iteration time 15: 51.219 seconds
2019-06-18 12:04:46.153851: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881085.185206 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:04:49] minmax time: 3.306 seconds
2019-06-18 12:04:49.469608: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:04:49.475048: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:04:49.479755: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881089.491493 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 15}}
[2019-06-18 12:04:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:04:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=17 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=1023779848 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=2047559679 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=3071339510 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=4095119341 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=5118899172 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=6142679003 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=7166458834 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=8190238665 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=9214018496 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=10237798327 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=11261578158 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=12285357989 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=13309137820 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=14332917651 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=15356697482 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=16380477313 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=17404257144 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=18428036975 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=19451816806 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000016-000010 --seed=20475596637 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:05:01] eval finished: 11.909 seconds
[2019-06-18 12:05:01] Win rate 000016-000010 vs 000015-000009: 0.480
:::MLL 1560881101.466283 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:05:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=18 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=1023779849 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=2047559680 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=3071339511 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=4095119342 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=5118899173 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=6142679004 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=7166458835 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=8190238666 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=9214018497 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=10237798328 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=11261578159 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=12285357990 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=13309137821 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=14332917652 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=15356697483 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=16380477314 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=17404257145 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000017-000009 --seed=18428036976 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:05:31] selfplay finished: 29.584 seconds
[2019-06-18 12:05:31] selfplay mn: 29.604 seconds
[2019-06-18 12:05:31] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-18-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779849 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559680 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339511 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119342 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899173 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679004 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458835 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238666 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018497 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798328 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578159 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357990 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137821 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917652 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697483 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477314 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257145 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036976 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816807 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596638 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376469 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156300 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000017-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:05:33] train finished: 43.722 seconds
:::MLL 1560881094.703404 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.704276 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.705070 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.797571 47578298954624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.712208 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.712975 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.713647 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.797643 47871411680128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.798542 47578298954624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvz2_1nfd
W0618 12:04:54.798600 47871411680128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpka2wsrfz
I0618 12:04:54.799569 47578298954624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvz2_1nfd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45f8320e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.799615 47871411680128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpka2wsrfz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a37134da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.799961 47578298954624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.800010 47871411680128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.804707 47578298954624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.804732 47871411680128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.824054 47578298954624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.824283 47871411680128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.872663 47578298954624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881094.798856 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.799264 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.799615 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.873418 47006994051968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.873578 47871411680128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881094.793758 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.794339 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.794807 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.873612 47747075101568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.874451 47006994051968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdnkwerhp
W0618 12:04:54.874581 47747075101568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpoa1cp0g5
I0618 12:04:54.875441 47006994051968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdnkwerhp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0f3c50da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.875551 47747075101568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpoa1cp0g5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d4409ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.875843 47006994051968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.875942 47747075101568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.876989 47578298954624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.877938 47871411680128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.880448 47747075101568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.880490 47006994051968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:54.882075 47578298954624 estimator.py:1111] Calling model_fn.
W0618 12:04:54.882184 47578298954624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:54.883043 47871411680128 estimator.py:1111] Calling model_fn.
W0618 12:04:54.883152 47871411680128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:54.883543 47578298954624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:54.884503 47871411680128 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881094.793715 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.794611 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.795428 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.888611 47763354944384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.794736 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.795601 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.796336 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.888737 47592083870592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.889708 47763354944384 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpsejw4rml
W0618 12:04:54.889842 47592083870592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjxf68th0
I0618 12:04:54.890809 47763354944384 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpsejw4rml', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b710e645e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.890943 47592083870592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjxf68th0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b492dd72e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.891250 47763354944384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.891395 47592083870592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.896466 47763354944384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.896589 47592083870592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.899543 47747075101568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.899821 47006994051968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.918287 47763354944384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.918790 47592083870592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881094.824778 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.825557 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.826286 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.919402 47601909302144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.821509 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.822325 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.823013 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.919663 47369393009536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.920467 47601909302144 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo_eutbxk
W0618 12:04:54.920678 47369393009536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1a1bvvhs
I0618 12:04:54.921515 47601909302144 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo_eutbxk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4b777b4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.921726 47369393009536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1a1bvvhs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b15546e4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.922020 47601909302144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.922197 47369393009536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.926891 47601909302144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.926955 47369393009536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881094.829270 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.830187 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.831036 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.928394 47196032603008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.846973 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.847766 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.848470 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.928638 47838636475264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.929532 47196032603008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmph5_a4b0e
W0618 12:04:54.929714 47838636475264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcu5o46vm
I0618 12:04:54.930670 47196032603008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmph5_a4b0e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aecf7586e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.930950 47838636475264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcu5o46vm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8295854dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.931121 47196032603008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.931391 47838636475264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.936383 47196032603008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881094.874349 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.874797 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.875178 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.937504 47521653744512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.936513 47838636475264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.938577 47521653744512 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp03eszhlq
I0618 12:04:54.939618 47521653744512 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp03eszhlq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b38c7e0be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.940050 47521653744512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881094.812690 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.813581 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.814350 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.940101 47508547269504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.812278 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.813147 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.813983 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.940203 47812695249792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.941162 47812695249792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpt9hchqlq
W0618 12:04:54.941102 47508547269504 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfzxp7c1m
I0618 12:04:54.942112 47508547269504 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfzxp7c1m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b35baabbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.942148 47812695249792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpt9hchqlq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c8b4dae80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.942510 47508547269504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.942545 47812695249792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.944919 47521653744512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881094.884151 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.884596 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.884984 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.945197 47483455210368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.946741 47747075101568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:54.946151 47483455210368 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpepmm_g1o
W0618 12:04:54.947155 47006994051968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:54.947128 47483455210368 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpepmm_g1o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2fe3112e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.947523 47483455210368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.946865 47369393009536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.947412 47812695249792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.946920 47601909302144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.947473 47508547269504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881094.845999 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.846831 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.847529 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.950095 47305869955968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.849404 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.850172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.850865 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.950349 47865018127232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.951030 47747075101568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.951449 47006994051968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.951291 47305869955968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpur9z7xoe
W0618 12:04:54.951425 47865018127232 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg8k603t_
I0618 12:04:54.952402 47305869955968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpur9z7xoe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b068a296e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:54.952037 47483455210368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:54.952609 47865018127232 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg8k603t_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b88b9fd6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.952808 47305869955968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.953007 47865018127232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.956071 47747075101568 estimator.py:1111] Calling model_fn.
W0618 12:04:54.956177 47747075101568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:54.956538 47006994051968 estimator.py:1111] Calling model_fn.
W0618 12:04:54.956647 47006994051968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:54.957510 47305869955968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.957720 47865018127232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.957529 47747075101568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:54.958012 47006994051968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:54.958616 47838636475264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.959266 47196032603008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.964240 47521653744512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.966818 47812695249792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.967038 47508547269504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881094.886659 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.887139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.887548 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.967092 47662437299072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.888994 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.889473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.889903 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.967215 47094005474176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.968120 47662437299072 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp0ag8mal
W0618 12:04:54.968182 47094005474176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkljj4rln
I0618 12:04:54.969103 47662437299072 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp0ag8mal', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b598f3b3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.969158 47094005474176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkljj4rln', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5360dee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.969506 47662437299072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.969551 47094005474176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.970592 47763354944384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:54.971252 47592083870592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:54.971296 47483455210368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881094.862470 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.862958 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.863400 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.970859 47827367936896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.859398 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.859900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.860322 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.971038 47164918121344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
I0618 12:04:54.971915 47827367936896 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ff5dd1d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:54.972060 47164918121344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4hh7v58r
I0618 12:04:54.973033 47827367936896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.973040 47164918121344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4hh7v58r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5b8c71e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.973439 47164918121344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.974162 47662437299072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.974179 47094005474176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.975174 47763354944384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.975883 47592083870592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:54.976816 47305869955968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.977773 47865018127232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881094.905097 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.905551 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.905876 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.976341 47880353506176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.907199 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.907613 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.907976 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.976674 47945228731264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.977629 47827367936896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.977951 47164918121344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.977344 47880353506176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjh4osmhq
I0618 12:04:54.978323 47880353506176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjh4osmhq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c4c0cae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:54.977649 47945228731264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk4s6kwww
I0618 12:04:54.978631 47945228731264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk4s6kwww', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b66ea1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.978721 47880353506176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.979022 47945228731264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.980506 47763354944384 estimator.py:1111] Calling model_fn.
W0618 12:04:54.980636 47763354944384 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:54.981313 47592083870592 estimator.py:1111] Calling model_fn.
W0618 12:04:54.981432 47592083870592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:54.982096 47763354944384 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:54.982905 47592083870592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:54.983434 47880353506176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.983671 47945228731264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881094.900216 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.900688 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.901093 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.990985 47786692760448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560881094.901792 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881094.902247 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881094.902641 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:54.991348 46950477951872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:04:54.992039 47786692760448 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvh_uld6d
I0618 12:04:54.993105 47786692760448 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvh_uld6d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b767d6f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:54.992376 46950477951872 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu_l_x4b_
I0618 12:04:54.993434 46950477951872 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu_l_x4b_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3cb25ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:54.993523 47786692760448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:54.993858 46950477951872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:54.993531 47662437299072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.993623 47094005474176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.996870 47827367936896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.997117 47164918121344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:54.998395 47786692760448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.998675 46950477951872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:54.997318 47369393009536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:54.997459 47601909302144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:55.001690 47369393009536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:55.001842 47601909302144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can u[2019-06-18 12:05:34] divide_golden_chunk finished: 3.288 seconds
[2019-06-18 12:05:34] generate golden chunk: 3.302 seconds
[2019-06-18 12:05:34] iteration time 16: 49.188 seconds
2019-06-18 12:05:35.356296: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
Got 343572 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000009.tfrecord.zz: 12.818 seconds
Got 377794 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000004.tfrecord.zz: 15.027 seconds
Got 379267 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000005.tfrecord.zz: 14.614 seconds
Got 347710 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz: 0.210 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000006.tfrecord.zz: 14.237 seconds
Got 391379 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz: 0.307 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000000.tfrecord.zz: 15.658 seconds
Got 382550 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000002.tfrecord.zz: 15.226 seconds
Got 348599 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000007.tfrecord.zz: 14.092 seconds
Got 346341 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000008.tfrecord.zz: 13.383 seconds
Got 388821 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz: 0.308 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000003.tfrecord.zz: 15.858 seconds
Got 388250 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000000-000001.tfrecord.zz: 14.248 seconds
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/checkpoint_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/checkpointlog.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000001-000001_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000001-000001log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000002-000002_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000002-000002log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000003-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000003-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000004-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000004-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000005-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000005-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000006-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000006-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000007-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000007-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000008-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000008-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000009-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000009-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000010-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000010-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000011-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000011-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000012-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000012-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000013-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000013-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000014-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000014-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000015-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000015-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000016-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000016-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000017-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000017-000010log.txt['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881134.373559 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:05:38] minmax time: 3.274 seconds
2019-06-18 12:05:38.640461: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:05:38.645947: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:05:38.650644: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881138.663663 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 16}}
[2019-06-18 12:05:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:05:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=18 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=1023779849 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=2047559680 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=3071339511 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=4095119342 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=5118899173 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=6142679004 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=7166458835 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=8190238666 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=9214018497 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=10237798328 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=11261578159 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=12285357990 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=13309137821 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=14332917652 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=15356697483 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=16380477314 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=17404257145 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=18428036976 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=19451816807 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000017-000010 --seed=20475596638 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:05:49] eval finished: 10.453 seconds
[2019-06-18 12:05:49] Win rate 000017-000010 vs 000015-000009: 0.480
:::MLL 1560881149.182414 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:05:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=19 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=1023779850 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=2047559681 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=3071339512 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=4095119343 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=5118899174 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=6142679005 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=7166458836 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=8190238667 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=9214018498 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=10237798329 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=11261578160 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=12285357991 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=13309137822 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=14332917653 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=15356697484 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=16380477315 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=17404257146 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000018-000009 --seed=18428036977 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:06:18] selfplay finished: 29.408 seconds
[2019-06-18 12:06:18] selfplay mn: 29.426 seconds
[2019-06-18 12:06:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-19-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779850 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559681 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339512 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119343 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899174 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679005 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458836 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238667 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018498 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798329 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578160 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357991 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137822 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917653 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697484 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477315 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257146 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036977 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816808 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596639 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376470 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156301 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000018-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:06:21] divide_golden_chunk finished: 3.314 seconds
[2019-06-18 12:06:21] generate golden chunk: 3.329 seconds
[2019-06-18 12:06:22] train finished: 43.827 seconds
:::MLL 1560881143.898512 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.899353 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.900150 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:43.995234 47783297033088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881143.903496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.904165 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.904834 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:43.995291 47215365817216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:43.996303 47783297033088 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdbis8jgs
W0618 12:05:43.996335 47215365817216 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz3j2q8ne
I0618 12:05:43.997308 47783297033088 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdbis8jgs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75b3086e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:43.997335 47215365817216 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz3j2q8ne', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af177b1ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:43.997708 47783297033088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:43.997735 47215365817216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.002573 47215365817216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.002580 47783297033088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.021882 47215365817216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.021894 47783297033088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881143.939848 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.940704 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.941504 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.034711 47986380514176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881143.939806 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.940662 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.941465 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.034916 47063867319168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.035734 47986380514176 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_wpv7ji6
W0618 12:05:44.035862 47063867319168 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo7ezjvdr
I0618 12:05:44.036734 47986380514176 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_wpv7ji6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba4fbc07e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.036834 47063867319168 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo7ezjvdr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace31ae3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.037141 47986380514176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.037239 47063867319168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.042110 47986380514176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.042253 47063867319168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881143.973158 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.973708 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.974189 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.044738 47510338741120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881143.977608 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.978179 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.978570 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.046227 47648501396352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.045759 47510338741120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk9454s7m
I0618 12:05:44.046750 47510338741120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk9454s7m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3625737e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.047147 47510338741120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881143.952399 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.953149 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.953777 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.047988 47582201926528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881143.951049 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.951816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.952576 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.048024 47801124758400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
I0618 12:05:44.047238 47648501396352 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5650964d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.048368 47648501396352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.049058 47582201926528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0gv0una5
W0618 12:05:44.049093 47801124758400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx_qkwvxo
I0618 12:05:44.050061 47582201926528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0gv0una5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b46e0d4ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.050078 47801124758400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx_qkwvxo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b79d9a5fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.050474 47801124758400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.050472 47582201926528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.051820 47510338741120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.052998 47648501396352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.055582 47801124758400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.055634 47582201926528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.061603 47986380514176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.061690 47063867319168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.069855 47215365817216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.069975 47783297033088 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.071061 47510338741120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.072137 47648501396352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.074173 47215365817216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.074288 47783297033088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.077527 47801124758400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.077883 47582201926528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:05:44.079257 47215365817216 estimator.py:1111] Calling model_fn.
W0618 12:05:44.079366 47215365817216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:44.079362 47783297033088 estimator.py:1111] Calling model_fn.
W0618 12:05:44.079471 47783297033088 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:44.080732 47215365817216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.080836 47783297033088 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881144.017327 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.017756 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.018129 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.089574 47565861311360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881144.013461 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.014019 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.014507 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.089666 47773030695808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.090664 47565861311360 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa9any7rm
W0618 12:05:44.090708 47773030695808 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5657xd9f
I0618 12:05:44.091660 47565861311360 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa9any7rm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4312daae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.091681 47773030695808 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5657xd9f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b734f1c9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881144.004229 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.004990 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.005702 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.090471 47265070560128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881143.993074 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881143.993973 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881143.994811 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.090596 47578796487552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
I0618 12:05:44.092070 47565861311360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.092092 47773030695808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.091549 47265070560128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpaztoa2je
W0618 12:05:44.091637 47578796487552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6de94i51
I0618 12:05:44.092626 47265070560128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpaztoa2je', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd0a53fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.092699 47578796487552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6de94i51', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4615d9ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.093030 47265070560128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.093094 47578796487552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.096797 47565861311360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.096804 47773030695808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.097870 47265070560128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.097891 47578796487552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881144.014417 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.015166 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.015885 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.100500 47973163365248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881144.008562 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.009477 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.010291 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.100528 47925532836736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.101651 47925532836736 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbqg8ai5n
W0618 12:05:44.101682 47973163365248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx41zhsch
I0618 12:05:44.102782 47925532836736 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbqg8ai5n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96d0f29e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.102780 47973163365248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx41zhsch', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba1e7f2ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.103244 47925532836736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.103249 47973163365248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.108583 47973163365248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.108624 47925532836736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.110149 47063867319168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.110242 47986380514176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.114454 47063867319168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.114561 47986380514176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.115919 47565861311360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.116038 47773030695808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.117023 47578796487552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.117032 47265070560128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881144.047447 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.047868 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.048215 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.118080 47111584650112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881144.047369 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.047783 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.048132 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.118100 47349720515456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
I0618 12:05:44.119562 47063867319168 estimator.py:1111] Calling model_fn.
W0618 12:05:44.119670 47063867319168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:44.119727 47986380514176 estimator.py:1111] Calling model_fn.
W0618 12:05:44.119837 47986380514176 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:44.119036 47510338741120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.119138 47111584650112 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7llc1_9q
W0618 12:05:44.119165 47349720515456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0ffnqgab
I0618 12:05:44.120118 47111584650112 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7llc1_9q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad94ddaee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.120172 47349720515456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0ffnqgab', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b10bfdbdda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:44.119794 47648501396352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:44.120529 47111584650112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.120565 47349720515456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.121017 47063867319168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.121199 47986380514176 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.123335 47510338741120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.124108 47648501396352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.125223 47349720515456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.125262 47111584650112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.128882 47801124758400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:44.128421 47510338741120 estimator.py:1111] Calling model_fn.
W0618 12:05:44.128534 47510338741120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:44.129319 47582201926528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:44.129223 47648501396352 estimator.py:1111] Calling model_fn.
W0618 12:05:44.129334 47648501396352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:44.129896 47510338741120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.130711 47648501396352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.130567 47973163365248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.130930 47925532836736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.133166 47801124758400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:44.133605 47582201926528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:44.138308 47801124758400 estimator.py:1111] Calling model_fn.
W0618 12:05:44.138415 47801124758400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:44.138720 47582201926528 estimator.py:1111] Calling model_fn.
W0618 12:05:44.138835 47582201926528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:44.139779 47801124758400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:44.140213 47582201926528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881144.058068 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.058527 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.058985 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.141563 47068778058624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881144.056564 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.057048 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.057459 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.141852 47884826637184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.142604 47068778058624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpf2qt51ck
W0618 12:05:44.144400 47349720515456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:44.142820 47884826637184 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpn8u_fauz
W0618 12:05:44.144450 47111584650112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:05:44.143604 47068778058624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpf2qt51ck', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf56622dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.143790 47884826637184 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpn8u_fauz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d56ab4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.144009 47068778058624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.144187 47884826637184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.148693 47068778058624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:44.148828 47884826637184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881144.083197 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.083634 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.084022 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.156259 47647665333120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560881144.086709 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881144.087187 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881144.087725 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:44.156823 46918498026368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:05:44.157269 47647665333120 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzk30xcro
I0618 12:05:44.158264 47647665333120 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzk30xcro', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b561ec0ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:44.157797 46918498026368 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6y8qoqny
I0618 12:05:44.158659 47647665333120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:44.158786 46918498026368 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6y8qoqny', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aac58fece48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:44.159189 46918498026368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:44.163635 47565861311360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:44.164077 47773030695808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the[2019-06-18 12:06:22] iteration time 17: 48.138 seconds
2019-06-18 12:06:23.542510: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881182.512029 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:06:26] minmax time: 3.222 seconds
2019-06-18 12:06:26.775081: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:06:26.780744: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:06:26.785480: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881186.798818 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 17}}
[2019-06-18 12:06:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000019-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:06:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=19 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=1023779850 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=2047559681 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=3071339512 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=4095119343 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=5118899174 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=6142679005 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=7166458836 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=8190238667 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=9214018498 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=10237798329 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=11261578160 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=12285357991 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=13309137822 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=14332917653 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=15356697484 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=16380477315 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=17404257146 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=18428036977 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=19451816808 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000018-000010 --seed=20475596639 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:06:37] eval finished: 10.393 seconds
[2019-06-18 12:06:37] Win rate 000018-000010 vs 000015-000009: 0.510
:::MLL 1560881197.259071 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:06:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=20 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=1023779851 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=2047559682 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=3071339513 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=4095119344 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=5118899175 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=6142679006 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=7166458837 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=8190238668 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=9214018499 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=10237798330 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=11261578161 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=12285357992 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=13309137823 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=14332917654 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=15356697485 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=16380477316 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=17404257147 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000019-000009 --seed=18428036978 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:07:07] selfplay finished: 29.758 seconds
[2019-06-18 12:07:07] selfplay mn: 29.778 seconds
[2019-06-18 12:07:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-20-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779851 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559682 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339513 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119344 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899175 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679006 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458837 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238668 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018499 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798330 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578161 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357992 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137823 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917654 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697485 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477316 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257147 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036978 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816809 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596640 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376471 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156302 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000019-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:07:10] divide_golden_chunk finished: 3.313 seconds
[2019-06-18 12:07:10] generate golden chunk: 3.328 seconds
[2019-06-18 12:07:10] train finished: 43.768 seconds
:::MLL 1560881191.981831 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881191.982688 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881191.983502 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.082703 47019097617280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.083727 47019097617280 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5mk5378m
I0618 12:06:32.084727 47019097617280 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5mk5378m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3c532be80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.085170 47019097617280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.090140 47019097617280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881192.010263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.011042 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.011745 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.093010 47912195093376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.094005 47912195093376 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptnn2m5sx
I0618 12:06:32.095160 47912195093376 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptnn2m5sx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93b5f4ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.095641 47912195093376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.100755 47912195093376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.109545 47019097617280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881192.013289 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.014176 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.014997 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.111976 47790531806080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.017877 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.018540 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.019257 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.112336 47304861258624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.113064 47790531806080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptwt31cl9
W0618 12:06:32.113325 47304861258624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2ktcn91o
I0618 12:06:32.114073 47790531806080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptwt31cl9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7762425e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.114314 47304861258624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2ktcn91o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b064e09ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.114484 47790531806080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.114705 47304861258624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.119255 47790531806080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.119366 47304861258624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.122948 47912195093376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.138490 47790531806080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.138745 47304861258624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881192.070370 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.070846 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.071243 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.143818 46986279453568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.071732 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.072170 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.072546 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.144018 46923096028032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:06:32.144864 46986279453568 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc21155cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:32.145002 46923096028032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplh5987uz
I0618 12:06:32.145975 46986279453568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.145981 46923096028032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplh5987uz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad6b0ebe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.146371 46923096028032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.150674 46986279453568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.151004 46923096028032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.158210 47019097617280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.162560 47019097617280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:06:32.167726 47019097617280 estimator.py:1111] Calling model_fn.
W0618 12:06:32.167839 47019097617280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:32.169209 47019097617280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:32.169706 46986279453568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.170085 46923096028032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.170404 47912195093376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.174709 47912195093376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881192.098616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.099125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.099562 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.179672 47631724921728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:06:32.179802 47912195093376 estimator.py:1111] Calling model_fn.
W0618 12:06:32.179911 47912195093376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:32.180771 47631724921728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4kqattkp
I0618 12:06:32.181829 47631724921728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4kqattkp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5268a19e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:32.181264 47912195093376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:06:32.182254 47631724921728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.186799 47790531806080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.187063 47631724921728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.187116 47304861258624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881192.102255 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.102699 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.103079 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.188361 46918363620224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.189349 46918363620224 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpg4yxjozt
I0618 12:06:32.190310 46918363620224 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpg4yxjozt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aac50fbfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.190702 46918363620224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.191102 47790531806080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:32.191442 47304861258624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:32.195288 46918363620224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:06:32.196204 47790531806080 estimator.py:1111] Calling model_fn.
W0618 12:06:32.196317 47790531806080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:32.196588 47304861258624 estimator.py:1111] Calling model_fn.
W0618 12:06:32.196700 47304861258624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881192.096312 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.097195 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.098044 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.195214 46920427520896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.102079 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.102813 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.103449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.195379 47246298387328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.197684 47790531806080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:32.198050 47304861258624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:32.196325 46920427520896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqtg7lpsc
W0618 12:06:32.196470 47246298387328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpstxp2zvg
I0618 12:06:32.197419 46920427520896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqtg7lpsc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaccc009e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.197640 47246298387328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpstxp2zvg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af8ab6b6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.197862 46920427520896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.198095 47246298387328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.203078 46920427520896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.203320 47246298387328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.206325 47631724921728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881192.103198 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.103958 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.104616 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.206116 47270594196352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.100756 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.101481 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.102232 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.206139 47784885158784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.207161 47270594196352 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps6pqc0_x
W0618 12:06:32.207334 47784885158784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_vi8a1zt
I0618 12:06:32.208303 47270594196352 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps6pqc0_x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe53900e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.208534 47784885158784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_vi8a1zt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7611b15e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.208732 47270594196352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881192.107625 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.108351 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.109051 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.209565 47937693488000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:06:32.208937 47784885158784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881192.110095 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.110895 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.111587 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.210005 47463667745664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.210715 47937693488000 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpi5j2zo_m
I0618 12:06:32.211837 47937693488000 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpi5j2zo_m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b99a5c76e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:32.211077 47463667745664 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqnkuehj4
I0618 12:06:32.212177 47463667745664 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqnkuehj4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2b47a49e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.212285 47937693488000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.212640 47463667745664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.214422 46918363620224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.213631 47270594196352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.213655 47784885158784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.217545 47937693488000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.217189 46986279453568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.217812 47463667745664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.217820 46923096028032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.221472 46986279453568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:32.222125 46923096028032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:32.225021 46920427520896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.225652 47246298387328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:32.226526 46986279453568 estimator.py:1111] Calling model_fn.
W0618 12:06:32.226634 46986279453568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:32.227207 46923096028032 estimator.py:1111] Calling model_fn.
W0618 12:06:32.227316 46923096028032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:32.227988 46986279453568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:32.228656 46923096028032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:32.233458 47270594196352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.233966 47784885158784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.239522 47937693488000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:32.239628 47463667745664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881192.165239 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.165685 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.166048 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.248261 47956535133056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.167975 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.168397 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.168745 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.248509 46942786614144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.249314 47956535133056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6v89u0rv
W0618 12:06:32.249500 46942786614144 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpb19agch9
I0618 12:06:32.250341 47956535133056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6v89u0rv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e08d41e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.250484 46942786614144 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpb19agch9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab200b53e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.250733 47956535133056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.250882 46942786614144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.253797 47631724921728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:32.255355 47956535133056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.255404 46942786614144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.258101 47631724921728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881192.167204 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.167682 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.168133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.258193 47326617068416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.168186 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.168668 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.169172 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.258356 46939184403328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.259259 47326617068416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_mq2dq1x
W0618 12:06:32.259388 46939184403328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz3s7auj7
I0618 12:06:32.260314 47326617068416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_mq2dq1x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b5ec93dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.260433 46939184403328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz3s7auj7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab129ffde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.260716 47326617068416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:32.260836 46939184403328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.262051 46918363620224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:06:32.263189 47631724921728 estimator.py:1111] Calling model_fn.
W0618 12:06:32.263295 47631724921728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:32.264640 47631724921728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881192.193647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.194095 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.194482 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.265847 47634285298560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.266399 46918363620224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:32.265395 47326617068416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:32.265425 46939184403328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881192.196788 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.197175 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.197549 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.267015 47289933894528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.266860 47634285298560 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyjsgfcfe
I0618 12:06:32.267858 47634285298560 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyjsgfcfe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53013dde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.268262 47634285298560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:32.267971 47289933894528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpczvxuwxa
I0618 12:06:32.268959 47289933894528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpczvxuwxa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b02d44c5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.269358 47289933894528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881192.161558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.162429 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.163335 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.269632 47230422610816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560881192.166402 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881192.167156 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881192.167862 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:32.269673 46919702246272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:06:32.270805 47230422610816 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpncrzsj94
W0618 12:06:32.270836 46919702246272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6969krp7
I0618 12:06:32.271543 46918363620224 estimator.py:1111] Calling model_fn.
W0618 12:06:32.271653 46918363620224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:32.271847 47230422610816 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpncrzsj94', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af4f9264da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:32.271876 46919702246272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6969krp7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn[2019-06-18 12:07:10] moving /lfs/lfs12/gma_akey/results/epb162/models/000019-000010.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb
[2019-06-18 12:07:10] moving /lfs/lfs12/gma_akey/results/epb162/models/000019-000010.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000019-000011.meta
[2019-06-18 12:07:10] moving /lfs/lfs12/gma_akey/results/epb162/models/000019-000010.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000019-000011.data-00000-of-00001
[2019-06-18 12:07:10] moving /lfs/lfs12/gma_akey/results/epb162/models/000019-000010.index --> /lfs/lfs12/gma_akey/results/epb162/models/000019-000011.index
[2019-06-18 12:07:10] iteration time 18: 48.123 seconds
2019-06-18 12:07:11.746181: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881230.635633 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:07:15] minmax time: 3.295 seconds
2019-06-18 12:07:15.050768: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:15.056210: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:15.060865: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881235.072651 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 18}}
[2019-06-18 12:07:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:07:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=20 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=1023779851 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=2047559682 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=3071339513 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=4095119344 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=5118899175 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=6142679006 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=7166458837 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=8190238668 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=9214018499 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=10237798330 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=11261578161 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=12285357992 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=13309137823 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=14332917654 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=15356697485 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=16380477316 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=17404257147 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=18428036978 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=19451816809 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000019-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000019-000011 --seed=20475596640 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:07:25] eval finished: 10.125 seconds
[2019-06-18 12:07:25] Win rate 000019-000011 vs 000018-000010: 0.430
:::MLL 1560881245.263587 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:07:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=21 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=1023779852 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=2047559683 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=3071339514 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=4095119345 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=5118899176 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=6142679007 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=7166458838 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=8190238669 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=9214018500 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=10237798331 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=11261578162 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=12285357993 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=13309137824 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=14332917655 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=15356697486 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=16380477317 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=17404257148 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000020-000010 --seed=18428036979 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:07:55] selfplay finished: 30.359 seconds
[2019-06-18 12:07:55] selfplay mn: 30.379 seconds
[2019-06-18 12:07:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-21-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779852 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559683 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339514 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119345 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899176 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679007 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458838 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238669 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018500 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798331 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578162 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357993 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137824 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917655 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697486 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477317 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257148 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036979 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816810 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596641 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376472 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156303 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000020-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:07:58] train finished: 43.645 seconds
:::MLL 1560881240.306957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.307809 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.308574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.407646 47985918444416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.307658 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.308474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.309221 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.407784 47269885141888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.408680 47985918444416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8k92bnm_
I0618 12:07:20.409814 47985918444416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8k92bnm_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba4e035de80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:20.408754 47269885141888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9hznno3x
I0618 12:07:20.410201 47269885141888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9hznno3x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe294cae80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.410261 47985918444416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.410646 47269885141888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.415089 47985918444416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.415310 47269885141888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881240.325033 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.325903 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.326710 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.422249 47668097479552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.337634 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.338393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.339080 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.422468 47931427873664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.423290 47668097479552 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpoegiq9qc
W0618 12:07:20.423430 47931427873664 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuoyezj_x
I0618 12:07:20.424291 47668097479552 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpoegiq9qc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ae09ace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.424427 47931427873664 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuoyezj_x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b983051be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.424696 47668097479552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.424833 47931427873664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.429968 47931427873664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.430023 47668097479552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.434601 47985918444416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.434901 47269885141888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.449434 47931427873664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.449596 47668097479552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881240.380737 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.381207 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.381603 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.461162 47116403831680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.382351 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.382815 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.383208 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.461178 47474617721728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
I0618 12:07:20.462197 47116403831680 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada6d19ccf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:20.462162 47474617721728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppmel91b4
I0618 12:07:20.463134 47474617721728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppmel91b4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2dd44fee80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.463318 47116403831680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.463535 47474617721728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.468047 47116403831680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.468172 47474617721728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.482802 47985918444416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.483371 47269885141888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.487108 47985918444416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.487314 47116403831680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.487575 47474617721728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.487704 47269885141888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:20.492188 47985918444416 estimator.py:1111] Calling model_fn.
W0618 12:07:20.492298 47985918444416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:20.492785 47269885141888 estimator.py:1111] Calling model_fn.
W0618 12:07:20.492892 47269885141888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:20.493656 47985918444416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.494260 47269885141888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.498339 47668097479552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.498630 47931427873664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.502783 47668097479552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.503064 47931427873664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:20.508018 47668097479552 estimator.py:1111] Calling model_fn.
W0618 12:07:20.508132 47668097479552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:20.508344 47931427873664 estimator.py:1111] Calling model_fn.
W0618 12:07:20.508455 47931427873664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:20.509495 47668097479552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.509823 47931427873664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881240.434947 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.435438 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.435791 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.511173 47117976413056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.437886 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.438291 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.438678 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.512388 47619518329728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.512189 47117976413056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpl_tkrc7r
I0618 12:07:20.513184 47117976413056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpl_tkrc7r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adacad57e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.513583 47117976413056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.513335 47619518329728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbo6js62t
I0618 12:07:20.514311 47619518329728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbo6js62t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f910fce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881240.407944 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.408678 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.409399 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.513913 47850089313152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
I0618 12:07:20.514708 47619518329728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881240.410489 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.411221 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.411875 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.514336 47972780524416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.400599 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.401345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.402029 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.514469 47949155353472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.515045 47850089313152 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpb73hkekx
:::MLL 1560881240.402951 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.403684 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.404379 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.514546 47042203779968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
I0618 12:07:20.516153 47850089313152 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpb73hkekx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b854029bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:20.515454 47972780524416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpipac1hh2
I0618 12:07:20.516553 47972780524416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpipac1hh2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba1d1210e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.516604 47850089313152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.517004 47972780524416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.515611 47949155353472 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7s4yd95r
W0618 12:07:20.515640 47042203779968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptoalr703
I0618 12:07:20.516715 47949155353472 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7s4yd95r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9c50f59e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.516716 47042203779968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptoalr703', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac9266ede48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:20.518304 47117976413056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:20.517165 47949155353472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.517168 47042203779968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.519232 47619518329728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.521922 47850089313152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.522421 47972780524416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.522558 47949155353472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.522563 47042203779968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.534916 47116403831680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.535653 47474617721728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.537498 47117976413056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.538260 47619518329728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.539204 47116403831680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.540050 47474617721728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.543991 47850089313152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:20.544261 47116403831680 estimator.py:1111] Calling model_fn.
W0618 12:07:20.544371 47116403831680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:20.544947 47972780524416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.544252 47042203779968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:20.545178 47474617721728 estimator.py:1111] Calling model_fn.
W0618 12:07:20.545287 47474617721728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:20.544640 47949155353472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.545735 47116403831680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.546654 47474617721728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881240.465557 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.466047 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.466604 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.560479 47213485798272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.470892 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.471304 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.471661 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.560703 47685095334784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.561498 47213485798272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3kppla3a
W0618 12:07:20.561676 47685095334784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1gmgauxz
I0618 12:07:20.562490 47213485798272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3kppla3a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af107a30e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.562661 47685095334784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1gmgauxz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ed5c16e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.562887 47213485798272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.563057 47685095334784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.567540 47213485798272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.567692 47685095334784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881240.499702 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.500097 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.500416 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.575009 47385016324992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.467776 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.468542 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.469281 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.576217 47190788883328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.454571 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.455478 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.456348 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.576421 47304857817984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881240.502388 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.502768 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881240.503128 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:20.576226 47668450538368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:07:20.576028 47385016324992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpe_fu2umy
I0618 12:07:20.577019 47385016324992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpe_fu2umy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18f7a72e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.577413 47385016324992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.577225 47190788883328 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8ssus691
W0618 12:07:20.577416 47304857817984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpq1zcoq5y
I0618 12:07:20.578226 47190788883328 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8ssus691', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aebbecb8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:20.577214 47668450538368 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjk_w55zc
I0618 12:07:20.578399 47304857817984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpq1zcoq5y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b064dd56e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.578618 47190788883328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.578182 47668450538368 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjk_w55zc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5af5a60e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:20.578860 47304857817984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:20.578575 47668450538368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:20.582139 47385016324992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.583499 47190788883328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.583621 47304857817984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.583201 47668450538368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:20.585296 47619518329728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.585350 47117976413056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.586663 47213485798272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.586850 47685095334784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:20.589595 47619518329728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.589708 47117976413056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.592624 47042203779968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.593854 47850089313152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.592817 47949155353472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:20.594630 47619518329728 estimator.py:1111] Calling model_fn.
W0618 12:07:20.594739 47619518329728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:20.594813 47117976413056 estimator.py:1111] Calling model_fn.
W0618 12:07:20.594924 47117976413056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:20.594811 47972780524416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:20.596090 47619518329728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.596286 47117976413056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:20.596902 47042203779968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.597105 47949155353472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.598151 47850089313152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:20.599123 47972780524416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881240.449920 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881240.450835 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'[2019-06-18 12:07:58] divide_golden_chunk finished: 3.276 seconds
[2019-06-18 12:07:58] generate golden chunk: 3.290 seconds
[2019-06-18 12:07:58] iteration time 19: 48.299 seconds
2019-06-18 12:08:00.085707: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881278.935000 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:08:03] minmax time: 3.253 seconds
2019-06-18 12:08:03.348499: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:08:03.353997: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:08:03.358618: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881283.371964 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 19}}
[2019-06-18 12:08:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000021-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:08:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=21 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=1023779852 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=2047559683 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=3071339514 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=4095119345 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=5118899176 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=6142679007 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=7166458838 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=8190238669 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=9214018500 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=10237798331 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=11261578162 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=12285357993 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=13309137824 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=14332917655 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=15356697486 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=16380477317 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=17404257148 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=18428036979 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=19451816810 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000020-000011 --seed=20475596641 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:14] eval finished: 10.762 seconds
[2019-06-18 12:08:14] Win rate 000020-000011 vs 000018-000010: 0.590
:::MLL 1560881294.203204 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:08:14] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=22 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=1023779853 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=2047559684 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=3071339515 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=4095119346 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=5118899177 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=6142679008 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=7166458839 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=8190238670 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=9214018501 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=10237798332 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=11261578163 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=12285357994 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=13309137825 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=14332917656 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=15356697487 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=16380477318 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=17404257149 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000021-000010 --seed=18428036980 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:08:44] selfplay finished: 30.159 seconds
[2019-06-18 12:08:44] selfplay mn: 30.176 seconds
[2019-06-18 12:08:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-22-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779853 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559684 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339515 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119346 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899177 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679008 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458839 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238670 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018501 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798332 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578163 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357994 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137825 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917656 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697487 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477318 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257149 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036980 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816811 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596642 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376473 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156304 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000021-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:08:47] train finished: 44.000 seconds
:::MLL 1560881288.575085 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.575794 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.576461 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.672063 47438278173568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.566958 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.567831 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.568640 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.672081 47980068713344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.673101 47438278173568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnt8ve1tx
W0618 12:08:08.673130 47980068713344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcpvuvb5p
I0618 12:08:08.674107 47980068713344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcpvuvb5p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba3838a0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.674108 47438278173568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnt8ve1tx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b255e4e7e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.674510 47438278173568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.674507 47980068713344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.679532 47438278173568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.679530 47980068713344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.698961 47980068713344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.699013 47438278173568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881288.641276 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.641858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.642355 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.725395 47593311581056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.650442 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.650894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.651272 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.725809 47212040254336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.726420 47593311581056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppfmjem0h
I0618 12:08:08.727390 47593311581056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppfmjem0h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4977047e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.726834 47212040254336 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af0b179cd68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.727799 47593311581056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.727952 47212040254336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.732423 47593311581056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.732574 47212040254336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881288.631473 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.632085 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.632822 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.734850 47306208445312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.633292 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.633983 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.634628 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.735040 47899180667776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.735878 47306208445312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxdn88j3i
W0618 12:08:08.736038 47899180667776 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_h9id62t
I0618 12:08:08.736868 47306208445312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxdn88j3i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b069e565e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.737043 47899180667776 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_h9id62t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90ae3c4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.737279 47306208445312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.737447 47899180667776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.742238 47306208445312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.742353 47899180667776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.747164 47980068713344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.747335 47438278173568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.751508 47593311581056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.751484 47980068713344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.751659 47438278173568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.751736 47212040254336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:08.756535 47980068713344 estimator.py:1111] Calling model_fn.
W0618 12:08:08.756646 47980068713344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:08.756732 47438278173568 estimator.py:1111] Calling model_fn.
W0618 12:08:08.756842 47438278173568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:08.758009 47980068713344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.758197 47438278173568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.761849 47306208445312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.762181 47899180667776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881288.680297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.681139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.681935 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.784749 47349153309568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.680549 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.681419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.682205 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.784986 47404488762240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.785897 47349153309568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2_qfcxct
W0618 12:08:08.786075 47404488762240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpcvk04o83
I0618 12:08:08.787007 47349153309568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2_qfcxct', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b109e0cfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.787152 47404488762240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpcvk04o83', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1d804cfda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.787455 47349153309568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.787591 47404488762240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.792704 47349153309568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.792848 47404488762240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.798964 47593311581056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.799353 47212040254336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881288.722621 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.723112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.723536 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.800783 47457251775360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.801797 47457251775360 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdn69a2jv
I0618 12:08:08.802790 47457251775360 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdn69a2jv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b29c938ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881288.728584 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.729043 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.729439 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.802982 47443009090432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:08:08.803197 47457251775360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.803259 47593311581056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.803643 47212040254336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.803953 47443009090432 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptd8czbb0
I0618 12:08:08.804931 47443009090432 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptd8czbb0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b26784a8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.805328 47443009090432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.807898 47457251775360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:08.808314 47593311581056 estimator.py:1111] Calling model_fn.
W0618 12:08:08.808422 47593311581056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:08.808679 47212040254336 estimator.py:1111] Calling model_fn.
W0618 12:08:08.808793 47212040254336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:08.809855 47443009090432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.809787 47593311581056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.810416 47306208445312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.810141 47212040254336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.810941 47899180667776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.814743 47306208445312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.814792 47349153309568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.814874 47404488762240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.815316 47899180667776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:08.819827 47306208445312 estimator.py:1111] Calling model_fn.
W0618 12:08:08.819938 47306208445312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:08.820406 47899180667776 estimator.py:1111] Calling model_fn.
W0618 12:08:08.820515 47899180667776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:08.821303 47306208445312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.821886 47899180667776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881288.691994 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.692771 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.693450 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.822408 47432861270912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.685859 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.686782 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.687627 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.822581 46913906164608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.823508 47432861270912 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpv5ml9nu2
W0618 12:08:08.823667 46913906164608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0a4i6grg
I0618 12:08:08.824597 47432861270912 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpv5ml9nu2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b241b6efe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.824777 46913906164608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0a4i6grg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab474c9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.825037 47432861270912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.825250 46913906164608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.827129 47457251775360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.828993 47443009090432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.830377 47432861270912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.830489 46913906164608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881288.704319 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.705246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.706084 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.837300 47251739161472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.712986 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.713741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.714470 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.837407 47185145172864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.838334 47251739161472 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwif9_yp7
W0618 12:08:08.838406 47185145172864 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6kkezpyt
I0618 12:08:08.839341 47251739161472 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwif9_yp7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9efb6fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.839410 47185145172864 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6kkezpyt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea6e676e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.839742 47251739161472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.839809 47185145172864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.844681 47185145172864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.844782 47251739161472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.852143 47432861270912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.852389 46913906164608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881288.734886 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.735685 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.736379 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.854838 47083872129920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.738205 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.738913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.739619 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.855052 47414414742400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.855843 47083872129920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0gl9i13z
I0618 12:08:08.856898 47083872129920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0gl9i13z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad2da0f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:08.856025 47414414742400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2pka1n_l
I0618 12:08:08.857146 47414414742400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2pka1n_l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1fcfef6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.857345 47083872129920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.857579 47414414742400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881288.779888 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.780260 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.780579 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.857295 47725110973312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.777992 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.778364 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.778688 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.857765 47122206438272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.858315 47725110973312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx7wb6p1y
I0618 12:08:08.859304 47725110973312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx7wb6p1y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6826dfada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:08.858758 47122206438272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp05bl0r0a
I0618 12:08:08.859706 47725110973312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.859733 47122206438272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp05bl0r0a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adbc6f68e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.860128 47122206438272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.862202 47083872129920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.862265 47414414742400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.864382 47725110973312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.864682 47122206438272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.864250 47185145172864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.864617 47251739161472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:08.865972 47349153309568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.866109 47404488762240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881288.761899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.762473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.762864 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.866034 46935531811712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881288.758459 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881288.758952 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881288.759418 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:08.866342 47107382064000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:08:08.867070 46935531811712 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp96bpoof
W0618 12:08:08.867339 47107382064000 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp23t7s3ky
I0618 12:08:08.868052 46935531811712 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp96bpoof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab05049be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.868308 47107382064000 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp23t7s3ky', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8535c8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:08.868450 46935531811712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:08.868702 47107382064000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:08.870268 47349153309568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.870391 47404488762240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:08.873111 46935531811712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:08.874574 47457251775360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.873264 47107382064000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:08.875304 47349153309568 estimator.py:1111] Calling model_fn.
W0618 12:08:08.875412 47349153309568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:08.875424 47404488762240 estimator.py:1111] Calling model_fn.
W0618 12:08:08.875533 47404488762240 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:08.876116 47443009090432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:08.876761 47349153309568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:08.876897 47404488762240 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be re[2019-06-18 12:08:47] divide_golden_chunk finished: 3.312 seconds
[2019-06-18 12:08:47] generate golden chunk: 3.326 seconds
[2019-06-18 12:08:47] moving /lfs/lfs12/gma_akey/results/epb162/models/000021-000011.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb
[2019-06-18 12:08:47] moving /lfs/lfs12/gma_akey/results/epb162/models/000021-000011.index --> /lfs/lfs12/gma_akey/results/epb162/models/000021-000012.index
[2019-06-18 12:08:47] moving /lfs/lfs12/gma_akey/results/epb162/models/000021-000011.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000021-000012.data-00000-of-00001
[2019-06-18 12:08:47] moving /lfs/lfs12/gma_akey/results/epb162/models/000021-000011.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000021-000012.meta
[2019-06-18 12:08:47] iteration time 20: 48.821 seconds
2019-06-18 12:08:48.958472: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881327.756103 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:08:52] minmax time: 3.266 seconds
2019-06-18 12:08:52.234964: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:08:52.240385: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:08:52.244992: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881332.257348 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 20}}
[2019-06-18 12:08:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000022-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:08:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=22 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=1023779853 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=2047559684 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=3071339515 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=4095119346 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=5118899177 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=6142679008 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=7166458839 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=8190238670 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=9214018501 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=10237798332 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=11261578163 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=12285357994 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=13309137825 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=14332917656 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=15356697487 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=16380477318 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=17404257149 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=18428036980 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=19451816811 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000020-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000021-000012 --seed=20475596642 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:09:03] eval finished: 10.788 seconds
[2019-06-18 12:09:03] Win rate 000021-000012 vs 000020-000011: 0.500
:::MLL 1560881343.112315 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:09:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=23 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=1023779854 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=2047559685 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=3071339516 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=4095119347 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=5118899178 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=6142679009 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=7166458840 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=8190238671 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=9214018502 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=10237798333 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=11261578164 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=12285357995 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=13309137826 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=14332917657 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=15356697488 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=16380477319 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=17404257150 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000022-000011 --seed=18428036981 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:09:33] selfplay finished: 30.406 seconds
[2019-06-18 12:09:33] selfplay mn: 30.424 seconds
[2019-06-18 12:09:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-23-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=23 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779854 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559685 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339516 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119347 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899178 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679009 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458840 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238671 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018502 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798333 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578164 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357995 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137826 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917657 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697488 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477319 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257150 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036981 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816812 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596643 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376474 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156305 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000022-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:09:35] train finished: 43.662 seconds
:::MLL 1560881337.426279 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.427183 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.428012 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.532752 47612221866880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.441769 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.442482 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.443173 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.533192 47338138420096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.533921 47612221866880 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuvucehyl
I0618 12:08:57.534955 47612221866880 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuvucehyl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4dde289e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:57.534281 47338138420096 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7se3qfif
I0618 12:08:57.535273 47338138420096 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7se3qfif', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e0d831e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.535363 47612221866880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.535671 47338138420096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.540087 47612221866880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.540320 47338138420096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.559623 47612221866880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.559787 47338138420096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881337.473586 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.474437 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.475188 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.577984 47848040653696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.474616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.475427 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.476150 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.578714 47869344932736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.578994 47848040653696 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvde80ex8
I0618 12:08:57.580040 47848040653696 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvde80ex8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b84c60dbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.580471 47848040653696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.579691 47869344932736 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7k8pu_5s
I0618 12:08:57.580752 47869344932736 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7k8pu_5s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b89bbe32da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.581183 47869344932736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.585416 47848040653696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.585921 47869344932736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881337.518604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.518980 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.519297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.597243 47433989747584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.516954 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.517394 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.517771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.597385 47833408942976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
I0618 12:08:57.598272 47433989747584 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b245eb24d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:57.598371 47833408942976 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzfn84o_w
I0618 12:08:57.599351 47833408942976 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzfn84o_w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b815def7e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.599401 47433989747584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.599749 47833408942976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.604084 47433989747584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.604807 47848040653696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.604325 47833408942976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.605265 47869344932736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.608633 47338138420096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.608600 47612221866880 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.612954 47338138420096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:57.612924 47612221866880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:57.617987 47612221866880 estimator.py:1111] Calling model_fn.
I0618 12:08:57.618036 47338138420096 estimator.py:1111] Calling model_fn.
W0618 12:08:57.618099 47612221866880 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.618143 47338138420096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.619500 47338138420096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:57.619462 47612221866880 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:57.623226 47433989747584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.623420 47833408942976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.653166 47869344932736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.653463 47848040653696 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881337.562305 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.562784 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.563284 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.654331 47875622617984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.534470 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.535337 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.536206 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.653237 47517879497600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.540653 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.541401 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.542132 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.653815 47794395214720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.655423 47875622617984 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuecchue7
:::MLL 1560881337.548025 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.548996 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.549870 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.656008 47680833192832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
I0618 12:08:57.656543 47875622617984 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuecchue7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b32111e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881337.561398 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.562113 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.562816 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.656160 47324056089472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.654397 47517879497600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1a_k_m29
I0618 12:08:57.655528 47517879497600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1a_k_m29', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b37e6ea4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.656949 47875622617984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.655004 47794395214720 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8tkbrlf5
I0618 12:08:57.655976 47517879497600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.656109 47794395214720 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8tkbrlf5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7848894e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:57.657459 47869344932736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:57.657788 47848040653696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:57.656565 47794395214720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.657176 47680833192832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkx7l8dyr
W0618 12:08:57.657276 47324056089472 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjjhdpkgo
I0618 12:08:57.658356 47680833192832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkx7l8dyr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5dd7b65e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.658377 47324056089472 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjjhdpkgo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0ac623be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.658817 47680833192832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.658837 47324056089472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.661644 47875622617984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881337.566603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.566978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.567420 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.661607 47779102274432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
I0618 12:08:57.662515 47869344932736 estimator.py:1111] Calling model_fn.
W0618 12:08:57.662630 47869344932736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.661288 47517879497600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:57.662881 47848040653696 estimator.py:1111] Calling model_fn.
W0618 12:08:57.662994 47848040653696 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.661656 47794395214720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.662599 47779102274432 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkpap1nrk
I0618 12:08:57.663587 47779102274432 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkpap1nrk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b74b9017e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.663989 47779102274432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.663998 47869344932736 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:57.664341 47848040653696 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:57.664183 47680833192832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.664232 47324056089472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.668543 47779102274432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.671009 47433989747584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.671005 47833408942976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.675292 47833408942976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:57.675323 47433989747584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:57.680344 47833408942976 estimator.py:1111] Calling model_fn.
W0618 12:08:57.681081 47875622617984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:57.680396 47433989747584 estimator.py:1111] Calling model_fn.
W0618 12:08:57.680452 47833408942976 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.680501 47433989747584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:57.681813 47833408942976 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:57.681882 47433989747584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881337.585653 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.586442 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.587131 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.684684 47365843547008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.577981 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.578912 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.579786 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.684823 47057948967808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.683626 47517879497600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.683676 47794395214720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.685738 47365843547008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc4gjwv9t
W0618 12:08:57.685809 47057948967808 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0fni89ni
I0618 12:08:57.686755 47365843547008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc4gjwv9t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1480ddbda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.686802 47057948967808 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0fni89ni', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2accd0eb5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:57.686403 47324056089472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:57.687167 47365843547008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.687199 47057948967808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.687243 47680833192832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.687837 47779102274432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.692017 47365843547008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.692036 47057948967808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881337.607447 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.607861 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.608316 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.703782 47635620938624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.605932 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.606365 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.606736 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.703841 47511291204480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
:::MLL 1560881337.557934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.558684 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.559351 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.705325 46914189763456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.704822 47635620938624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpty7pnlgh
W0618 12:08:57.704855 47511291204480 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmtldn0bj
:::MLL 1560881337.555232 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.555989 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.556706 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.705909 47932742714240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
I0618 12:08:57.705810 47635620938624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpty7pnlgh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5350da0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.705848 47511291204480 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmtldn0bj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b365e38edd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.706213 47635620938624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.706246 47511291204480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.706354 46914189763456 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpoph4qkv_
I0618 12:08:57.707356 46914189763456 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpoph4qkv_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab5833fdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:57.706878 47932742714240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqbedezyd
I0618 12:08:57.707756 46914189763456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:57.707864 47932742714240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqbedezyd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b987eb09e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.708260 47932742714240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.711388 47365843547008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.711547 47057948967808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.710836 47635620938624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.710852 47511291204480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.712543 46914189763456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.713145 47932742714240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881337.639151 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.639596 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.639982 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.719058 47432446301056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.720092 47432446301056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnk_hbzpz
I0618 12:08:57.721086 47432446301056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnk_hbzpz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2402b32e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.721481 47432446301056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881337.644182 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881337.644619 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881337.645004 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:57.722102 47181020050304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000012-000006.tfrecord.zz_0_0
W0618 12:08:57.723098 47181020050304 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnxnauktf
I0618 12:08:57.724088 47181020050304 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnxnauktf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae978870da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:57.724481 47181020050304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:57.726162 47432446301056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.728817 47875622617984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.729027 47181020050304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:57.729833 47511291204480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.729989 47635620938624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.731753 46914189763456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.733109 47875622617984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:57.732414 47932742714240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:57.732336 47517879497600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.732563 47794395214720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.735588 47779102274432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:57.736923 47324056089472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated [2019-06-18 12:09:36] divide_golden_chunk finished: 3.296 seconds
[2019-06-18 12:09:36] generate golden chunk: 3.311 seconds
[2019-06-18 12:09:36] moving /lfs/lfs12/gma_akey/results/epb162/models/000022-000012.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000022-000013.data-00000-of-00001
[2019-06-18 12:09:36] moving /lfs/lfs12/gma_akey/results/epb162/models/000022-000012.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000022-000013.meta
[2019-06-18 12:09:36] moving /lfs/lfs12/gma_akey/results/epb162/models/000022-000012.index --> /lfs/lfs12/gma_akey/results/epb162/models/000022-000013.index
[2019-06-18 12:09:36] moving /lfs/lfs12/gma_akey/results/epb162/models/000022-000012.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb
[2019-06-18 12:09:36] iteration time 21: 49.133 seconds
2019-06-18 12:09:38.134601: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881376.888836 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:09:41] minmax time: 3.278 seconds
2019-06-18 12:09:41.423121: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:09:41.428699: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:09:41.433295: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881381.445178 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 21}}
[2019-06-18 12:09:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000023-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:09:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=23 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=1023779854 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=2047559685 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=3071339516 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=4095119347 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=5118899178 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=6142679009 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=7166458840 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=8190238671 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=9214018502 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=10237798333 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=11261578164 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=12285357995 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=13309137826 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=14332917657 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=15356697488 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=16380477319 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=17404257150 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=18428036981 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=19451816812 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000021-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000022-000013 --seed=20475596643 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:09:51] eval finished: 10.440 seconds
[2019-06-18 12:09:51] Win rate 000022-000013 vs 000021-000012: 0.670
:::MLL 1560881391.953764 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:09:51] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=24 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=1023779855 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=2047559686 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=3071339517 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=4095119348 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=5118899179 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=6142679010 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=7166458841 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=8190238672 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=9214018503 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=10237798334 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=11261578165 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=12285357996 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=13309137827 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=14332917658 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=15356697489 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=16380477320 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=17404257151 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000023-000012 --seed=18428036982 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:10:21] selfplay finished: 29.892 seconds
[2019-06-18 12:10:21] selfplay mn: 29.913 seconds
[2019-06-18 12:10:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-24-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=24 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779855 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559686 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339517 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119348 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899179 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679010 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458841 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238672 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018503 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798334 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578165 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357996 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137827 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917658 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697489 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477320 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257151 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036982 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816813 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596644 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376475 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156306 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000023-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:10:25] divide_golden_chunk finished: 3.303 seconds
[2019-06-18 12:10:25] generate golden chunk: 3.318 seconds
[2019-06-18 12:10:25] train finished: 43.763 seconds
:::MLL 1560881386.749387 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.750089 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.750754 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.855401 47363775845248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.856428 47363775845248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqspcdiy3
I0618 12:09:46.857423 47363775845248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqspcdiy3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14059f1e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.857834 47363775845248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881386.738142 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.739036 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.739880 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.858353 46945516102528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.859557 46945516102528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2u13lk4y
:::MLL 1560881386.767152 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.767876 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.768536 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.861164 47518775456640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.749278 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.750181 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.751060 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.861231 48000608678784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
I0618 12:09:46.860782 46945516102528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2u13lk4y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2a365ee80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.861185 46945516102528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.862235 47518775456640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuuqr9jx_
W0618 12:09:46.862295 48000608678784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9cbzxln_
I0618 12:09:46.863209 47518775456640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuuqr9jx_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b381c517e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.863265 48000608678784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9cbzxln_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba84bd10e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.863609 47518775456640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.862835 47363775845248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:46.863664 48000608678784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.866322 46945516102528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.868328 47518775456640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.868405 48000608678784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.882437 47363775845248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.887677 47518775456640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.887691 48000608678784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.889478 46945516102528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881386.802752 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.803468 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.804128 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.897188 47784878596992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.790020 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.790934 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.791710 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.897267 47959345824640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.898319 47784878596992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpm_fs5xbn
W0618 12:09:46.898374 47959345824640 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5tb36th4
I0618 12:09:46.899394 47784878596992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpm_fs5xbn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b76114d2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.899473 47959345824640 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5tb36th4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9eb05bde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.899848 47784878596992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.899920 47959345824640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.905103 47959345824640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.905103 47784878596992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881386.803340 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.803901 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.804381 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.908724 47316837507968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.816752 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.817234 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.817648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.908860 47878863500160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.909755 47316837507968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1qrxucif
I0618 12:09:46.909908 47878863500160 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8bf33d0d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.910712 47316837507968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1qrxucif', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0917e0ee80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.911022 47878863500160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.911122 47316837507968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881386.807612 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.808342 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.809033 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.911127 47015198118784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.802284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.803199 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.804100 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.911695 47615406068608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.912243 47015198118784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdiey_z9m
I0618 12:09:46.913375 47015198118784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdiey_z9m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac2dcc52e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:46.912798 47615406068608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpl1togv4k
I0618 12:09:46.913826 47015198118784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.913998 47615406068608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpl1togv4k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4e9bf3ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.914466 47615406068608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.915690 47316837507968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.915724 47878863500160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.919103 47015198118784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.919674 47615406068608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.927019 47959345824640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.927083 47784878596992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.931411 47363775845248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.934731 47316837507968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.934890 47878863500160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.936136 47518775456640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.936521 48000608678784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.935799 47363775845248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881386.843164 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.843621 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.844063 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.939389 48004289536896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.843563 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.844077 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.844451 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.939407 47662025991040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.940460 47518775456640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.940860 48000608678784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.940440 48004289536896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4tcq8f9r
W0618 12:09:46.940474 47662025991040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvpebf81i
I0618 12:09:46.941413 48004289536896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4tcq8f9r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba927367e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.941451 47662025991040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvpebf81i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5976b72e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.940949 47363775845248 estimator.py:1111] Calling model_fn.
W0618 12:09:46.941056 47363775845248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:46.941811 48004289536896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.941847 47662025991040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.940995 47015198118784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.942449 47363775845248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.941996 47615406068608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.944230 46945516102528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:46.945567 47518775456640 estimator.py:1111] Calling model_fn.
W0618 12:09:46.945675 47518775456640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:46.945964 48000608678784 estimator.py:1111] Calling model_fn.
W0618 12:09:46.946075 48000608678784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:46.946600 48004289536896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.946584 47662025991040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.947056 47518775456640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.947449 48000608678784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.949243 46945516102528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:46.955269 46945516102528 estimator.py:1111] Calling model_fn.
W0618 12:09:46.955397 46945516102528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:46.957040 46945516102528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.965977 47662025991040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.966167 48004289536896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881386.840100 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.841002 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.841849 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.966616 47322957796224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.847692 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.848453 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.849129 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.967030 47600783111040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.967689 47322957796224 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpi56a7xda
I0618 12:09:46.968693 47322957796224 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpi56a7xda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0a84ad2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881386.874329 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.874780 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.875212 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.966880 47338353816448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
:::MLL 1560881386.874514 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.874981 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.875383 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.966936 47457695957888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.968073 47600783111040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzj_u6bib
I0618 12:09:46.969094 47322957796224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.969079 47600783111040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzj_u6bib', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4b345afe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.969486 47600783111040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.967901 47338353816448 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpqujcsyn0
W0618 12:09:46.967930 47457695957888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7nqhysjj
I0618 12:09:46.968888 47338353816448 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpqujcsyn0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e1a59bdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.968905 47457695957888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7nqhysjj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b29e3b25e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.969288 47338353816448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:46.969303 47457695957888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881386.889186 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.889615 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.890012 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.972015 47126429983616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.973827 47322957796224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.974226 47600783111040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.973036 47126429983616 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9v2uhg4s
I0618 12:09:46.974023 47126429983616 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9v2uhg4s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adcc2b49e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.974426 47126429983616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881386.892367 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.892819 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.893198 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.974345 47260964053888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.973942 47457695957888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.973957 47338353816448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.975312 47260964053888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpq5znwstj
I0618 12:09:46.976268 47260964053888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpq5znwstj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc158fbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.976665 47260964053888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.979154 47126429983616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.980667 47959345824640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.980751 47784878596992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881386.824831 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.825749 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.826605 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.979753 47460093031296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.981254 47260964053888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.980784 47460093031296 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_l_idx5g
I0618 12:09:46.981858 47460093031296 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_l_idx5g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2a7292ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:46.982295 47316837507968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.982465 47878863500160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:46.982280 47460093031296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881386.840602 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881386.841379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881386.842085 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:46.983748 47157971207040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000022-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000013-000006.tfrecord.zz_0_0
W0618 12:09:46.985334 47959345824640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.985394 47784878596992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.984749 47157971207040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphfv8bhqe
I0618 12:09:46.985729 47157971207040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphfv8bhqe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae41ab58e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:46.986129 47157971207040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:46.986598 47316837507968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.986747 47878863500160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:46.986999 47460093031296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:46.990812 47959345824640 estimator.py:1111] Calling model_fn.
I0618 12:09:46.990842 47784878596992 estimator.py:1111] Calling model_fn.
W0618 12:09:46.990928 47959345824640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:46.990956 47784878596992 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:46.991023 47157971207040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:46.990836 47015198118784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:46.991655 47316837507968 estimator.py:1111] Calling model_fn.
W0618 12:09:46.991767 47316837507968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:46.991791 47878863500160 estimator.py:1111] Calling model_fn.
W0618 12:09:46.991896 47878863500160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:46.991181 47615406068608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:46.992372 47959345824640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.992412 47784878596992 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.993222 47322957796224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.993140 47316837507968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:46.994038 47600783111040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:46.993273 47878863500160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be re[2019-06-18 12:10:25] moving /lfs/lfs12/gma_akey/results/epb162/models/000023-000013.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000023-000014.meta
[2019-06-18 12:10:25] moving /lfs/lfs12/gma_akey/results/epb162/models/000023-000013.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb
[2019-06-18 12:10:25] moving /lfs/lfs12/gma_akey/results/epb162/models/000023-000013.index --> /lfs/lfs12/gma_akey/results/epb162/models/000023-000014.index
[2019-06-18 12:10:25] moving /lfs/lfs12/gma_akey/results/epb162/models/000023-000013.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000023-000014.data-00000-of-00001
[2019-06-18 12:10:25] iteration time 22: 48.385 seconds
2019-06-18 12:10:26.563849: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881425.273616 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:10:29] minmax time: 3.273 seconds
2019-06-18 12:10:29.847152: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:10:29.852476: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:10:29.857070: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881429.868791 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 22}}
[2019-06-18 12:10:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000024-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:10:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=24 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=1023779855 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=2047559686 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=3071339517 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=4095119348 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=5118899179 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=6142679010 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=7166458841 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=8190238672 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=9214018503 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=10237798334 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=11261578165 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=12285357996 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=13309137827 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=14332917658 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=15356697489 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=16380477320 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=17404257151 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=18428036982 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=19451816813 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000022-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000023-000014 --seed=20475596644 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:10:41] eval finished: 11.260 seconds
[2019-06-18 12:10:41] Win rate 000023-000014 vs 000022-000013: 0.520
:::MLL 1560881441.195973 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:10:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=25 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=1023779856 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=2047559687 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=3071339518 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=4095119349 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=5118899180 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=6142679011 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=7166458842 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=8190238673 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=9214018504 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=10237798335 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=11261578166 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=12285357997 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=13309137828 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=14332917659 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=15356697490 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=16380477321 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=17404257152 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000024-000013 --seed=18428036983 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:11:11] selfplay finished: 29.817 seconds
[2019-06-18 12:11:11] selfplay mn: 29.835 seconds
[2019-06-18 12:11:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-25-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=25 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779856 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559687 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339518 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119349 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899180 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679011 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458842 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238673 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018504 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798335 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578166 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357997 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137828 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917659 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697490 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477321 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257152 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036983 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816814 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596645 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376476 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156307 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000024-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:11:13] train finished: 43.850 seconds
:::MLL 1560881435.142109 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.142858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.143562 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.245273 47102528443264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.140177 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.140932 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.141664 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.245388 47587347088256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.246281 47102528443264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp_dnkhuv
W0618 12:10:35.246387 47587347088256 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgy7sktyh
I0618 12:10:35.247291 47102528443264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp_dnkhuv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad732102e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.247382 47587347088256 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgy7sktyh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4813819e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.247698 47102528443264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.247821 47587347088256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.252623 47102528443264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.252711 47587347088256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881435.124580 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.125345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.126121 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.259923 47897013003136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.126339 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.127116 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.127829 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.260406 47217123742592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.260947 47897013003136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7gffjgwp
I0618 12:10:35.261959 47897013003136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7gffjgwp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b902d087e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:35.261396 47217123742592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6ir05unn
I0618 12:10:35.262364 47897013003136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.262383 47217123742592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6ir05unn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af1e079ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.262775 47217123742592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.267312 47897013003136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.267791 47217123742592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.272193 47102528443264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.272565 47587347088256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.286616 47897013003136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.287573 47217123742592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881435.210502 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.210975 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.211378 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.303276 47579347825536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.212785 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.213252 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.213658 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.303811 47496738722688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
I0618 12:10:35.304312 47579347825536 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4636b68d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.305413 47579347825536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.304832 47496738722688 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkcyovpnr
I0618 12:10:35.305823 47496738722688 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkcyovpnr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b32fad39e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.306228 47496738722688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881435.189073 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.189610 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.190093 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.307727 47373293503360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.308779 47373293503360 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphqt0hn74
I0618 12:10:35.309753 47373293503360 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphqt0hn74', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b163ceb1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.310158 47373293503360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.310031 47579347825536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.310801 47496738722688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881435.200111 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.200570 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.200962 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.311744 47033231467392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.312723 47033231467392 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzdbyallc
I0618 12:10:35.313689 47033231467392 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzdbyallc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac70fa42e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.314089 47033231467392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.314742 47373293503360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.318629 47033231467392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.320761 47102528443264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.321033 47587347088256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.325114 47102528443264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.325382 47587347088256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.329118 47579347825536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.329930 47496738722688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:10:35.330237 47102528443264 estimator.py:1111] Calling model_fn.
W0618 12:10:35.330350 47102528443264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:35.330486 47587347088256 estimator.py:1111] Calling model_fn.
W0618 12:10:35.330596 47587347088256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:35.331716 47102528443264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.331973 47587347088256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.333955 47373293503360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881435.194540 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.195446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.196289 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.333724 47074301907840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.224548 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.225317 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.226052 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.334819 47209087914880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.335152 47897013003136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881435.202666 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.203409 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.204084 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.334066 47996531557248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.227179 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.227949 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.228640 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.335195 47904380105600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.335972 47217123742592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.335950 47209087914880 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfy1kdln0
W0618 12:10:35.334887 47074301907840 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuyyrn6zb
W0618 12:10:35.335167 47996531557248 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp3kj_8168
I0618 12:10:35.337048 47209087914880 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfy1kdln0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af00180ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.336007 47074301907840 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuyyrn6zb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad09fa16e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:35.336322 47904380105600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpffa0saop
I0618 12:10:35.336247 47996531557248 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp3kj_8168', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba758cd0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.337437 47904380105600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpffa0saop', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b91e4257e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.337498 47209087914880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.336467 47074301907840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.337922 47033231467392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:10:35.336687 47996531557248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.337898 47904380105600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.339509 47897013003136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.340356 47217123742592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.342729 47209087914880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.341739 47074301907840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.341904 47996531557248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.343210 47904380105600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:35.344651 47897013003136 estimator.py:1111] Calling model_fn.
W0618 12:10:35.344762 47897013003136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:35.345519 47217123742592 estimator.py:1111] Calling model_fn.
W0618 12:10:35.345630 47217123742592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:35.346134 47897013003136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.346993 47217123742592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.364517 47209087914880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.363606 47074301907840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.363837 47996531557248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.365921 47904380105600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881435.206697 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.207589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.208320 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.370550 47175310300032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.371565 47175310300032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphxucyg0r
I0618 12:10:35.372648 47175310300032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphxucyg0r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae824332dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.373124 47175310300032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.376555 47579347825536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.377247 47496738722688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881435.210241 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.210971 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.211634 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.377457 47699468804992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.377777 47175310300032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.378437 47699468804992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpu4_56zbr
I0618 12:10:35.379410 47699468804992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpu4_56zbr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b622e7b3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.379808 47699468804992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.381404 47373293503360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.380829 47579347825536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881435.260478 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.260974 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.261410 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.380490 47105936900992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.264489 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.264904 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.265266 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.380581 47367508886400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.381551 47496738722688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.381507 47105936900992 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpp3o8njfo
W0618 12:10:35.381576 47367508886400 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkc5ghj2r
I0618 12:10:35.382486 47105936900992 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpp3o8njfo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad7fd391e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.382565 47367508886400 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkc5ghj2r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14e420cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.382883 47105936900992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.382960 47367508886400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.385098 47033231467392 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:35.385701 47373293503360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:35.384632 47699468804992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:35.385903 47579347825536 estimator.py:1111] Calling model_fn.
W0618 12:10:35.386011 47579347825536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:35.386644 47496738722688 estimator.py:1111] Calling model_fn.
W0618 12:10:35.386758 47496738722688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:35.387362 47579347825536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.388118 47496738722688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.387495 47367508886400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.387503 47105936900992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.389392 47033231467392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:35.390790 47373293503360 estimator.py:1111] Calling model_fn.
W0618 12:10:35.390897 47373293503360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:35.392242 47373293503360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:10:35.394471 47033231467392 estimator.py:1111] Calling model_fn.
W0618 12:10:35.394588 47033231467392 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881435.307762 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.308258 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.308607 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.394080 47029361853312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.309937 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.310351 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.310717 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.394448 47023546848128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.395932 47033231467392 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:35.395104 47029361853312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6urdcwxy
I0618 12:10:35.396134 47029361853312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6urdcwxy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac628fe9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:35.395455 47023546848128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpf9l9r3xd
I0618 12:10:35.396508 47023546848128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpf9l9r3xd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4ce64ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.396558 47029361853312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.396951 47023546848128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.397413 47175310300032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881435.228852 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.229641 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.230517 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.400403 47270445929344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
:::MLL 1560881435.230422 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881435.231200 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881435.231916 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:35.400515 47786924106624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000023-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000014-000007.tfrecord.zz_0_0
W0618 12:10:35.401205 47029361853312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.401510 47023546848128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.401537 47270445929344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpq_ksf8p7
W0618 12:10:35.401584 47786924106624 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8bdvaelh
I0618 12:10:35.402568 47270445929344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpq_ksf8p7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe4ab99e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.402590 47786924106624 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8bdvaelh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b768b393e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:35.402979 47270445929344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:35.402997 47786924106624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:35.403889 47699468804992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.407963 47786924106624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.407968 47270445929344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:35.406534 47367508886400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.406550 47105936900992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:35.412077 47074301907840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    -[2019-06-18 12:11:14] divide_golden_chunk finished: 3.315 seconds
[2019-06-18 12:11:14] generate golden chunk: 3.329 seconds
[2019-06-18 12:11:14] moving /lfs/lfs12/gma_akey/results/epb162/models/000024-000014.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000024-000015.meta
[2019-06-18 12:11:14] moving /lfs/lfs12/gma_akey/results/epb162/models/000024-000014.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000024-000015.data-00000-of-00001
[2019-06-18 12:11:14] moving /lfs/lfs12/gma_akey/results/epb162/models/000024-000014.index --> /lfs/lfs12/gma_akey/results/epb162/models/000024-000015.index
[2019-06-18 12:11:14] moving /lfs/lfs12/gma_akey/results/epb162/models/000024-000014.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb
[2019-06-18 12:11:14] iteration time 23: 49.136 seconds
2019-06-18 12:11:15.726020: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881474.409474 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:11:18] minmax time: 3.265 seconds
2019-06-18 12:11:19.001605: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:11:19.007102: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:11:19.011688: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881479.023485 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 23}}
[2019-06-18 12:11:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000025-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:11:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=25 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=1023779856 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=2047559687 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=3071339518 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=4095119349 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=5118899180 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=6142679011 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=7166458842 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=8190238673 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=9214018504 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=10237798335 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=11261578166 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=12285357997 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=13309137828 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=14332917659 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=15356697490 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=16380477321 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=17404257152 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=18428036983 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=19451816814 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000023-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000024-000015 --seed=20475596645 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:11:29] eval finished: 10.791 seconds
[2019-06-18 12:11:29] Win rate 000024-000015 vs 000023-000014: 0.490
:::MLL 1560881489.883440 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:11:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=26 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=1023779857 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=2047559688 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=3071339519 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=4095119350 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=5118899181 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=6142679012 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=7166458843 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=8190238674 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=9214018505 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=10237798336 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=11261578167 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=12285357998 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=13309137829 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=14332917660 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=15356697491 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=16380477322 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=17404257153 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000025-000014 --seed=18428036984 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:11:59] selfplay finished: 29.902 seconds
[2019-06-18 12:11:59] selfplay mn: 29.920 seconds
[2019-06-18 12:11:59] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-26-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=26 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779857 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559688 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339519 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119350 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899181 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679012 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458843 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238674 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018505 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798336 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578167 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357998 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137829 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917660 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697491 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477322 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257153 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036984 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816815 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596646 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376477 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156308 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000025-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:12:02] train finished: 43.533 seconds
:::MLL 1560881484.246440 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.247378 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.248242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.356380 47927789474688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.357414 47927789474688 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpf_6ok6gy
I0618 12:11:24.358417 47927789474688 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpf_6ok6gy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9757742e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.358823 47927789474688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.363689 47927789474688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881484.272933 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.273719 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.274510 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.365600 47654456861568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.366593 47654456861568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplysdpktn
I0618 12:11:24.367606 47654456861568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplysdpktn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b57b38f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.368017 47654456861568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.372827 47654456861568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.383098 47927789474688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.392296 47654456861568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881484.271899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.272638 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.273320 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.416682 47798278464384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881484.269715 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.270431 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.271136 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.417160 47998635451264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.417738 47798278464384 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyb0xt8cj
I0618 12:11:24.418752 47798278464384 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyb0xt8cj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b792ffefe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:24.418154 47998635451264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpz77_j5q_
I0618 12:11:24.419228 47798278464384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:24.419299 47998635451264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpz77_j5q_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7d633ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.419743 47998635451264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.424111 47798278464384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.424450 47998635451264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881484.298362 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.299259 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.300141 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.423427 47997685040000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881484.347275 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.347749 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.348169 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.424963 47036331058048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.424534 47997685040000 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpzeppt04z
W0618 12:11:24.426066 47036331058048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4mt5cu0u
I0618 12:11:24.425642 47997685040000 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpzeppt04z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba79d8dde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.427173 47036331058048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4mt5cu0u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7c8643e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.426096 47997685040000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:24.427655 47036331058048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881484.312400 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.313162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.313854 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.427365 47664306684800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.428466 47664306684800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpejuqlu9a
I0618 12:11:24.429579 47664306684800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpejuqlu9a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b59fea7de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.430018 47664306684800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.431738 47927789474688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:24.432714 47036331058048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.431388 47997685040000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.436071 47927789474688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.435206 47664306684800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.440477 47654456861568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:24.441174 47927789474688 estimator.py:1111] Calling model_fn.
W0618 12:11:24.441282 47927789474688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.442649 47927789474688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881484.353661 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.354103 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.354523 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.442939 47202708693888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.443587 47798278464384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.443604 47998635451264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881484.323996 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.324913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.325767 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.444171 47814870823808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.443932 47202708693888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpuf5kz6fv
W0618 12:11:24.444825 47654456861568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:24.444926 47202708693888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpuf5kz6fv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee85457e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.445337 47202708693888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.445311 47814870823808 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphpegat4m
:::MLL 1560881484.349088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.349865 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.350680 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.446240 47755426235264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
I0618 12:11:24.446414 47814870823808 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphpegat4m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d0cfa4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.446872 47814870823808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.447319 47755426235264 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2gpx1e9g
I0618 12:11:24.448415 47755426235264 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2gpx1e9g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f35cdde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.448861 47755426235264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.449856 47202708693888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:24.449928 47654456861568 estimator.py:1111] Calling model_fn.
W0618 12:11:24.450046 47654456861568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.451415 47654456861568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.452359 47036331058048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.452175 47814870823808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.454015 47755426235264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.453266 47997685040000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.457192 47664306684800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881484.327587 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.328054 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.328462 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.461035 47517653140352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881484.321898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.322430 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.322900 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.461216 47101719950208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
I0618 12:11:24.462069 47517653140352 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b37d96c5d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:24.462210 47101719950208 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplk8_7ocd
I0618 12:11:24.463176 47101719950208 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplk8_7ocd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad701df8e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.463201 47517653140352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:24.463570 47101719950208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.467860 47517653140352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.468076 47101719950208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.468847 47202708693888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.474075 47814870823808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.476009 47755426235264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881484.371839 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.372303 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.372697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.477525 47812479148928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881484.371732 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.372183 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.372579 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.477859 47573674042240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.478529 47812479148928 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpk5owz7ue
I0618 12:11:24.479517 47812479148928 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpk5owz7ue', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c7e6c3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:24.478862 47573674042240 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplgln1zgw
I0618 12:11:24.479835 47573674042240 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplgln1zgw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b44e4877e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.479913 47812479148928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:24.480231 47573674042240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881484.351635 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.352523 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.353410 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.483902 47477137560448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
:::MLL 1560881484.362244 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.362952 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.363677 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.484097 46985125884800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.484941 47477137560448 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp8e7_uc0k
W0618 12:11:24.485099 46985125884800 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo2x2gt5i
I0618 12:11:24.486022 47477137560448 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp8e7_uc0k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e6a81ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:24.484539 47812479148928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:24.486165 46985125884800 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo2x2gt5i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abbdc533e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:24.484755 47573674042240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:24.486467 47477137560448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:24.486606 46985125884800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.486722 47517653140352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.486961 47101719950208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.491313 47477137560448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.491339 46985125884800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.491471 47998635451264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:24.492360 47798278464384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:24.495756 47998635451264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.496726 47798278464384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.499565 47036331058048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:24.500771 47998635451264 estimator.py:1111] Calling model_fn.
W0618 12:11:24.500880 47998635451264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:24.501836 47798278464384 estimator.py:1111] Calling model_fn.
W0618 12:11:24.501944 47798278464384 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.502239 47998635451264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.503307 47798278464384 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.503834 47036331058048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.503296 47812479148928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.503662 47573674042240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.504989 47997685040000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:24.508877 47036331058048 estimator.py:1111] Calling model_fn.
W0618 12:11:24.508988 47036331058048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.507582 47664306684800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:24.510465 46985125884800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.510484 47477137560448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:24.510346 47036331058048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.509647 47997685040000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.512212 47664306684800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881484.428236 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.428607 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.428932 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.515002 47281340502912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.516004 47202708693888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:24.515130 47997685040000 estimator.py:1111] Calling model_fn.
W0618 12:11:24.515246 47997685040000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.516024 47281340502912 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo1e72k5e
:::MLL 1560881484.430558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.431090 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.431560 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.516607 46946879898496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
I0618 12:11:24.517026 47281340502912 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo1e72k5e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b00d417ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.517424 47281340502912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.516707 47997685040000 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.517562 46946879898496 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpub3l5nbt
I0618 12:11:24.518549 46946879898496 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpub3l5nbt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2f4afce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.517691 47664306684800 estimator.py:1111] Calling model_fn.
W0618 12:11:24.517804 47664306684800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:24.518950 46946879898496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.520319 47202708693888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.519276 47664306684800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.522084 47281340502912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.523456 46946879898496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:24.524870 47814870823808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:24.525378 47202708693888 estimator.py:1111] Calling model_fn.
W0618 12:11:24.525486 47202708693888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:24.526840 47202708693888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:24.526909 47755426235264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881484.303418 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881484.304229 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881484.305043 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:24.527185 47689007219584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000024-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000015-000008.tfrecord.zz_0_0
W0618 12:11:24.529152 47814870823808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:24.528231 47689007219584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmcs03xzg
I0618 12:11:24.529229 47689007219584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmcs03xzg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5fbeec1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:24.529628 47689007219584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:24.531230 47755426235264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:24.534179 47814870823808 estimator.py:1111] Calling model_fn.
W0618 12:11:24.534286 47814870823808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:[2019-06-18 12:12:03] divide_golden_chunk finished: 3.303 seconds
[2019-06-18 12:12:03] generate golden chunk: 3.317 seconds
[2019-06-18 12:12:03] moving /lfs/lfs12/gma_akey/results/epb162/models/000025-000015.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000025-000016.meta
[2019-06-18 12:12:03] moving /lfs/lfs12/gma_akey/results/epb162/models/000025-000015.index --> /lfs/lfs12/gma_akey/results/epb162/models/000025-000016.index
[2019-06-18 12:12:03] moving /lfs/lfs12/gma_akey/results/epb162/models/000025-000015.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb
[2019-06-18 12:12:03] moving /lfs/lfs12/gma_akey/results/epb162/models/000025-000015.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000025-000016.data-00000-of-00001
[2019-06-18 12:12:03] iteration time 24: 48.756 seconds
2019-06-18 12:12:04.521369: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881523.165161 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:12:07] minmax time: 3.287 seconds
2019-06-18 12:12:07.818865: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:07.824380: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:07.828920: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881527.840777 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 24}}
[2019-06-18 12:12:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:12:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=26 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=1023779857 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=2047559688 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=3071339519 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=4095119350 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=5118899181 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=6142679012 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=7166458843 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=8190238674 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=9214018505 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=10237798336 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=11261578167 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=12285357998 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=13309137829 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=14332917660 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=15356697491 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=16380477322 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=17404257153 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=18428036984 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=19451816815 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000025-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000025-000016 --seed=20475596646 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:12:18] eval finished: 10.237 seconds
[2019-06-18 12:12:18] Win rate 000025-000016 vs 000024-000015: 0.340
:::MLL 1560881538.146986 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:12:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=27 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=1023779858 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=2047559689 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=3071339520 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=4095119351 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=5118899182 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=6142679013 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=7166458844 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=8190238675 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=9214018506 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=10237798337 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=11261578168 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=12285357999 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=13309137830 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=14332917661 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=15356697492 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=16380477323 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=17404257154 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000026-000015 --seed=18428036985 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:12:47] selfplay finished: 29.678 seconds
[2019-06-18 12:12:47] selfplay mn: 29.699 seconds
[2019-06-18 12:12:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-27-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=27 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779858 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559689 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339520 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119351 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899182 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679013 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458844 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238675 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018506 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798337 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578168 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285357999 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137830 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917661 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697492 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477323 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257154 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036985 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816816 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596647 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376478 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156309 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000026-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:12:51] divide_golden_chunk finished: 3.420 seconds
[2019-06-18 12:12:51] generate golden chunk: 3.434 seconds
[2019-06-18 12:12:52] train finished: 44.180 seconds
:::MLL 1560881533.080153 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.080878 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.081530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.190708 47711685841792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560881533.075058 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.075975 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.076812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.190747 47853450265472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.191735 47711685841792 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpki6lx7ux
W0618 12:12:13.191783 47853450265472 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp08rbhn75
I0618 12:12:13.192735 47711685841792 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpki6lx7ux', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6506ac6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.192820 47853450265472 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp08rbhn75', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b86087dde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.193130 47711685841792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:13.193238 47853450265472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.198089 47711685841792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.198211 47853450265472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.217272 47711685841792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.217808 47853450265472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881533.138113 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.138876 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.139590 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.250254 47827693990784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560881533.132345 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.133227 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.134077 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.250727 47144601600896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.251531 47827693990784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdsg06yl8
W0618 12:12:13.251820 47144601600896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7wyxcvus
I0618 12:12:13.252651 47827693990784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdsg06yl8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b80094c3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.252928 47144601600896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7wyxcvus', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae0fdd19e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.253104 47827693990784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:13.253382 47144601600896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881533.150239 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.150762 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.151240 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.254120 46921764230016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.255201 46921764230016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1xjcgmak
I0618 12:12:13.256235 46921764230016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1xjcgmak', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad1bad2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.256632 46921764230016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.258383 47827693990784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.258524 47144601600896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881533.156936 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.157431 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.157853 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.259844 47388266767232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.261306 46921764230016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.260866 47388266767232 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpotjbvhfp
I0618 12:12:13.261850 47388266767232 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpotjbvhfp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b19b964fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.262249 47388266767232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.265434 47711685841792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.265531 47853450265472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.266805 47388266767232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.269788 47711685841792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.269851 47853450265472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:13.274865 47711685841792 estimator.py:1111] Calling model_fn.
I0618 12:12:13.274922 47853450265472 estimator.py:1111] Calling model_fn.
W0618 12:12:13.274977 47711685841792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:13.275044 47853450265472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:13.276336 47711685841792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.276407 47853450265472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.280344 46921764230016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.280225 47827693990784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.280401 47144601600896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881533.095072 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.095794 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.096561 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.280363 47786576876416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560881533.096643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.097420 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.098094 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.280401 47800640840576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.281392 47800640840576 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1av22_jh
W0618 12:12:13.281422 47786576876416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo1ljs7it
I0618 12:12:13.282397 47800640840576 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1av22_jh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b79bccdfe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.282410 47786576876416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo1ljs7it', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b767686ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.282814 47800640840576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:13.282813 47786576876416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.286071 47388266767232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.287784 47800640840576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.287794 47786576876416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.307086 47800640840576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.307047 47786576876416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881533.217397 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.218084 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.218572 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.308299 47572466258816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.309330 47572466258816 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6mkbp47g
I0618 12:12:13.310325 47572466258816 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6mkbp47g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b449c8a2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.310732 47572466258816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881533.144833 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.145386 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.145804 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.311956 47790805640064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560881533.148222 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.148708 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.149156 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.313548 47801197556608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.312992 47790805640064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpo9p8_n3z
I0618 12:12:13.313982 47790805640064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpo9p8_n3z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b777294be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.314382 47790805640064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881533.230700 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.231119 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.231474 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.314826 47930127500160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.315422 47572466258816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:13.314592 47801197556608 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b79ddfcccf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.315714 47801197556608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.315797 47930127500160 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1d130aen
I0618 12:12:13.316773 47930127500160 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1d130aen', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b97e2cf9da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.317168 47930127500160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.319078 47790805640064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881533.143838 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.144747 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.145569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.319246 47525230908288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.320352 47801197556608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.321699 47930127500160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.320392 47525230908288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0vo3n3w0
I0618 12:12:13.321509 47525230908288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0vo3n3w0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b399d17ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.321946 47525230908288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881533.152360 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.153107 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.153825 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.324516 47354326184832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.327268 46921764230016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.325640 47354326184832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1v1hcu4f
I0618 12:12:13.326739 47354326184832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1v1hcu4f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11d260ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.327186 47354326184832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.327354 47525230908288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.330100 47144601600896 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.330178 47827693990784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.331533 46921764230016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.332961 47388266767232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.332389 47354326184832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.334382 47144601600896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.334456 47827693990784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.334648 47572466258816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:13.336573 46921764230016 estimator.py:1111] Calling model_fn.
W0618 12:12:13.336683 46921764230016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:13.337249 47388266767232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.338037 46921764230016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.338090 47790805640064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:13.339422 47144601600896 estimator.py:1111] Calling model_fn.
W0618 12:12:13.339531 47144601600896 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:13.339532 47827693990784 estimator.py:1111] Calling model_fn.
W0618 12:12:13.339645 47827693990784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:13.339529 47801197556608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.340669 47930127500160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.340894 47144601600896 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.340999 47827693990784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:12:13.342287 47388266767232 estimator.py:1111] Calling model_fn.
W0618 12:12:13.342393 47388266767232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881533.176825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.177740 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.178636 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.342324 47008510727040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.343744 47388266767232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881533.189280 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.190021 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.190724 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.342728 47088220201856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.343361 47008510727040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpojtnam9z
I0618 12:12:13.344359 47008510727040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpojtnam9z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac14e2bae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:13.343744 47088220201856 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprch340q2
I0618 12:12:13.344740 47088220201856 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprch340q2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad3dd398e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.344767 47008510727040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:13.345154 47088220201856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881533.159057 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.159824 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.160543 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.346704 47616553685888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.347826 47616553685888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9n_mrxco
I0618 12:12:13.348883 47616553685888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9n_mrxco', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4ee05ace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.349314 47616553685888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.349216 47525230908288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.349655 47008510727040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.350000 47088220201856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.354474 47616553685888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.353882 47354326184832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.355251 47786576876416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.355346 47800640840576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:13.359586 47786576876416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:13.359706 47800640840576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881533.205648 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.206174 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.206614 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.360175 47438172582784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
:::MLL 1560881533.212318 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.212728 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.213098 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.360889 47146844877696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
W0618 12:12:13.361212 47438172582784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvp1k1hll
I0618 12:12:13.362206 47438172582784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvp1k1hll', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2558034e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.362602 47438172582784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.361896 47146844877696 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4es3blcp
:::MLL 1560881533.161591 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881533.162333 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881533.163034 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:13.364065 47005849863040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000025-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000016-000008.tfrecord.zz_0_0
I0618 12:12:13.362873 47146844877696 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4es3blcp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae183874e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.363330 47146844877696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:13.364687 47786576876416 estimator.py:1111] Calling model_fn.
W0618 12:12:13.364804 47786576876416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:13.364812 47800640840576 estimator.py:1111] Calling model_fn.
W0618 12:12:13.364923 47800640840576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:13.365042 47005849863040 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpb6canuqg
I0618 12:12:13.366024 47005849863040 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpb6canuqg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0af921e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:13.366416 47005849863040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:13.366168 47786576876416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.366296 47800640840576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:13.367292 47438172582784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.367969 47146844877696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.369010 47008510727040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.369427 47088220201856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.371186 47005849863040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:13.373972 47616553685888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:13.382203 47572466258816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instruc[2019-06-18 12:12:52] iteration time 25: 48.878 seconds
2019-06-18 12:12:53.468447: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881572.042870 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:12:56] minmax time: 3.223 seconds
2019-06-18 12:12:56.701330: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:56.706937: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:56.711697: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881576.725611 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 25}}
[2019-06-18 12:12:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000027-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:12:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=27 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=1023779858 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=2047559689 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=3071339520 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=4095119351 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=5118899182 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=6142679013 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=7166458844 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=8190238675 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=9214018506 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=10237798337 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=11261578168 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=12285357999 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=13309137830 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=14332917661 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=15356697492 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=16380477323 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=17404257154 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=18428036985 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=19451816816 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000024-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000026-000016 --seed=20475596647 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:07] eval finished: 11.124 seconds
[2019-06-18 12:13:07] Win rate 000026-000016 vs 000024-000015: 0.570
:::MLL 1560881587.918886 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:13:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=28 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=1023779859 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=2047559690 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=3071339521 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=4095119352 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=5118899183 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=6142679014 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=7166458845 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=8190238676 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=9214018507 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=10237798338 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=11261578169 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=12285358000 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=13309137831 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=14332917662 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=15356697493 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=16380477324 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=17404257155 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000027-000015 --seed=18428036986 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:13:37] selfplay finished: 29.255 seconds
[2019-06-18 12:13:37] selfplay mn: 29.274 seconds
[2019-06-18 12:13:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-28-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=28 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779859 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559690 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339521 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119352 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899183 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679014 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458845 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238676 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018507 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798338 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578169 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285358000 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137831 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917662 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697493 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477324 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257155 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036986 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816817 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596648 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376479 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156310 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000027-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:13:40] train finished: 43.569 seconds
:::MLL 1560881581.996064 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881581.996965 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881581.997786 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.113658 46973844370304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.005110 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.005807 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.006484 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.113726 46940132029312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.114693 46973844370304 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp1auhzzxk
W0618 12:13:02.114738 46940132029312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpapkbh6l5
I0618 12:13:02.115702 46973844370304 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp1auhzzxk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab93be50da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.115729 46940132029312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpapkbh6l5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab1627b7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.116112 46973844370304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.116130 46940132029312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.121094 46940132029312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.121097 46973844370304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.140312 46940132029312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.140521 46973844370304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881582.055548 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.056321 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.057049 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.173168 47545138312064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.057509 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.058254 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.058941 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.174067 46991797240704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.174294 47545138312064 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0uqnuqx2
I0618 12:13:02.175410 47545138312064 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0uqnuqx2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3e3faace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.175880 47545138312064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.175163 46991797240704 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4_t1w6l9
I0618 12:13:02.176299 46991797240704 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4_t1w6l9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abd69f81e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.176784 46991797240704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.181244 47545138312064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.182058 46991797240704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881582.007330 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.008033 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.008698 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.181551 47776369693568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.002773 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.003654 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.004458 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.181783 47117957755776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.084226 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.084694 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.085141 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.183697 47607617803136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.182581 47776369693568 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfnclzdxb
:::MLL 1560881582.078038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.078567 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.079040 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.183727 47767734145920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.182772 47117957755776 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp69n434q6
I0618 12:13:02.183578 47776369693568 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfnclzdxb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7416219e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.183757 47117957755776 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp69n434q6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adac9b8ce80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.184058 47776369693568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.184393 47117957755776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.184787 47607617803136 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbmlzncnn
W0618 12:13:02.184756 47767734145920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpm3q8bwjw
I0618 12:13:02.185735 47767734145920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpm3q8bwjw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b721369ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.185759 47607617803136 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbmlzncnn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4ccbbc2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.186143 47767734145920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.186161 47607617803136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.188140 46940132029312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.188313 46973844370304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.188967 47776369693568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.189154 47117957755776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.190944 47607617803136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.190941 47767734145920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.192478 46940132029312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.192641 46973844370304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:13:02.197566 46940132029312 estimator.py:1111] Calling model_fn.
W0618 12:13:02.197675 46940132029312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:02.197745 46973844370304 estimator.py:1111] Calling model_fn.
W0618 12:13:02.197854 46973844370304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881582.066102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.066852 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.067558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.197262 47270435095424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.068778 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.069533 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.070234 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.197406 46940832236416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.199036 46940132029312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.199223 46973844370304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.198284 47270435095424 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpkxr4ini6
W0618 12:13:02.198403 46940832236416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphoxly77_
I0618 12:13:02.199284 47270435095424 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpkxr4ini6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afe4a145e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.199393 46940832236416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphoxly77_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab18c37de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.199689 47270435095424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.199791 46940832236416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.202928 47545138312064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.204355 46991797240704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.204608 47270435095424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.204730 46940832236416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.208729 47776369693568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.208901 47117957755776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.210054 47607617803136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.210030 47767734145920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.223776 47270435095424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881582.070158 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.070741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.071258 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.224163 47724321694592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.224159 46940832236416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881582.078390 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.078871 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.079273 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.225072 47902360101760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.225199 47724321694592 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpehwaff7n
I0618 12:13:02.226186 47724321694592 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpehwaff7n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b67f7d43e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.226588 47724321694592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.226103 47902360101760 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b916bbead68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.227224 47902360101760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.231219 47724321694592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.231875 47902360101760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881582.077166 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.077886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.078580 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.242896 47320279352192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.071147 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.072019 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.072853 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.242962 47815851623296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.244023 47320279352192 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprfkxth12
W0618 12:13:02.243994 47815851623296 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpooh4cl_8
I0618 12:13:02.244984 47320279352192 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprfkxth12', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b09e5075e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.244984 47815851623296 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpooh4cl_8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d47701e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.245383 47320279352192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.245388 47815851623296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881582.150095 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.150467 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.150798 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.244709 47754904134528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.143203 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.143734 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.144180 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.245051 47506733233024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.245724 47754904134528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp79dtg379
I0618 12:13:02.246735 47754904134528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp79dtg379', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f16af2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:02.246030 47506733233024 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp0y8uwjxa
I0618 12:13:02.247014 47506733233024 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp0y8uwjxa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b354e8bce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.247148 47754904134528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.247409 47506733233024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.250460 47320279352192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.250459 47815851623296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.250200 47724321694592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.251068 47902360101760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.252015 47754904134528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.252066 47506733233024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.253538 47545138312064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.254186 46991797240704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.257473 47767734145920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.257613 47607617803136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.256905 47117957755776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.257075 47776369693568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.257839 47545138312064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.258470 46991797240704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881582.144748 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.145409 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.145900 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.258214 47368711771008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
:::MLL 1560881582.151515 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.152009 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.152446 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.258407 47204981339008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.259247 47368711771008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpn5t2iwv2
W0618 12:13:02.259415 47204981339008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptjnf5sij
I0618 12:13:02.260238 47368711771008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpn5t2iwv2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b152bd34e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.260395 47204981339008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptjnf5sij', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aef0cbb3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:02.260641 47368711771008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:02.260793 47204981339008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:02.261787 47767734145920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.261900 47607617803136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.261218 47117957755776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.261433 47776369693568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:13:02.262861 47545138312064 estimator.py:1111] Calling model_fn.
W0618 12:13:02.262970 47545138312064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:02.263492 46991797240704 estimator.py:1111] Calling model_fn.
W0618 12:13:02.263602 46991797240704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:02.264327 47545138312064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.264966 46991797240704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.265311 47368711771008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:02.265428 47204981339008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:02.266294 47117957755776 estimator.py:1111] Calling model_fn.
I0618 12:13:02.266855 47767734145920 estimator.py:1111] Calling model_fn.
W0618 12:13:02.266405 47117957755776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:02.266964 47767734145920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:02.266951 47607617803136 estimator.py:1111] Calling model_fn.
W0618 12:13:02.267066 47607617803136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:02.266522 47776369693568 estimator.py:1111] Calling model_fn.
W0618 12:13:02.266630 47776369693568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:02.267783 47117957755776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.268322 47767734145920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.268414 47607617803136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.267999 47776369693568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:02.269483 47320279352192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.269722 47815851623296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.271086 47506733233024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.271578 47754904134528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:02.271608 47270435095424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.272581 46940832236416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:02.275909 47270435095424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:02.276922 46940832236416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:13:02.280975 47270435095424 estimator.py:1111] Calling model_fn.
W0618 12:13:02.281085 47270435095424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:02.281990 46940832236416 estimator.py:1111] Calling model_fn.
W0618 12:13:02.282099 46940832236416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:02.282462 47270435095424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881582.130235 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881582.130712 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881582.131152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:02.284476 47389863072640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000026-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000017-000009.tfrecord.zz_0_0
W0618 12:13:02.283461 46940832236416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a[2019-06-18 12:13:40] divide_golden_chunk finished: 3.211 seconds
[2019-06-18 12:13:40] generate golden chunk: 3.226 seconds
[2019-06-18 12:13:40] moving /lfs/lfs12/gma_akey/results/epb162/models/000027-000016.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000027-000017.data-00000-of-00001
[2019-06-18 12:13:40] moving /lfs/lfs12/gma_akey/results/epb162/models/000027-000016.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000027-000017.meta
[2019-06-18 12:13:40] moving /lfs/lfs12/gma_akey/results/epb162/models/000027-000016.index --> /lfs/lfs12/gma_akey/results/epb162/models/000027-000017.index
[2019-06-18 12:13:40] moving /lfs/lfs12/gma_akey/results/epb162/models/000027-000016.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb
[2019-06-18 12:13:40] iteration time 26: 48.425 seconds
2019-06-18 12:13:41.999569: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881620.467798 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:13:45] minmax time: 3.239 seconds
2019-06-18 12:13:45.249368: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:13:45.254815: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:13:45.259300: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881625.271460 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 26}}
[2019-06-18 12:13:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000028-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:13:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=28 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=1023779859 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=2047559690 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=3071339521 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=4095119352 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=5118899183 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=6142679014 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=7166458845 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=8190238676 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=9214018507 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=10237798338 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=11261578169 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=12285358000 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=13309137831 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=14332917662 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=15356697493 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=16380477324 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=17404257155 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=18428036986 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=19451816817 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000026-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000027-000017 --seed=20475596648 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:56] eval finished: 10.886 seconds
[2019-06-18 12:13:56] Win rate 000027-000017 vs 000026-000016: 0.520
:::MLL 1560881636.225416 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:13:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=29 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=1023779860 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=2047559691 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=3071339522 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=4095119353 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=5118899184 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=6142679015 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=7166458846 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=8190238677 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=9214018508 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=10237798339 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=11261578170 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=12285358001 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=13309137832 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=14332917663 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=15356697494 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=16380477325 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=17404257156 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000028-000016 --seed=18428036987 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:14:25] selfplay finished: 28.972 seconds
[2019-06-18 12:14:25] selfplay mn: 28.994 seconds
[2019-06-18 12:14:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-29-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=29 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779860 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559691 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339522 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119353 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899184 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679015 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458846 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238677 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018508 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798339 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578170 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285358001 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137832 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917663 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697494 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477325 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257156 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036987 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816818 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596649 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376480 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156311 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000028-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:14:28] divide_golden_chunk finished: 3.246 seconds
[2019-06-18 12:14:28] generate golden chunk: 3.261 seconds
[2019-06-18 12:14:28] train finished: 43.277 seconds
:::MLL 1560881630.518660 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.519542 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.520396 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.644486 47437664953216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.526561 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.527283 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.527925 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.644501 47984882754432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.645528 47437664953216 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5xznaew5
W0618 12:13:50.645557 47984882754432 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpmai0zelg
I0618 12:13:50.646595 47437664953216 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5xznaew5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2539c17e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.646615 47984882754432 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpmai0zelg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba4a27a6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.647025 47437664953216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.647050 47984882754432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.651879 47437664953216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.651893 47984882754432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.671261 47437664953216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.671407 47984882754432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881630.557127 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.557962 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.558760 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.692951 47394705445760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.557976 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.558863 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.559584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.693299 47689847473024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.694077 47394705445760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_n6w5jlt
I0618 12:13:50.695159 47394705445760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_n6w5jlt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b392b5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:50.694399 47689847473024 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp079y9z0m
I0618 12:13:50.695498 47689847473024 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp079y9z0m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ff1014e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.695615 47394705445760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.695948 47689847473024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.700868 47394705445760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.701176 47689847473024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881630.602666 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.603162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.603575 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.708509 47256645542784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.598191 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.598776 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.599246 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.708609 47085314171776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.534819 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.535528 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.536226 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.708650 47558996390784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.537576 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.538358 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.539046 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.708819 47646978466688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.709536 47256645542784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp9zwabfx8
W0618 12:13:50.709614 47085314171776 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgxa2db1i
I0618 12:13:50.710537 47256645542784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp9zwabfx8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afb14287e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.710580 47085314171776 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgxa2db1i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad330031e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.710940 47256645542784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.710983 47085314171776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.709728 47558996390784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_g0xdhwo
W0618 12:13:50.709831 47646978466688 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpjur_pwe5
I0618 12:13:50.710735 47558996390784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_g0xdhwo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4179ac4e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.710830 47646978466688 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpjur_pwe5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55f5d03e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.711148 47558996390784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.711248 47646978466688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.715638 47256645542784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.715652 47085314171776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.715996 47646978466688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.716142 47558996390784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.719109 47437664953216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.719538 47984882754432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.722520 47394705445760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.723408 47437664953216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.723123 47689847473024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.723859 47984882754432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881630.585856 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.586751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.587624 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.722952 47889166885760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.595590 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.596337 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.597043 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.723188 47966946222976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.724031 47889166885760 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnjbzifzx
I0618 12:13:50.725163 47889166885760 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnjbzifzx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e595e3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:50.724375 47966946222976 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4b9567m9
I0618 12:13:50.725452 47966946222976 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4b9567m9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba07560ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.725589 47889166885760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.725847 47966946222976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.728447 47437664953216 estimator.py:1111] Calling model_fn.
W0618 12:13:50.728561 47437664953216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:50.728960 47984882754432 estimator.py:1111] Calling model_fn.
W0618 12:13:50.729073 47984882754432 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:50.729921 47437664953216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.730426 47984882754432 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.730306 47889166885760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.730463 47966946222976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.734761 47256645542784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.734763 47085314171776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.735525 47646978466688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.736257 47558996390784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881630.609998 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.610970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.611869 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.749398 46915638633344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.614895 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.615635 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.616297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.749655 47128422630272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.586750 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.587321 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.587818 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.749287 47343150556032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.749411 47889166885760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.749467 47966946222976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881630.594137 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.594607 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.595029 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.750211 47560924369792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.750500 46915638633344 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6b_wqkjm
W0618 12:13:50.750742 47128422630272 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplz6djs5p
I0618 12:13:50.751503 46915638633344 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6b_wqkjm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aabae8ffe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.751739 47128422630272 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplz6djs5p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2add397a1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.751910 46915638633344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.750318 47343150556032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmphawc6r3s
I0618 12:13:50.752140 47128422630272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.751317 47343150556032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmphawc6r3s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0f38423e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881630.637645 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.638054 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.638412 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.751710 47903072232320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
I0618 12:13:50.751724 47343150556032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881630.631575 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.632084 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.632519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.751886 47133758329728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
I0618 12:13:50.751238 47560924369792 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41ec96ed68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.752356 47560924369792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.752711 47903072232320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpvjktgwlr
W0618 12:13:50.752889 47133758329728 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpfldsq002
I0618 12:13:50.753712 47903072232320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpvjktgwlr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b919630fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.753873 47133758329728 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpfldsq002', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade77826e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.754115 47903072232320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.754269 47133758329728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.756693 46915638633344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.756849 47128422630272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.756462 47343150556032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.757112 47560924369792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.758820 47903072232320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.758866 47133758329728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881630.560494 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.561437 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.562324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.764780 47972251186048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.765923 47972251186048 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6flt1oof
I0618 12:13:50.767011 47972251186048 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6flt1oof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba1b1940e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.767458 47972251186048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.771339 47394705445760 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.772184 47689847473024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.772852 47972251186048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.776083 46915638633344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.775596 47394705445760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.776298 47128422630272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.775841 47343150556032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.776518 47689847473024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.776459 47560924369792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.777802 47133758329728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.777976 47903072232320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:50.780609 47394705445760 estimator.py:1111] Calling model_fn.
W0618 12:13:50.780724 47394705445760 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:50.781623 47689847473024 estimator.py:1111] Calling model_fn.
W0618 12:13:50.781740 47689847473024 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:50.782078 47394705445760 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.782901 47256645542784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.782903 47085314171776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.783106 47689847473024 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.783390 47646978466688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881630.560825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.561745 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.562600 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.782624 47164999795584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.783966 47558996390784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:50.783718 47164999795584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplqdufry5
I0618 12:13:50.784795 47164999795584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplqdufry5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5bda55e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.785239 47164999795584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881630.668291 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.668774 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.669200 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.785806 47033388643200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
:::MLL 1560881630.664674 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881630.665225 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881630.665657 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:50.785941 47735303615360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000027-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000018-000009.tfrecord.zz_0_0
W0618 12:13:50.787252 47256645542784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.787252 47085314171776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.787694 47646978466688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:50.786846 47033388643200 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpbk7_b988
W0618 12:13:50.786957 47735303615360 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpgp5i37m_
I0618 12:13:50.787849 47033388643200 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpbk7_b988', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac719028dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:50.788306 47558996390784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:13:50.787927 47735303615360 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpgp5i37m_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a86670e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:50.788253 47033388643200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:50.788319 47735303615360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:50.790293 47164999795584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:50.792319 47085314171776 estimator.py:1111] Calling model_fn.
I0618 12:13:50.792338 47256645542784 estimator.py:1111] Calling model_fn.
W0618 12:13:50.792428 47085314171776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:50.792449 47256645542784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:50.792749 47646978466688 estimator.py:1111] Calling model_fn.
W0618 12:13:50.792857 47646978466688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:50.793786 47085314171776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.793809 47256645542784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.792929 47033388643200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:50.793366 47558996390784 estimator.py:1111] Calling model_fn.
W0618 12:13:50.792937 47735303615360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:50.793476 47558996390784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:50.794212 47646978466688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.794833 47558996390784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:50.794697 47972251186048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:50.797117 47889166885760 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instruc[2019-06-18 12:14:28] moving /lfs/lfs12/gma_akey/results/epb162/models/000028-000017.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000028-000018.data-00000-of-00001
[2019-06-18 12:14:28] moving /lfs/lfs12/gma_akey/results/epb162/models/000028-000017.index --> /lfs/lfs12/gma_akey/results/epb162/models/000028-000018.index
[2019-06-18 12:14:28] moving /lfs/lfs12/gma_akey/results/epb162/models/000028-000017.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000028-000018.meta
[2019-06-18 12:14:28] moving /lfs/lfs12/gma_akey/results/epb162/models/000028-000017.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb
[2019-06-18 12:14:28] iteration time 27: 48.143 seconds
2019-06-18 12:14:30.058351: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881668.611409 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:14:33] minmax time: 3.256 seconds
2019-06-18 12:14:33.324401: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:14:33.329945: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:14:33.334593: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881673.346839 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 27}}
[2019-06-18 12:14:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:14:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=29 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=1023779860 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=2047559691 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=3071339522 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=4095119353 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=5118899184 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=6142679015 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=7166458846 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=8190238677 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=9214018508 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=10237798339 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=11261578170 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=12285358001 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=13309137832 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=14332917663 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=15356697494 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=16380477325 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=17404257156 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=18428036987 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=19451816818 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000028-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000028-000018 --seed=20475596649 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:14:43] eval finished: 9.881 seconds
[2019-06-18 12:14:43] Win rate 000028-000018 vs 000027-000017: 0.480
:::MLL 1560881683.296401 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:14:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=30 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=1023779861 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=2047559692 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=3071339523 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=4095119354 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=5118899185 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=6142679016 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=7166458847 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=8190238678 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=9214018509 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=10237798340 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=11261578171 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=12285358002 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=13309137833 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=14332917664 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=15356697495 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=16380477326 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=17404257157 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000029-000017 --seed=18428036988 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:15:13] selfplay finished: 30.203 seconds
[2019-06-18 12:15:13] selfplay mn: 30.221 seconds
[2019-06-18 12:15:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-30-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=30 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779861 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559692 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339523 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119354 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899185 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679016 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458847 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238678 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018509 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798340 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578171 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285358002 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137833 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917664 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697495 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477326 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257157 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036988 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816819 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596650 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376481 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156312 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000029-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:15:16] divide_golden_chunk finished: 3.239 seconds
[2019-06-18 12:15:16] generate golden chunk: 3.255 seconds
[2019-06-18 12:15:16] train finished: 43.613 seconds
:::MLL 1560881678.656922 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.657658 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.658449 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.796180 47760627078016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
:::MLL 1560881678.658562 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.659306 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.659994 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.796146 47009755575168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.797295 47760627078016 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp21sm3s19
W0618 12:14:38.797323 47009755575168 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpx7ekyicm
I0618 12:14:38.798298 47760627078016 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp21sm3s19', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b706bcc6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.798299 47009755575168 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpx7ekyicm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac1985e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.798703 47760627078016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:38.798701 47009755575168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.803539 47760627078016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.803548 47009755575168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.822682 47009755575168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.822801 47760627078016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881678.645437 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.646167 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.646952 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.828130 47960435471232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.829259 47960435471232 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy33uy9bn
I0618 12:14:38.830421 47960435471232 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy33uy9bn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ef14e8e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.830894 47960435471232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.836078 47960435471232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881678.646401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.647208 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.647891 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.839355 47733390795648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.840344 47733390795648 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp2cp6aqho
I0618 12:14:38.841316 47733390795648 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp2cp6aqho', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a1463be80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.841711 47733390795648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.846407 47733390795648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.855795 47960435471232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881678.735385 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.735846 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.736250 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.858360 47626201576320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
:::MLL 1560881678.737941 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.738408 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.738810 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.859534 47105332978560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.859396 47626201576320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmptl83ul1t
I0618 12:14:38.860379 47626201576320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmptl83ul1t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b511f6a0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.860776 47626201576320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.860537 47105332978560 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpizr67875
I0618 12:14:38.861559 47105332978560 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpizr67875', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad7d939ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.861958 47105332978560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.865418 47626201576320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.865826 47733390795648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.866493 47105332978560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.870619 47009755575168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.870815 47760627078016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.874966 47009755575168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.875164 47760627078016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881678.715975 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.716432 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.716835 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.877854 47909152650112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
:::MLL 1560881678.710696 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.711263 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.711744 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.877868 47836471366528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
I0618 12:14:38.880084 47009755575168 estimator.py:1111] Calling model_fn.
W0618 12:14:38.880194 47009755575168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:38.880231 47760627078016 estimator.py:1111] Calling model_fn.
I0618 12:14:38.878919 47909152650112 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93009ccd68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:38.880339 47760627078016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.878886 47836471366528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpd29ie69_
I0618 12:14:38.879884 47836471366528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpd29ie69_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8214786e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.880032 47909152650112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:38.880283 47836471366528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.881548 47009755575168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.881670 47760627078016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.884399 47626201576320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.884713 47909152650112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.885488 47105332978560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.884921 47836471366528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881678.731614 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.732528 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.733417 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.885909 47825960108928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.887101 47825960108928 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwebweg_c
I0618 12:14:38.888285 47825960108928 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwebweg_c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7fa1f35e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.888765 47825960108928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881678.749393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.750143 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.750836 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.889097 47776307315584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.890193 47776307315584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy0j8qbj7
I0618 12:14:38.891304 47776307315584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy0j8qbj7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b741269de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.891773 47776307315584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.894325 47825960108928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.897026 47776307315584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881678.728668 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.729418 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.730181 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.899729 46928075457408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.900816 46928075457408 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmprpixjln7
I0618 12:14:38.901894 46928075457408 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmprpixjln7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae93dade10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.902319 46928075457408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.903727 47909152650112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.903817 47960435471232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.903893 47836471366528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.907654 46928075457408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.908130 47960435471232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:38.913231 47960435471232 estimator.py:1111] Calling model_fn.
W0618 12:14:38.913342 47960435471232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.913362 47733390795648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881678.730622 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.731397 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.732128 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.914464 47410082947968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.914689 47960435471232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.915468 47410082947968 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppnjzhj1o
I0618 12:14:38.916468 47410082947968 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppnjzhj1o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ecdbd7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.916875 47410082947968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.917030 47825960108928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.917665 47733390795648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.919117 47776307315584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.921819 47410082947968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:14:38.922720 47733390795648 estimator.py:1111] Calling model_fn.
W0618 12:14:38.922827 47733390795648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.924170 47733390795648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.927296 46928075457408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.931755 47626201576320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.932741 47105332978560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.936050 47626201576320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.937058 47105332978560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:38.941052 47626201576320 estimator.py:1111] Calling model_fn.
W0618 12:14:38.941347 47410082947968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:38.941160 47626201576320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:38.942129 47105332978560 estimator.py:1111] Calling model_fn.
W0618 12:14:38.942238 47105332978560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.942502 47626201576320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.943599 47105332978560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881678.833610 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.834397 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.834800 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.947120 47218980082560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.948153 47218980082560 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps0kdyqxm
:::MLL 1560881678.825441 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.826033 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.826520 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.948864 47557598081920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
I0618 12:14:38.949156 47218980082560 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps0kdyqxm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af24f1f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.949557 47218980082560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881678.774850 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.775791 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.776518 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.948904 47714837599104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
:::MLL 1560881678.778717 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.779483 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.780153 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.949188 47527325004672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.949861 47557598081920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpt2svugk_
I0618 12:14:38.950876 47557598081920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpt2svugk_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b412653ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.951271 47557598081920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.950040 47714837599104 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpyn0w21hf
W0618 12:14:38.951501 47909152650112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.951589 47836471366528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.950270 47527325004672 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpypfxw_g8
I0618 12:14:38.951165 47714837599104 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpyn0w21hf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b65c2885e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.951356 47527325004672 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpypfxw_g8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3a19e91e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.951619 47714837599104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:38.951797 47527325004672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881678.760389 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.761305 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.762138 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.952078 47132548989824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.954274 47218980082560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.953084 47132548989824 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpl5fa9lof
I0618 12:14:38.954075 47132548989824 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpl5fa9lof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade2f6d5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.954474 47132548989824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.955804 47557598081920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.955817 47909152650112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.955921 47836471366528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881678.784347 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.784826 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.785302 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.956720 46993118188416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
:::MLL 1560881678.785280 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.785743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.786216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.956996 47520302580608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.956897 47714837599104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.957002 47527325004672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.957806 46993118188416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpckau6a0l
W0618 12:14:38.958037 47520302580608 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7mkpcdl4
I0618 12:14:38.958833 46993118188416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpckau6a0l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abdb8b42da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.959068 47520302580608 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7mkpcdl4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3877579e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:38.959255 46993118188416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:38.959490 47520302580608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.959470 47132548989824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:14:38.960900 47909152650112 estimator.py:1111] Calling model_fn.
I0618 12:14:38.960963 47836471366528 estimator.py:1111] Calling model_fn.
W0618 12:14:38.961009 47909152650112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.961067 47836471366528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:38.962382 47909152650112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.962416 47836471366528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:38.964174 46993118188416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:38.964371 47520302580608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881678.761191 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881678.762094 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881678.762877 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:38.965412 47817121256320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000028-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000019-000009.tfrecord.zz_0_0
W0618 12:14:38.966393 47825960108928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:38.966465 47817121256320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa8s7objo
I0618 12:14:38.967475 47817121256320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa8s7objo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d931d2dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:38.968189 47776307315584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:14:38.967884 47817121256320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:38.970713 47825960108928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.972534 47776307315584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:38.973207 47218980082560 deprecation.py:323] From ./preprocessing.py[2019-06-18 12:15:16] iteration time 28: 48.370 seconds
2019-06-18 12:15:18.500976: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881716.981858 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:15:21] minmax time: 3.258 seconds
2019-06-18 12:15:21.769752: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:15:21.775226: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:15:21.779736: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881721.793304 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 28}}
[2019-06-18 12:15:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000030-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:15:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=30 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=1023779861 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=2047559692 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=3071339523 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=4095119354 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=5118899185 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=6142679016 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=7166458847 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=8190238678 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=9214018509 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=10237798340 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=11261578171 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=12285358002 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=13309137833 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=14332917664 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=15356697495 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=16380477326 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=17404257157 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=18428036988 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=19451816819 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000027-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000029-000018 --seed=20475596650 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:15:32] eval finished: 10.342 seconds
[2019-06-18 12:15:32] Win rate 000029-000018 vs 000027-000017: 0.590
:::MLL 1560881732.204616 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:15:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=31 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=1023779862 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=2047559693 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=3071339524 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=4095119355 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=5118899186 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=6142679017 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=7166458848 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=8190238679 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=9214018510 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=10237798341 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=11261578172 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=12285358003 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=13309137834 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=14332917665 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=15356697496 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=16380477327 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=17404257158 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000030-000017 --seed=18428036989 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:16:02] selfplay finished: 30.104 seconds
[2019-06-18 12:16:02] selfplay mn: 30.123 seconds
[2019-06-18 12:16:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-31-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=31 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779862 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559693 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339524 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119355 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899186 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679017 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458848 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238679 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018510 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798341 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578172 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285358003 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137834 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917665 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697496 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477327 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257158 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036989 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816820 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596651 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376482 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156313 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000030-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:16:05] train finished: 43.787 seconds
:::MLL 1560881727.023837 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.024567 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.025344 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.187030 47799846417280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.025519 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.026280 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.026983 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.188110 47352870364032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.188170 47799846417280 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmps1ropn5o
I0618 12:15:27.189172 47799846417280 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmps1ropn5o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b798d740e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.189571 47799846417280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.189103 47352870364032 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp6_yxn6v7
I0618 12:15:27.190070 47352870364032 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp6_yxn6v7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b117b9aae80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.190466 47352870364032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.194321 47799846417280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.195055 47352870364032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.213828 47799846417280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.214748 47352870364032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881727.103093 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.103608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.104057 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.238760 47011629224832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.094905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.095476 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.095945 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.239102 47788675564416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
I0618 12:15:27.239856 47011629224832 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac2080c1d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:27.240180 47788675564416 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc187huzf
I0618 12:15:27.241083 47011629224832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.241260 47788675564416 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc187huzf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b76f39e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.241690 47788675564416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.246047 47011629224832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.246541 47788675564416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881727.111571 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.112307 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.112956 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.258387 47511679742848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.114681 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.115416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.116085 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.258576 47247908156288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.259442 47511679742848 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxolgrlrh
W0618 12:15:27.259587 47247908156288 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpj36v0zfb
I0618 12:15:27.260432 47511679742848 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxolgrlrh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3675618e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.260581 47247908156288 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpj36v0zfb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af90b5e8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.260826 47511679742848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.260991 47247908156288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.261968 47799846417280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.262314 47352870364032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.265552 47511679742848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.265720 47247908156288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.266283 47799846417280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.266636 47352870364032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.266667 47011629224832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.267211 47788675564416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:27.271387 47799846417280 estimator.py:1111] Calling model_fn.
W0618 12:15:27.271498 47799846417280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:27.271761 47352870364032 estimator.py:1111] Calling model_fn.
W0618 12:15:27.271867 47352870364032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:27.272852 47799846417280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:27.273232 47352870364032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881727.140493 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.141253 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.141966 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.277477 47598509257600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.278592 47598509257600 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpco27ye57
I0618 12:15:27.279705 47598509257600 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpco27ye57', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4aacd2ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.280164 47598509257600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881727.135898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.136841 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.137671 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.280974 46967122133888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.282051 46967122133888 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpopdj_g33
I0618 12:15:27.283043 46967122133888 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpopdj_g33', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab7ab37ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.283449 46967122133888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.284658 47511679742848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881727.141690 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.142388 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.143065 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.284834 47644527526784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.284940 47247908156288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881727.142827 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.143587 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.144274 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.285211 47272310940544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.285530 47598509257600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.285841 47644527526784 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmph7fz9n9r
I0618 12:15:27.286836 47644527526784 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmph7fz9n9r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5563b9dda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.287233 47644527526784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.286328 47272310940544 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4dvajrqe
I0618 12:15:27.287400 47272310940544 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4dvajrqe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afeb9e37da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:27.288216 46967122133888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:15:27.287839 47272310940544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.292012 47644527526784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.292978 47272310940544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881727.157645 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.158571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.159473 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.305740 47939770618752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.307529 46967122133888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881727.176516 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.177226 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.177922 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.305998 47771807691648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.307398 47598509257600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.306889 47939770618752 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5e8msrao
W0618 12:15:27.307099 47771807691648 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpknpkwclp
I0618 12:15:27.307986 47939770618752 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5e8msrao', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a2195ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.308198 47771807691648 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpknpkwclp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7306370dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.308439 47939770618752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.308634 47771807691648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.311456 47644527526784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.313725 47939770618752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.313820 47771807691648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.314764 47272310940544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.317113 47011629224832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.317728 47788675564416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881727.192180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.192636 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.193039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.318339 47172451185536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.188180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.188749 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.189225 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.318413 47472884097920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.319362 47172451185536 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpy7uq6v1q
W0618 12:15:27.319412 47472884097920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpdh84r_lr
I0618 12:15:27.320350 47172451185536 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpy7uq6v1q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae779c88e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.320395 47472884097920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpdh84r_lr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2d6cfafe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.320753 47172451185536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.320798 47472884097920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.321528 47011629224832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.322196 47788675564416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.325526 47172451185536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.325525 47472884097920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:15:27.326817 47011629224832 estimator.py:1111] Calling model_fn.
W0618 12:15:27.326945 47011629224832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:27.327482 47788675564416 estimator.py:1111] Calling model_fn.
W0618 12:15:27.327600 47788675564416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:27.328417 47011629224832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:27.329042 47788675564416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:27.332314 47511679742848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.333072 47247908156288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.336580 47511679742848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.335645 47939770618752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.335870 47771807691648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.337384 47247908156288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:15:27.341657 47511679742848 estimator.py:1111] Calling model_fn.
W0618 12:15:27.341766 47511679742848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:27.342506 47247908156288 estimator.py:1111] Calling model_fn.
W0618 12:15:27.342612 47247908156288 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:27.343116 47511679742848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:27.343976 47247908156288 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881727.216263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.216733 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.217149 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.344635 47605032285056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.344692 47172451185536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881727.213608 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.214086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.214498 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.344647 47735628776320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.344738 47472884097920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:27.345678 47735628776320 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpa3xynlv4
W0618 12:15:27.345713 47605032285056 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5peseul5
I0618 12:15:27.346663 47735628776320 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpa3xynlv4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a99c89e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.346692 47605032285056 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5peseul5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4c31a03e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.347113 47735628776320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.347142 47605032285056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881727.234033 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.234484 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.234920 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.348734 47730029151104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.234849 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.235271 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.235654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.349489 47805114987392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.168950 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.169728 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.170450 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.349008 47395106784128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
:::MLL 1560881727.164490 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881727.165379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881727.166253 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:27.349117 47290492801920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000029-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000020-000010.tfrecord.zz_0_0
W0618 12:15:27.349767 47730029151104 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpc1lykhfv
I0618 12:15:27.350755 47730029151104 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpc1lykhfv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b694c050e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.351162 47730029151104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.350483 47805114987392 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmplahx85q1
W0618 12:15:27.351789 47605032285056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.351788 47735628776320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:15:27.351457 47805114987392 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmplahx85q1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ac77bee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:27.350056 47395106784128 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpnx45ncf2
W0618 12:15:27.350139 47290492801920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpec71y41m
I0618 12:15:27.351103 47395106784128 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpnx45ncf2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b51174dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.351207 47290492801920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpec71y41m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b02f59cae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:27.351915 47805114987392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.351516 47395106784128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:27.351615 47290492801920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:27.355262 46967122133888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.355940 47730029151104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.356579 47805114987392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.356195 47395106784128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.356445 47290492801920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:27.357593 47598509257600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.358478 47644527526784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:27.359574 46967122133888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.361930 47598509257600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.362757 47644527526784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:27.363588 47272310940544 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:15:27.364709 46967122133888 estimator.py:1111] Calling model_fn.
W0618 12:15:27.364821 46967122133888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:27.366190 46967122133888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:15:27.367052 47598509257600 estimator.py:1111] Calling model_fn.
W0618 12:15:27.367168 47598509257600 d[2019-06-18 12:16:06] divide_golden_chunk finished: 4.235 seconds
[2019-06-18 12:16:06] generate golden chunk: 4.250 seconds
[2019-06-18 12:16:06] moving /lfs/lfs12/gma_akey/results/epb162/models/000030-000018.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb162/models/000030-000019.data-00000-of-00001
[2019-06-18 12:16:06] moving /lfs/lfs12/gma_akey/results/epb162/models/000030-000018.pb --> /lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb
[2019-06-18 12:16:06] moving /lfs/lfs12/gma_akey/results/epb162/models/000030-000018.index --> /lfs/lfs12/gma_akey/results/epb162/models/000030-000019.index
[2019-06-18 12:16:06] moving /lfs/lfs12/gma_akey/results/epb162/models/000030-000018.meta --> /lfs/lfs12/gma_akey/results/epb162/models/000030-000019.meta
[2019-06-18 12:16:06] iteration time 29: 49.638 seconds
2019-06-18 12:16:08.229124: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881766.619663 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:16:11] minmax time: 3.306 seconds
2019-06-18 12:16:11.545500: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:16:11.551124: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:16:11.555789: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881771.568081 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 29}}
[2019-06-18 12:16:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb321 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb149 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb162/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb162/models/000031-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb193 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:16:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-eval-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=31 : \
-host epb326 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=1023779862 : \
-host epb161 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=2047559693 : \
-host epb283 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=3071339524 : \
-host epb325 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=4095119355 : \
-host epb327 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=5118899186 : \
-host epb163 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=6142679017 : \
-host epb324 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=7166458848 : \
-host epb118 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=8190238679 : \
-host epb148 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=9214018510 : \
-host epb320 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=10237798341 : \
-host epb246 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=11261578172 : \
-host epb329 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=12285358003 : \
-host epb299 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=13309137834 : \
-host epb298 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=14332917665 : \
-host epb242 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=15356697496 : \
-host epb244 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=16380477327 : \
-host epb245 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=17404257158 : \
-host epb243 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=18428036989 : \
-host epb240 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=19451816820 : \
-host epb248 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000030-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/000030-000019 --seed=20475596651 : \
-host epb249 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:16:22] eval finished: 10.466 seconds
[2019-06-18 12:16:22] Win rate 000030-000019 vs 000029-000018: 0.430
:::MLL 1560881782.103865 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:16:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-selfplay-32-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb162 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=32 : \
-host epb326 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=1023779863 : \
-host epb161 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=2047559694 : \
-host epb283 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=3071339525 : \
-host epb325 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=4095119356 : \
-host epb327 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=5118899187 : \
-host epb163 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=6142679018 : \
-host epb324 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=7166458849 : \
-host epb118 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=8190238680 : \
-host epb148 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=9214018511 : \
-host epb320 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=10237798342 : \
-host epb246 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=11261578173 : \
-host epb329 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=12285358004 : \
-host epb299 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=13309137835 : \
-host epb298 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=14332917666 : \
-host epb242 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=15356697497 : \
-host epb244 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=16380477328 : \
-host epb245 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=17404257159 : \
-host epb243 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb162/data/holdout/000031-000018 --seed=18428036990 : \
-host epb240 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb162/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000029-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb16
[2019-06-18 12:16:50] selfplay finished: 28.795 seconds
[2019-06-18 12:16:50] selfplay mn: 28.818 seconds
[2019-06-18 12:16:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb162/mpi/out-divide_golden_chunk-32-%r.txt \
-host epb162 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=32 : \
-host epb326 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=1023779863 : \
-host epb161 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=2047559694 : \
-host epb283 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=3071339525 : \
-host epb325 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=4095119356 : \
-host epb327 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=5118899187 : \
-host epb163 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=6142679018 : \
-host epb324 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=7166458849 : \
-host epb118 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=8190238680 : \
-host epb148 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=9214018511 : \
-host epb320 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=10237798342 : \
-host epb246 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=11261578173 : \
-host epb329 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=12285358004 : \
-host epb299 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=13309137835 : \
-host epb298 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=14332917666 : \
-host epb242 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=15356697497 : \
-host epb244 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=16380477328 : \
-host epb245 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=17404257159 : \
-host epb243 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=18428036990 : \
-host epb240 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=19451816821 : \
-host epb248 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=20475596652 : \
-host epb249 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=21499376483 : \
-host epb247 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000031-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb162 --seed=22523156314 : \
-host epb241 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb162/data/selfplay/000031-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb162/data/golde
[2019-06-18 12:16:54] divide_golden_chunk finished: 3.284 seconds
[2019-06-18 12:16:54] generate golden chunk: 3.299 seconds
[2019-06-18 12:16:55] train finished: 43.793 seconds
:::MLL 1560881776.844215 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.844914 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.845569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:16.993865 47762135425920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.839540 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.840393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.841203 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:16.994115 47312888030080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:16.995005 47762135425920 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxpkt946m
W0618 12:16:16.995229 47312888030080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpi7qapbry
I0618 12:16:16.996073 47762135425920 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxpkt946m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70c5b3ee80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:16.996250 47312888030080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpi7qapbry', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b082c78be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:16.996482 47762135425920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:16.996652 47312888030080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.001243 47762135425920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.001329 47312888030080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.020583 47762135425920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.020674 47312888030080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881776.880008 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.880867 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.881680 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.025060 47948803703680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.026173 47948803703680 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp13x7d63p
I0618 12:16:17.027181 47948803703680 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp13x7d63p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9c3bffde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.027600 47948803703680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.032542 47948803703680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881776.888631 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.889364 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.890012 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.033855 47713773638528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.034848 47713773638528 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpsupnt1kf
I0618 12:16:17.035832 47713773638528 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpsupnt1kf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b65831d8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.036240 47713773638528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.040984 47713773638528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881776.889393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.890119 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.890786 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.044174 47763276129152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.885127 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.886011 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.886834 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.044347 47992350552960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.045273 47763276129152 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpufmwtasa
W0618 12:16:17.045438 47992350552960 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmppyufmv8z
I0618 12:16:17.046367 47763276129152 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpufmwtasa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7109b18e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.046520 47992350552960 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmppyufmv8z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba65f980e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.046829 47763276129152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.046990 47992350552960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.052438 47948803703680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.052103 47763276129152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.052201 47992350552960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881776.905664 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.906134 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.906531 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.053004 47750633722752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.899727 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.900357 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.900814 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.052958 47438465577856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
I0618 12:16:17.054047 47750633722752 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb162/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6e1825ed68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:17.054018 47438465577856 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp5icq4nof
I0618 12:16:17.055003 47438465577856 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp5icq4nof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b25697a0e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.055163 47750633722752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.055399 47438465577856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.060235 47713773638528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.059877 47750633722752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.060002 47438465577856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.068660 47762135425920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.068911 47312888030080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.072986 47762135425920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.073240 47312888030080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.073422 47763276129152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.073949 47992350552960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:16:17.078055 47762135425920 estimator.py:1111] Calling model_fn.
W0618 12:16:17.078166 47762135425920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:17.078327 47312888030080 estimator.py:1111] Calling model_fn.
W0618 12:16:17.078437 47312888030080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.078878 47750633722752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.078923 47438465577856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.079535 47762135425920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.079807 47312888030080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881776.952839 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.953394 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.953876 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.094843 47141050459008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.957181 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.957665 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.958098 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.095802 47701244638080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.095877 47141050459008 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpama1h_ym
I0618 12:16:17.096879 47141050459008 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpama1h_ym', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae02a277da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.097283 47141050459008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.096807 47701244638080 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp_9vnwzly
I0618 12:16:17.097814 47701244638080 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp_9vnwzly', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6298544e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.098225 47701244638080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.100671 47948803703680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.101927 47141050459008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.102815 47701244638080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.105008 47948803703680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881776.965289 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.965774 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.966172 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.105587 46915805352832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.965952 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.966402 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.966763 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.106035 47038962365312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.106638 46915805352832 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpv82cryfk
W0618 12:16:17.107829 47713773638528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:16:17.107639 46915805352832 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpv82cryfk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aabb87fee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:17.107035 47038962365312 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpwgd98ws0
I0618 12:16:17.108039 46915805352832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.108027 47038962365312 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpwgd98ws0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8653ace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.108424 47038962365312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.110093 47948803703680 estimator.py:1111] Calling model_fn.
W0618 12:16:17.110202 47948803703680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.111541 47948803703680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.112163 47713773638528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.112730 46915805352832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.112998 47038962365312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:16:17.117265 47713773638528 estimator.py:1111] Calling model_fn.
W0618 12:16:17.117375 47713773638528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.118737 47713773638528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.120853 47141050459008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.121678 47701244638080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.121953 47763276129152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.122224 47992350552960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.126232 47763276129152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.126513 47992350552960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.126268 47438465577856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.126590 47750633722752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.130578 47438465577856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.130916 47750633722752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:16:17.131264 47763276129152 estimator.py:1111] Calling model_fn.
W0618 12:16:17.131373 47763276129152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:17.131548 47992350552960 estimator.py:1111] Calling model_fn.
W0618 12:16:17.131661 47992350552960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.131716 46915805352832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.131928 47038962365312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.132732 47763276129152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.133009 47992350552960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:16:17.135625 47438465577856 estimator.py:1111] Calling model_fn.
W0618 12:16:17.135735 47438465577856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:17.136016 47750633722752 estimator.py:1111] Calling model_fn.
W0618 12:16:17.136122 47750633722752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.137113 47438465577856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.137492 47750633722752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881776.940996 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.941851 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.942713 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.150533 47786671563648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.151562 47786671563648 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp7372copl
I0618 12:16:17.152571 47786671563648 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp7372copl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b767c2bbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.153193 47786671563648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.158245 47786671563648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.167946 47141050459008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.168451 47701244638080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.172258 47141050459008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.172742 47701244638080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881776.953182 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.953956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.954645 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.173625 47437607502720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.947412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.948256 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.949023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.173498 47669999616896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.947626 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.948495 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.949291 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.173834 47401054835584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.174664 47437607502720 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp56k12pwj
I0618 12:16:17.175656 47437607502720 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp56k12pwj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b253654cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:17.174639 47669999616896 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpxisltb_s
W0618 12:16:17.174938 47401054835584 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp4ajdo_2c
I0618 12:16:17.176068 47437607502720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.175743 47669999616896 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpxisltb_s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b51fb1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.176041 47401054835584 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmp4ajdo_2c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1cb39f6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:17.177299 47141050459008 estimator.py:1111] Calling model_fn.
W0618 12:16:17.177405 47141050459008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:17.176204 47669999616896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:17.177789 47701244638080 estimator.py:1111] Calling model_fn.
I0618 12:16:17.176498 47401054835584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:17.177898 47701244638080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.178754 47141050459008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.177996 47786671563648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:17.179255 47701244638080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.179149 46915805352832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.179197 47038962365312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:17.180751 47437607502720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.181679 47669999616896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.181715 47401054835584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:17.183471 46915805352832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:17.183506 47038962365312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:16:17.188532 46915805352832 estimator.py:1111] Calling model_fn.
I0618 12:16:17.188543 47038962365312 estimator.py:1111] Calling model_fn.
W0618 12:16:17.188644 46915805352832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.188656 47038962365312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:17.190010 46915805352832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:17.190012 47038962365312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881776.950938 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.951742 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.952548 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.190957 47497409975168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
:::MLL 1560881776.952727 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881776.953497 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881776.954208 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:17.191182 47495725220736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000030-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/000021-000010.tfrecord.zz_0_0
W0618 12:16:17.191995 47497409975168 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmpef7c0d7v
W0618 12:16:17.192177 47495725220736 estimator.py:1760] Using temporary folder as model directory: /tmp/96734.tmpdir/tmp03vzjo80
I0618 12:16:17.193041 47497409975168 estimator.py:201] Using config: {'_model_dir': '/tmp/96734.tmpdir/tmpef7c0d7v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps[2019-06-18 12:16:55] iteration time 30: 48.763 seconds
:::MLL 1560881815.382991 epoch_stop: {"value": null, "metadata": {'lineno': 737, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:16:55] Total time: 1701.205 seconds

numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000018-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000018-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000019-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000019-000011log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000020-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000020-000011log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000021-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000021-000012log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000022-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000022-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000023-000014_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000023-000014log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000024-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000024-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000025-000016_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000025-000016log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000026-000016_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000026-000016log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000027-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000027-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000028-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000028-000018log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000029-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000029-000018log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb162/models/000030-000019_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb162/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb162/models/000030-000019log.txt
:::MLL 1560881817.912662 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
I0618 12:16:57.913400 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=1
I0618 12:17:25.746650 46924693377920 utils.py:86] eval finished: 27.832 seconds
I0618 12:17:25.750042 46924693377920 reference_implementation.py:563] Win rate 000001-000001 vs target: 0.030
:::MLL 1560881845.750705 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560881845.751035 eval_accuracy: {"value": 0.03, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560881845.751349 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
I0618 12:17:25.751659 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=2
I0618 12:17:51.863216 46924693377920 utils.py:86] eval finished: 26.111 seconds
I0618 12:17:51.866074 46924693377920 reference_implementation.py:563] Win rate 000002-000002 vs target: 0.090
:::MLL 1560881871.879086 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560881871.879419 eval_accuracy: {"value": 0.09, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560881871.879729 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
I0618 12:17:51.880034 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=3
I0618 12:18:18.967144 46924693377920 utils.py:86] eval finished: 27.087 seconds
I0618 12:18:18.969988 46924693377920 reference_implementation.py:563] Win rate 000003-000003 vs target: 0.030
:::MLL 1560881898.970656 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560881898.971012 eval_accuracy: {"value": 0.03, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560881898.971334 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
I0618 12:18:18.971662 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=4
I0618 12:18:46.228952 46924693377920 utils.py:86] eval finished: 27.257 seconds
I0618 12:18:46.231817 46924693377920 reference_implementation.py:563] Win rate 000004-000003 vs target: 0.060
:::MLL 1560881926.232493 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560881926.232821 eval_accuracy: {"value": 0.06, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560881926.233139 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
I0618 12:18:46.233454 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=5
I0618 12:19:11.133570 46924693377920 utils.py:86] eval finished: 24.900 seconds
I0618 12:19:11.136562 46924693377920 reference_implementation.py:563] Win rate 000005-000004 vs target: 0.150
:::MLL 1560881951.141243 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560881951.141554 eval_accuracy: {"value": 0.15, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560881951.141863 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
I0618 12:19:11.142170 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=6
I0618 12:19:35.479706 46924693377920 utils.py:86] eval finished: 24.337 seconds
I0618 12:19:35.483097 46924693377920 reference_implementation.py:563] Win rate 000006-000005 vs target: 0.110
:::MLL 1560881975.483767 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560881975.484086 eval_accuracy: {"value": 0.11, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560881975.484395 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
I0618 12:19:35.484704 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=7
I0618 12:20:01.139401 46924693377920 utils.py:86] eval finished: 25.655 seconds
I0618 12:20:01.142268 46924693377920 reference_implementation.py:563] Win rate 000007-000005 vs target: 0.160
:::MLL 1560882001.143210 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882001.143541 eval_accuracy: {"value": 0.16, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882001.143897 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
I0618 12:20:01.144214 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=8
I0618 12:20:27.055091 46924693377920 utils.py:86] eval finished: 25.911 seconds
I0618 12:20:27.057913 46924693377920 reference_implementation.py:563] Win rate 000008-000006 vs target: 0.280
:::MLL 1560882027.058874 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882027.059196 eval_accuracy: {"value": 0.28, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882027.059502 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
I0618 12:20:27.059830 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=9
I0618 12:20:52.524875 46924693377920 utils.py:86] eval finished: 25.465 seconds
I0618 12:20:52.527772 46924693377920 reference_implementation.py:563] Win rate 000009-000007 vs target: 0.000
:::MLL 1560882052.528455 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882052.528782 eval_accuracy: {"value": 0.0, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882052.529105 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
I0618 12:20:52.529403 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=10
I0618 12:21:18.618881 46924693377920 utils.py:86] eval finished: 26.089 seconds
I0618 12:21:18.621734 46924693377920 reference_implementation.py:563] Win rate 000010-000007 vs target: 0.100
:::MLL 1560882078.622377 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882078.622679 eval_accuracy: {"value": 0.1, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882078.622982 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
I0618 12:21:18.623278 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000011-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=11
I0618 12:21:44.835011 46924693377920 utils.py:86] eval finished: 26.212 seconds
I0618 12:21:44.837857 46924693377920 reference_implementation.py:563] Win rate 000011-000007 vs target: 0.170
:::MLL 1560882104.838737 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882104.839063 eval_accuracy: {"value": 0.17, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882104.839376 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
I0618 12:21:44.839686 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000012-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=12
I0618 12:22:10.067705 46924693377920 utils.py:86] eval finished: 25.228 seconds
I0618 12:22:10.070500 46924693377920 reference_implementation.py:563] Win rate 000012-000007 vs target: 0.200
:::MLL 1560882130.071523 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882130.071839 eval_accuracy: {"value": 0.2, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882130.072137 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
I0618 12:22:10.072443 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000013-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=13
I0618 12:22:34.544013 46924693377920 utils.py:86] eval finished: 24.471 seconds
I0618 12:22:34.546835 46924693377920 reference_implementation.py:563] Win rate 000013-000008 vs target: 0.280
:::MLL 1560882154.547471 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882154.547782 eval_accuracy: {"value": 0.28, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882154.548080 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
I0618 12:22:34.548376 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000014-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=14
I0618 12:22:58.852693 46924693377920 utils.py:86] eval finished: 24.304 seconds
I0618 12:22:58.855694 46924693377920 reference_implementation.py:563] Win rate 000014-000009 vs target: 0.400
:::MLL 1560882178.856643 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882178.856973 eval_accuracy: {"value": 0.4, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882178.857280 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
I0618 12:22:58.857582 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000015-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=15
I0618 12:23:24.397553 46924693377920 utils.py:86] eval finished: 25.540 seconds
I0618 12:23:24.400437 46924693377920 reference_implementation.py:563] Win rate 000015-000009 vs target: 0.420
:::MLL 1560882204.401113 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882204.401433 eval_accuracy: {"value": 0.42, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882204.401748 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
I0618 12:23:24.402061 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000016-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=16
I0618 12:23:47.816895 46924693377920 utils.py:86] eval finished: 23.415 seconds
I0618 12:23:47.819771 46924693377920 reference_implementation.py:563] Win rate 000016-000010 vs target: 0.480
:::MLL 1560882227.820464 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882227.820800 eval_accuracy: {"value": 0.48, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882227.821122 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
I0618 12:23:47.821439 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000017-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=17
I0618 12:24:12.848704 46924693377920 utils.py:86] eval finished: 25.027 seconds
I0618 12:24:12.851564 46924693377920 reference_implementation.py:563] Win rate 000017-000010 vs target: 0.450
:::MLL 1560882252.852608 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882252.852947 eval_accuracy: {"value": 0.45, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882252.853243 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
I0618 12:24:12.853552 46924693377920 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb162/models/000018-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb162/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb162/sgf/eval/target --seed=18
I0618 12:24:38.010643 46924693377920 utils.py:86] eval finished: 25.157 seconds
I0618 12:24:38.013521 46924693377920 reference_implementation.py:563] Win rate 000018-000010 vs target: 0.550
:::MLL 1560882278.014186 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
:::MLL 1560882278.014505 eval_accuracy: {"value": 0.55, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
:::MLL 1560882278.014841 eval_result: {"value": null, "metadata": {'lineno': 52, 'file': 'ml_perf/eval_models.py', 'iteration': 17, 'timestamp': 892.016}}
:::MLL 1560882278.015140 run_stop: {"value": null, "metadata": {'lineno': 53, 'file': 'ml_perf/eval_models.py', 'status': 'success'}}
Model 000018-000010 beat target after 892.016s
~/submission/benchmarks/minigo/clx-8260l-2s-x32
