:::MLL 1560879840.690504678 submission_org: {"value": "Intel_Corp", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.691922439 submission_platform: {"value": "32xCLX-8260L_CPUs", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.693330095 submission_division: {"value": "closed", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.694571614 submission_status: {"value": "onprem", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.695870160 submission_benchmark: {"value": "minigo", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.697088991 submission_poc_name: {"value": "Guokai Ma, Letian Kang, Christine Cheng, Mingxiao Huang", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.698330950 submission_poc_email: {"value": "guokai.ma@intel.com, letian.kang@intel.com, christine.cheng@intel.com, mingxiao.huang@intel.com", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879840.699618403 submission_entry: {"value": {"framework": "TensorFlow 1.13.1", "power": "none", "notes": "none", "interconnect": "OPA", "os": "Oracle Linux Server 7.6", "libraries": "MKLDNN (v0.18), MKL (v2019.0.3.20190220), IntelMPI (2018.1.163)", "compilers": "GCC6.3", "nodes": [{"num_nodes": 32, "cpu": "Intel(R) Xeon(R) Platinum 8260L CPU @ 2.40GHz", "num_cores": 48, "num_vcpus": "NA", "accelerator": "NA", "num_accelerators": 0, "sys_mem_size": "192G", "sys_storage_type": "SSD", "sys_storage_size": "800G", "cpu_accel_interconnect": "100Gb OPA", "network_card": "100Gb OPA", "num_network_cards": 1, "notes": "NA"}]}, "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560879861.281881305 cache_clear: {value: true, metadata: {lineno: 0, file: manual}}
~/submission/benchmarks/minigo/implementations/tensorflow ~/submission/benchmarks/minigo/clx-8260l-2s-x32
Physical cores = 48
Virtual cores = 96
NUMA cores = 24
KMP_HW_SUBSET = 2T
Output to /lfs/lfs12/gma_akey
./run_mn.sh: line 20: ulimit: max user processes: cannot modify limit: Operation not permitted
Wiping dir /lfs/lfs12/gma_akey/results/epb134
:::MLL 1560879869.591512 init_start: {"value": null, "metadata": {'lineno': 742, 'file': 'ml_perf/reference_implementation.py'}}
Making dir /lfs/lfs12/gma_akey/results/epb134/models
Making dir /lfs/lfs12/gma_akey/results/epb134/data/selfplay
Making dir /lfs/lfs12/gma_akey/results/epb134/data/holdout
Making dir /lfs/lfs12/gma_akey/results/epb134/sgf/eval
Making dir /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks
Making dir /lfs/lfs12/gma_akey/results/epb134/work_dir
Making dir /lfs/lfs12/gma_akey/results/epb134/mpi
[2019-06-18 11:44:37] Selfplay nodes = ['epb134', 'epb171', 'epb133', 'epb132', 'epb173', 'epb131', 'epb174', 'epb130', 'epb175', 'epb219', 'epb176', 'epb218', 'epb217', 'epb178', 'epb215', 'epb214', 'epb200', 'epb179', 'epb201', 'epb213', 'epb202', 'epb203', 'epb211', 'epb204', 'epb205', 'epb206']
[2019-06-18 11:44:37] Train nodes = ['epb210', 'epb207', 'epb189', 'epb208', 'epb120', 'epb121']
[2019-06-18 11:44:37] Eval nodes = ['epb134', 'epb171', 'epb133', 'epb132', 'epb173', 'epb131', 'epb174', 'epb130', 'epb175', 'epb219', 'epb176', 'epb218', 'epb217', 'epb178', 'epb215', 'epb214', 'epb200', 'epb179', 'epb201', 'epb213', 'epb202', 'epb203', 'epb211', 'epb204', 'epb205', 'epb206']
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.49s/it]
[2019-06-18 11:47:30] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py:86: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
[2019-06-18 11:47:30] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
2019-06-18 11:47:30.764486: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-06-18 11:47:30.777241: I tensorflow/core/common_runtime/process_util.cc:113] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[2019-06-18 11:47:30] From ./quantize_graph.py:351: quantize_v2 (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.
Instructions for updating:
`tf.quantize_v2` is deprecated, please use `tf.quantization.quantize` instead.
2019-06-18 11:47:31.112250: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
[2019-06-18 11:47:31] From ./dual_net.py:679: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99']
Reading tf_records from 1 inputs
[2019-06-18 11:47:35] minmax time: 4.050 seconds
2019-06-18 11:47:35.173031: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:47:35.178645: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:47:35.183746: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880055.330519 init_stop: {"value": null, "metadata": {'lineno': 614, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880055.330893 run_start: {"value": null, "metadata": {'lineno': 615, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880055.331282 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:47:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:47:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=2 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=1023779833 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=2047559664 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=3071339495 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=4095119326 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=5118899157 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=6142678988 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=7166458819 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=8190238650 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=9214018481 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=10237798312 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=11261578143 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=12285357974 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=13309137805 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=14332917636 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=15356697467 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=16380477298 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=17404257129 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=18428036960 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000001-000000 --seed=19451816791 : \
-host epb2
[2019-06-18 11:48:11] selfplay finished: 35.627 seconds
[2019-06-18 11:48:11] selfplay mn: 35.649 seconds
[2019-06-18 11:48:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-2-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779833 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559664 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339495 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119326 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899157 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678988 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458819 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238650 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018481 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798312 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578143 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357974 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137805 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917636 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697467 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477298 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257129 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036960 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816791 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596622 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376453 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156284 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:48:37] divide_golden_chunk finished: 26.792 seconds
[2019-06-18 11:48:37] generate golden chunk: 26.809 seconds
[2019-06-18 11:48:40] train finished: 65.575 seconds
:::MLL 1560880080.523331 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.524243 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.524956 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.106081 47006198150016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.615104 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.615583 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.616008 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.106121 47229205992320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.615029 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.615500 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.615944 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.106093 47186851038080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.522844 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.523659 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.524495 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.106098 47712183214976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.110201 47712183214976 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk0375rmb
W0618 11:48:01.110446 47006198150016 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpep54zuff
I0618 11:48:01.111316 47712183214976 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk0375rmb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b652451ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.111567 47006198150016 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpep54zuff', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac0c4547e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.110903 47186851038080 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpr_46wmbw
W0618 11:48:01.110930 47229205992320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2d8lm9x7
I0618 11:48:01.111780 47712183214976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.111993 47229205992320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2d8lm9x7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af4b0a22e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.111983 47186851038080 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpr_46wmbw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aead414cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.112046 47006198150016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.112417 47229205992320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.112423 47186851038080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.120719 47229205992320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.120715 47186851038080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.120779 47006198150016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.120803 47712183214976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.142827 47186851038080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.143025 47229205992320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.143237 47006198150016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.143326 47712183214976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880080.631896 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.632322 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.632693 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.159876 47423742071680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.632310 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.632738 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.633062 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.159883 47352517604224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.556589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.557373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.558112 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.159888 47827272172416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.544782 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.545715 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.546566 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.159906 47764845032320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.161406 47423742071680 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz93jq2el
I0618 11:48:01.162539 47423742071680 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz93jq2el', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b21fbe32e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.161811 47352517604224 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6v55ghwn
I0618 11:48:01.162900 47352517604224 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6v55ghwn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b116693fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.162984 47423742071680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.162269 47827272172416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj5ey6y43
W0618 11:48:01.162327 47764845032320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpr229861u
I0618 11:48:01.163341 47352517604224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.163364 47827272172416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj5ey6y43', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ff027ddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.163413 47764845032320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpr229861u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7167354dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.163805 47827272172416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.163852 47764845032320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.171921 47352517604224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.171941 47423742071680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.171994 47764845032320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.172002 47827272172416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.193202 47423742071680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.193280 47352517604224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.194303 47827272172416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.194332 47764845032320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880080.653207 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.653674 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.654165 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.201073 47390892913536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.201258 47006198150016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880080.653597 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.654107 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.654492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.201071 47325560353664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.201305 47712183214976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880080.594106 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.594844 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.595461 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.201119 46932476253056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.589317 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.590215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.591076 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.201157 47303670272896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.202624 47325560353664 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp44vrvtdk
I0618 11:48:01.203701 47325560353664 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp44vrvtdk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b1fcd0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.204145 47325560353664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.205816 47229205992320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.205968 47006198150016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.206095 47712183214976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.205679 47390892913536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2lzjurp7
I0618 11:48:01.206737 47390892913536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2lzjurp7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1a55ecce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.206183 47303670272896 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkvac2uhq
W0618 11:48:01.206170 46932476253056 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmppy6866do
I0618 11:48:01.207173 47390892913536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.207238 47303670272896 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkvac2uhq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b06070cee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.207276 46932476253056 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmppy6866do', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaf9a29ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.207668 47303670272896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.207725 46932476253056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.210679 47229205992320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:01.211415 47006198150016 estimator.py:1111] Calling model_fn.
W0618 11:48:01.211531 47006198150016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:48:01.211638 47712183214976 estimator.py:1111] Calling model_fn.
W0618 11:48:01.211762 47712183214976 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.212761 47186851038080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.212912 46932476253056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.212925 47303670272896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.213054 47006198150016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:01.213010 47390892913536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.213038 47325560353664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.213306 47712183214976 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:48:01.216372 47229205992320 estimator.py:1111] Calling model_fn.
W0618 11:48:01.216494 47229205992320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.217638 47186851038080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.218073 47229205992320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:48:01.223432 47186851038080 estimator.py:1111] Calling model_fn.
W0618 11:48:01.223551 47186851038080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.225117 47186851038080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:01.234131 46932476253056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.234154 47303670272896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.235279 47325560353664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.235292 47390892913536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880080.699080 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.699596 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.700039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.245014 47410885432192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.705274 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.705684 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.706074 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.245052 47317018612608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.628934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.629728 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.630386 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.245087 47659335889792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.632266 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.632991 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.633691 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.245126 47751528719232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.246491 47410885432192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpympvtdgv
I0618 11:48:01.247563 47410885432192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpympvtdgv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1efd926e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.246840 47317018612608 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp77pw26oq
I0618 11:48:01.247913 47317018612608 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp77pw26oq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0922ac6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.247172 47751528719232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvzg1_nc4
W0618 11:48:01.247164 47659335889792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpx_oymzj4
I0618 11:48:01.248009 47410885432192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.248254 47751528719232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvzg1_nc4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6e4d7e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.248283 47659335889792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpx_oymzj4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b58d65f7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.248355 47317018612608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.248694 47751528719232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.248739 47659335889792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.252798 47352517604224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.252871 47423742071680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.252926 47764845032320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.253010 47827272172416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:01.256456 47751528719232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.256467 47659335889792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.256729 47410885432192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.256747 47317018612608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.257433 47352517604224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.257539 47423742071680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.257591 47764845032320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:01.257657 47827272172416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:01.262910 47352517604224 estimator.py:1111] Calling model_fn.
I0618 11:48:01.262996 47423742071680 estimator.py:1111] Calling model_fn.
W0618 11:48:01.263024 47352517604224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.263112 47423742071680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:48:01.263107 47764845032320 estimator.py:1111] Calling model_fn.
I0618 11:48:01.263154 47827272172416 estimator.py:1111] Calling model_fn.
W0618 11:48:01.263225 47764845032320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.263269 47827272172416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:01.264552 47352517604224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:01.264616 47423742071680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:01.264746 47764845032320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:01.264793 47827272172416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880080.718586 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.718944 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.719290 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.267913 47511604315008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.660658 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.661488 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.662337 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.267977 47808901403520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.719744 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.720115 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.720441 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.268031 47909750379392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880080.661334 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880080.662247 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880080.662949 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:01.268069 47762479727488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:48:01.268951 47511604315008 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj4trgubk
I0618 11:48:01.269708 47511604315008 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj4trgubk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3670e29e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.269168 47762479727488 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70da398cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.270071 47511604315008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.269415 47808901403520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqnpnlrov
I0618 11:48:01.270378 47762479727488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.270469 47808901403520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqnpnlrov', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ba92c2dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:01.269869 47909750379392 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0cq3c3ak
I0618 11:48:01.270891 47808901403520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:01.270978 47909750379392 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0cq3c3ak', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93243d6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:01.271423 47909750379392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:01.276468 47659335889792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.276536 47751528719232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.278454 47511604315008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.278619 47762479727488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.278636 47909750379392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.278649 47808901403520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:01.278978 47410885432192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.279030 47317018612608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:01.294433 46932476253056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated[2019-06-18 11:48:40] iteration time 0: 65.601 seconds
2019-06-18 11:48:41.312929: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880120.932937 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 11:48:44] minmax time: 3.257 seconds
2019-06-18 11:48:44.580312: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:48:44.585625: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:48:44.590480: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880124.602055 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 0}}
[2019-06-18 11:48:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:48:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=2 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=1023779833 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=2047559664 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=3071339495 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=4095119326 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=5118899157 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=6142678988 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=7166458819 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=8190238650 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=9214018481 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=10237798312 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=11261578143 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=12285357974 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=13309137805 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=14332917636 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=15356697467 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=16380477298 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=17404257129 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=18428036960 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=19451816791 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000001-000001 --seed=20475596622 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_
[2019-06-18 11:48:56] eval finished: 11.684 seconds
[2019-06-18 11:48:56] Win rate 000001-000001 vs checkpoint: 0.610
:::MLL 1560880136.364783 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:48:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=3 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=1023779834 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=2047559665 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=3071339496 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=4095119327 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=5118899158 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=6142678989 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=7166458820 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=8190238651 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=9214018482 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=10237798313 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=11261578144 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=12285357975 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=13309137806 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=14332917637 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=15356697468 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=16380477299 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=17404257130 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000002-000000 --seed=18428036961 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:49:25] selfplay finished: 28.844 seconds
[2019-06-18 11:49:25] selfplay mn: 28.862 seconds
[2019-06-18 11:49:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-3-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779834 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559665 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339496 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119327 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899158 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678989 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458820 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238651 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018482 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798313 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578144 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357975 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137806 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917637 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697468 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477299 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257130 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036961 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816792 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596623 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376454 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156285 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:49:28] train finished: 44.101 seconds
:::MLL 1560880129.904200 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.904964 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.905621 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.968048 47639728673664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880129.893271 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.894203 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.895039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.968145 47298898670464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:49.969076 47639728673664 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnwcxw4o0
W0618 11:48:49.969167 47298898670464 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_adpppej
I0618 11:48:49.970153 47639728673664 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnwcxw4o0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5445b12e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.970236 47298898670464 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_adpppej', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04eaa40da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.970568 47639728673664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:49.970645 47298898670464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880129.894494 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.895345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.896116 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.973083 47630617346944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880129.894251 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.895028 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.895804 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.973109 47929758602112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:49.974146 47929758602112 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzro1uz3c
W0618 11:48:49.974169 47630617346944 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpalh7ej67
I0618 11:48:49.975253 47929758602112 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzro1uz3c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b97ccd2ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.975253 47630617346944 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpalh7ej67', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b52269d4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.975667 47929758602112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:49.975670 47630617346944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:49.975957 47298898670464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880129.903170 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.904059 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.904815 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.975479 47638464119680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:49.975952 47639728673664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880129.902635 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.903469 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.904292 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.976169 47931858482048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:49.976465 47638464119680 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfokkjste
I0618 11:48:49.977509 47638464119680 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfokkjste', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53fa518e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.977928 47638464119680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:49.977277 47931858482048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpms90xwwe
I0618 11:48:49.978300 47931858482048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpms90xwwe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9849fc3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.978719 47931858482048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:49.980688 47630617346944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:49.980704 47929758602112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:49.983009 47638464119680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:49.983890 47931858482048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880129.975564 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.975990 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.976375 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:49.995853 47179798811520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:49.997810 47298898670464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:49.997907 47639728673664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:49.997226 47179798811520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_p06ypmk
I0618 11:48:49.998242 47179798811520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_p06ypmk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae92fbc6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:49.998666 47179798811520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.001339 47630617346944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.001526 47929758602112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.003128 47638464119680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880129.977803 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.978272 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.978681 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.003390 47449587086208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.003386 47179798811520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880129.975809 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.976265 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.976582 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.004646 47994704434048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880129.977357 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.977761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.978156 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.005190 46972784194432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.004779 47449587086208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkhgn9xg4
I0618 11:48:50.005847 47449587086208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkhgn9xg4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b28005ece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.006285 47449587086208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.005818 47931858482048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.005974 47994704434048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp117ge_1c
:::MLL 1560880129.983506 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.983888 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.984222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.006381 47891070604160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.006202 46972784194432 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6kos7ea7
I0618 11:48:50.007030 47994704434048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp117ge_1c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6ebe55e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.007230 46972784194432 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6kos7ea7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab8fcb40e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.007443 47994704434048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:50.007645 46972784194432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.007354 47891070604160 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfilqnrcl
I0618 11:48:50.008310 47891070604160 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfilqnrcl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8ecad6ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.008698 47891070604160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880129.982545 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.983035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.983437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.009815 47457942164352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.011182 47449587086208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.010743 47457942164352 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9wv0t_e9
I0618 11:48:50.011701 47457942164352 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9wv0t_e9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b29f25f0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.012201 47457942164352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.012305 47994704434048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.012543 46972784194432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.013174 47891070604160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.016929 47457942164352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.023010 47179798811520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.031060 47449587086208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.032070 47994704434048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.032328 46972784194432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.032738 47891070604160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.036717 47457942164352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880129.988151 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.988875 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.989596 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.045475 46969331389312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880129.980169 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880129.981110 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880129.981999 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.045501 47764559217536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.046733 47298898670464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.046735 47639728673664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.046515 46969331389312 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmps9w4hka6
W0618 11:48:50.046494 47764559217536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfus8gvq8
I0618 11:48:50.047558 47764559217536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfus8gvq8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b71562c0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.047570 46969331389312 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmps9w4hka6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab82ee66e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.047978 47764559217536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:50.047986 46969331389312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.048587 47630617346944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.049238 47929758602112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.050599 47638464119680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.051044 47298898670464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.051058 47639728673664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.052836 47630617346944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.053511 47929758602112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.053163 46969331389312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.053150 47764559217536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.054911 47638464119680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:50.056108 47298898670464 estimator.py:1111] Calling model_fn.
I0618 11:48:50.056121 47639728673664 estimator.py:1111] Calling model_fn.
W0618 11:48:50.056215 47298898670464 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.056230 47639728673664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.057574 47298898670464 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.057573 47639728673664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:48:50.057831 47630617346944 estimator.py:1111] Calling model_fn.
W0618 11:48:50.057938 47630617346944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.057766 47931858482048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:48:50.058554 47929758602112 estimator.py:1111] Calling model_fn.
W0618 11:48:50.058660 47929758602112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.059288 47630617346944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.060013 47929758602112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:48:50.059972 47638464119680 estimator.py:1111] Calling model_fn.
W0618 11:48:50.060075 47638464119680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.061433 47638464119680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.062462 47931858482048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:50.068188 47931858482048 estimator.py:1111] Calling model_fn.
W0618 11:48:50.068309 47931858482048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.069781 47931858482048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.070167 47179798811520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880130.002624 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880130.003481 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880130.004309 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.071550 47692432069504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880130.003697 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880130.004555 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880130.005262 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.071601 47898178364288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.072867 47764559217536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:48:50.072937 46969331389312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:48:50.072648 47692432069504 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b608b0f2d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:48:50.072651 47898178364288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpur4xxstg
I0618 11:48:50.073705 47898178364288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpur4xxstg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90727e7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:48:50.073864 47692432069504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:48:50.074139 47898178364288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:48:50.074455 47179798811520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.078254 47449587086208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.079390 47994704434048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:48:50.079483 47179798811520 estimator.py:1111] Calling model_fn.
W0618 11:48:50.079206 47692432069504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.079591 47179798811520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.079310 47898178364288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:48:50.079643 47891070604160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.080164 46972784194432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.080944 47179798811520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.082532 47449587086208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.083662 47994704434048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.083782 47457942164352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:48:50.083932 47891070604160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:48:50.084490 46972784194432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:50.087570 47449587086208 estimator.py:1111] Calling model_fn.
W0618 11:48:50.087679 47449587086208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.088079 47457942164352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:48:50.088704 47994704434048 estimator.py:1111] Calling model_fn.
W0618 11:48:50.088811 47994704434048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.089021 47449587086208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:48:50.088956 47891070604160 estimator.py:1111] Calling model_fn.
W0618 11:48:50.089065 47891070604160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:48:50.089596 46972784194432 estimator.py:1111] Calling model_fn.
W0618 11:48:50.089704 46972784194432 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:48:50.090177 47994704434048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880130.031407 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880130.032145 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880130.032879 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.089587 47305332544384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880130.020535 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880130.021454 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880130.022333 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:48:50.089593 47873006584704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 11:48:50.090416 47891070604160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.091063 46972784194432 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:48:50.090678 47305332544384 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpteh0izlw
W0618 11:48:50.090656 47873006584704 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp790xfnf0
I0618 11:48:50.091622 47873006584704 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp790xfnf0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': N[2019-06-18 11:49:28] divide_golden_chunk finished: 3.530 seconds
[2019-06-18 11:49:28] generate golden chunk: 3.545 seconds
[2019-06-18 11:49:28] moving /lfs/lfs12/gma_akey/results/epb134/models/000002-000001.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000002-000002.meta
[2019-06-18 11:49:28] moving /lfs/lfs12/gma_akey/results/epb134/models/000002-000001.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000002-000002.data-00000-of-00001
[2019-06-18 11:49:28] moving /lfs/lfs12/gma_akey/results/epb134/models/000002-000001.index --> /lfs/lfs12/gma_akey/results/epb134/models/000002-000002.index
[2019-06-18 11:49:28] moving /lfs/lfs12/gma_akey/results/epb134/models/000002-000001.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb
[2019-06-18 11:49:28] iteration time 1: 47.887 seconds
2019-06-18 11:49:29.212049: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880168.820176 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 11:49:32] minmax time: 3.232 seconds
2019-06-18 11:49:32.453380: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:49:32.458697: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:49:32.463404: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880172.473044 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 1}}
[2019-06-18 11:49:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:49:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=3 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=1023779834 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=2047559665 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=3071339496 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=4095119327 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=5118899158 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=6142678989 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=7166458820 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=8190238651 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=9214018482 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=10237798313 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=11261578144 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=12285357975 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=13309137806 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=14332917637 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=15356697468 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=16380477299 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=17404257130 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=18428036961 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=19451816792 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000002-000002 --seed=20475596623 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:49:43] eval finished: 10.755 seconds
[2019-06-18 11:49:43] Win rate 000002-000002 vs 000001-000001: 0.410
:::MLL 1560880183.301798 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 11:49:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=4 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=1023779835 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=2047559666 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=3071339497 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=4095119328 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=5118899159 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=6142678990 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=7166458821 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=8190238652 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=9214018483 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=10237798314 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=11261578145 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=12285357976 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=13309137807 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=14332917638 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=15356697469 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=16380477300 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=17404257131 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000003-000001 --seed=18428036962 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:50:12] selfplay finished: 29.059 seconds
[2019-06-18 11:50:12] selfplay mn: 29.076 seconds
[2019-06-18 11:50:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-4-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779835 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559666 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339497 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119328 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899159 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678990 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458821 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238652 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018483 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798314 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578145 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357976 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137807 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917638 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697469 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477300 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257131 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036962 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816793 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596624 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376455 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156286 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:50:15] divide_golden_chunk finished: 3.526 seconds
[2019-06-18 11:50:15] generate golden chunk: 3.540 seconds
[2019-06-18 11:50:17] train finished: 44.957 seconds
:::MLL 1560880177.700347 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.701055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.701728 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.777954 47660593640320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.702595 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.703308 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.704003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.778313 47272559465344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.778923 47660593640320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt_hubq6g
I0618 11:49:37.779905 47660593640320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt_hubq6g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5921574e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:49:37.779281 47272559465344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzqpf5dmb
:::MLL 1560880177.704921 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.705766 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.706551 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.780128 46966560314240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.705263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.706113 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.706877 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.780230 47209978045312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
I0618 11:49:37.780284 47272559465344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzqpf5dmb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afec8b3add8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.780311 47660593640320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.780683 47272559465344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.781121 46966560314240 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmporv5p3hl
W0618 11:49:37.781157 47209978045312 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6ygn70iw
I0618 11:49:37.782128 46966560314240 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmporv5p3hl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab789bb2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.782145 47209978045312 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6ygn70iw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af0368efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.782519 46966560314240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.782535 47209978045312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.785279 47660593640320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.785506 47272559465344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.787572 47209978045312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.787580 46966560314240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880177.727031 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.727927 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.728773 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.799249 46941169603456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.726891 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.727752 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.728605 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.799390 47136092943232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.800298 46941169603456 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp5mu_iqs2
W0618 11:49:37.800388 47136092943232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8retixx3
I0618 11:49:37.801332 46941169603456 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp5mu_iqs2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab1a053ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.801412 47136092943232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8retixx3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adf02a9ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.801728 46941169603456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.801810 47136092943232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.806617 46941169603456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.806651 47136092943232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.807632 46966560314240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.807713 47660593640320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.807986 47272559465344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.808829 47209978045312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880177.786659 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.787075 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.787413 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.821002 47094072222592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.785418 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.785848 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.786237 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.820749 47642759988096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.788943 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.789407 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.789799 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.822159 47964013101952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.789479 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.789927 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.790316 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.822070 48006926865280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.822062 47094072222592 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpem_8amwg
W0618 11:49:37.821856 47642759988096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqc7xwzp0
I0618 11:49:37.823028 47094072222592 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpem_8amwg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad53a087e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.822898 47642759988096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqc7xwzp0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54fa5f5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.823413 47094072222592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.823298 47642759988096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.823081 47964013101952 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp63ahai4h
I0618 11:49:37.824081 47964013101952 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp63ahai4h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9fc68ccda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:49:37.823093 48006926865280 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptz0au0cn
I0618 11:49:37.824071 48006926865280 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptz0au0cn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba9c468ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.824496 47964013101952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.824468 48006926865280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.826528 46941169603456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.826856 47136092943232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.828193 47094072222592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.828047 47642759988096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.829127 47964013101952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.829069 48006926865280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880177.802105 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.802572 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.802948 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.836411 47028912759680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.802680 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.803105 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.803462 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.836443 47166488810368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.837440 47028912759680 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp67j6yejk
W0618 11:49:37.837470 47166488810368 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2e03egja
I0618 11:49:37.838517 47028912759680 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp67j6yejk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac60e39ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.838576 47166488810368 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2e03egja', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae61665de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.838951 47028912759680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.839027 47166488810368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.843793 47028912759680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.843863 47166488810368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880177.782575 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.783303 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.783994 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.846486 47030795793280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.776966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.777909 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.778803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.846540 47898392957824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.847794 47094072222592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.847600 47642759988096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.848734 47964013101952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.848653 48006926865280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.847658 47898392957824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp10pcu79e
I0618 11:49:37.847732 47030795793280 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac67e76ccc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.848754 47898392957824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp10pcu79e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b907f48de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.848975 47030795793280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.849188 47898392957824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.854323 47030795793280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.854332 47898392957824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.855520 46966560314240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.856698 47209978045312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.857383 47660593640320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.857456 47272559465344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.859825 46966560314240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.861036 47209978045312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.861747 47272559465344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.861688 47660593640320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.863458 47028912759680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.863673 47166488810368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:49:37.864913 46966560314240 estimator.py:1111] Calling model_fn.
W0618 11:49:37.865021 46966560314240 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:49:37.866114 47209978045312 estimator.py:1111] Calling model_fn.
W0618 11:49:37.866225 47209978045312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:49:37.866372 46966560314240 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:49:37.866817 47272559465344 estimator.py:1111] Calling model_fn.
I0618 11:49:37.866775 47660593640320 estimator.py:1111] Calling model_fn.
W0618 11:49:37.866885 47660593640320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:49:37.866920 47272559465344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:49:37.867571 47209978045312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.868279 47272559465344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.868236 47660593640320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.874305 46941169603456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.874444 47136092943232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.874784 47030795793280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.874799 47898392957824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.878610 46941169603456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.878744 47136092943232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880177.814481 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.815367 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.816085 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.882753 47350557754240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.813747 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.814619 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.815475 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.882871 47728715387776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
I0618 11:49:37.883663 46941169603456 estimator.py:1111] Calling model_fn.
W0618 11:49:37.883774 46941169603456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:49:37.883817 47136092943232 estimator.py:1111] Calling model_fn.
W0618 11:49:37.883927 47136092943232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:49:37.883747 47350557754240 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf9bk_4iw
W0618 11:49:37.883835 47728715387776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptdq2h3h7
W0618 11:49:37.885119 46941169603456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:49:37.884731 47350557754240 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf9bk_4iw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b10f1c31e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:49:37.885282 47136092943232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:49:37.884819 47728715387776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptdq2h3h7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68fdb6ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.885125 47350557754240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.885211 47728715387776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.890172 47728715387776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.890178 47350557754240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:49:37.895325 47094072222592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.895049 47642759988096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.895598 48006926865280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.896126 47964013101952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.899620 47094072222592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.899352 47642759988096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.899875 48006926865280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:49:37.900427 47964013101952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:49:37.904703 47094072222592 estimator.py:1111] Calling model_fn.
W0618 11:49:37.904810 47094072222592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:49:37.904433 47642759988096 estimator.py:1111] Calling model_fn.
W0618 11:49:37.904538 47642759988096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:49:37.904921 48006926865280 estimator.py:1111] Calling model_fn.
W0618 11:49:37.905029 48006926865280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:49:37.905482 47964013101952 estimator.py:1111] Calling model_fn.
W0618 11:49:37.905587 47964013101952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880177.869559 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.869970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.870331 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.904631 47969544848256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880177.869631 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880177.870039 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880177.870403 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:49:37.904632 47002649781120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 11:49:37.906166 47094072222592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.905899 47642759988096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.906395 48006926865280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.906953 47964013101952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:49:37.905737 47002649781120 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmph3du7mea
W0618 11:49:37.905766 47969544848256 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi11hw7ha
I0618 11:49:37.906739 47002649781120 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmph3du7mea', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abff0d4be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.906780 47969544848256 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi11hw7ha', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba110449dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:49:37.907124 47002649781120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:49:37.907170 47969544848256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:49:37.910068 47350557754240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.910046 47728715387776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:49:37.910914 47028912759680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:49:37.911653 47166488810368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s ca[2019-06-18 11:50:17] iteration time 2: 48.631 seconds
2019-06-18 11:50:17.892426: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880217.450932 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 11:50:21] minmax time: 3.229 seconds
2019-06-18 11:50:21.131699: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:50:21.137339: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:50:21.142004: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880221.153656 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 2}}
[2019-06-18 11:50:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000004-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:50:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=4 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=1023779835 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=2047559666 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=3071339497 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=4095119328 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=5118899159 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=6142678990 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=7166458821 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=8190238652 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=9214018483 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=10237798314 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=11261578145 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=12285357976 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=13309137807 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=14332917638 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=15356697469 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=16380477300 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=17404257131 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=18428036962 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=19451816793 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000003-000002 --seed=20475596624 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:50:31] eval finished: 10.219 seconds
[2019-06-18 11:50:31] Win rate 000003-000002 vs 000001-000001: 0.670
:::MLL 1560880231.445582 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 11:50:31] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=5 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=1023779836 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=2047559667 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=3071339498 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=4095119329 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=5118899160 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=6142678991 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=7166458822 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=8190238653 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=9214018484 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=10237798315 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=11261578146 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=12285357977 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=13309137808 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=14332917639 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=15356697470 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=16380477301 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=17404257132 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000004-000001 --seed=18428036963 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:51:01] selfplay finished: 30.310 seconds
[2019-06-18 11:51:01] selfplay mn: 30.329 seconds
[2019-06-18 11:51:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-5-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779836 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559667 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339498 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119329 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899160 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678991 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458822 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238653 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018484 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798315 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578146 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357977 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137808 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917639 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697470 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477301 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257132 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036963 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816794 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596625 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376456 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156287 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000004-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:51:05] divide_golden_chunk finished: 3.326 seconds
[2019-06-18 11:51:05] generate golden chunk: 3.340 seconds
[2019-06-18 11:51:05] train finished: 44.188 seconds
:::MLL 1560880226.409076 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.409819 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.410487 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.475432 47625931424640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.405290 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.406181 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.406845 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.475485 47282228888448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.476474 47625931424640 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwb_djaac
W0618 11:50:26.476503 47282228888448 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfwpaecqd
I0618 11:50:26.477555 47625931424640 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwb_djaac', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b510f4fde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.477553 47282228888448 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfwpaecqd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b01090b4dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.477955 47625931424640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.477954 47282228888448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.483064 47282228888448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.483076 47625931424640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880226.415237 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.415956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.416656 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.487472 47185321272192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.417870 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.418602 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.419259 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.487464 47008806531968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.488554 47008806531968 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpunin4wgo
W0618 11:50:26.488582 47185321272192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzgmqfa4n
I0618 11:50:26.489653 47008806531968 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpunin4wgo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac15fcd4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.489661 47185321272192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzgmqfa4n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea78e67e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.490082 47185321272192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.490084 47008806531968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.495444 47185321272192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.495455 47008806531968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.505112 47282228888448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.505131 47625931424640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.515896 47008806531968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.516077 47185321272192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880226.486384 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.486761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.487079 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.522852 47815373513600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.485380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.485756 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.486101 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.522895 47443652977536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.523828 47815373513600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8yl1w_ql
W0618 11:50:26.523862 47443652977536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc9ykv5z_
I0618 11:50:26.524870 47815373513600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8yl1w_ql', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d2af0be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.524919 47443652977536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpc9ykv5z_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b269eab5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.525288 47815373513600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.525345 47443652977536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.529925 47815373513600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.529944 47443652977536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880226.498358 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.498791 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.499177 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.533867 47919650898816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.534936 47919650898816 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphpo01qs3
I0618 11:50:26.536020 47919650898816 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphpo01qs3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b95725b5da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880226.501786 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.502257 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.502671 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.535881 47143636235136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 11:50:26.536437 47919650898816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.536894 47143636235136 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyy5n_u6x
I0618 11:50:26.537892 47143636235136 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyy5n_u6x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae0c4474e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.538297 47143636235136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880226.471921 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.472670 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.473366 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.540904 47835222967168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.474372 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.475124 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.475829 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.540991 47275477308288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.541205 47919650898816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.542172 47835222967168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf0q2zlc4
W0618 11:50:26.542944 47143636235136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.542189 47275477308288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk8oy71u1
I0618 11:50:26.543320 47275477308288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk8oy71u1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff769e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.543323 47835222967168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf0q2zlc4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b81ca0f5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.543766 47275477308288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.543779 47835222967168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.549206 47275477308288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.549208 47835222967168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.549598 47443652977536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.549689 47815373513600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.555034 47282228888448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.555535 47625931424640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.559326 47282228888448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.559860 47625931424640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.560954 47919650898816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.562812 47143636235136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.563911 47008806531968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.564212 47185321272192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:50:26.564373 47282228888448 estimator.py:1111] Calling model_fn.
W0618 11:50:26.564483 47282228888448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:50:26.564919 47625931424640 estimator.py:1111] Calling model_fn.
W0618 11:50:26.565029 47625931424640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.565831 47282228888448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.566393 47625931424640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.568224 47008806531968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.568543 47185321272192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.570001 47835222967168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.570101 47275477308288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:50:26.573293 47008806531968 estimator.py:1111] Calling model_fn.
W0618 11:50:26.573407 47008806531968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:50:26.573598 47185321272192 estimator.py:1111] Calling model_fn.
W0618 11:50:26.573710 47185321272192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.574773 47008806531968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.575076 47185321272192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880226.522030 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.522880 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.523705 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.581328 47085920072576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.509954 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.510900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.511796 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.581363 47369203962752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.582419 47369203962752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfa6r1ozs
I0618 11:50:26.582434 47085920072576 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad354208d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.583440 47369203962752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfa6r1ozs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1549299e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.583561 47085920072576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.583839 47369203962752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.588682 47085920072576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.588785 47369203962752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880226.553656 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.554096 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.554448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.589548 47747694535552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.555178 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.555546 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.555895 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.589926 47254484738944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.590624 47747694535552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpheock0bi
I0618 11:50:26.591658 47747694535552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpheock0bi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d68f57e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:50:26.590949 47254484738944 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyarqajud
I0618 11:50:26.591964 47254484738944 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyarqajud', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa935d3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.592081 47747694535552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.592376 47254484738944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880226.527623 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.528532 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.529388 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.595987 47812590760832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880226.532213 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.532996 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.533717 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.596013 47958534435712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.596909 47747694535552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.597160 47254484738944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.597010 47815373513600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.597140 47443652977536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.597133 47958534435712 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvff0jw1y
W0618 11:50:26.597163 47812590760832 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwpppzucz
I0618 11:50:26.598235 47958534435712 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvff0jw1y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e7fff0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.598235 47812590760832 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwpppzucz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c85134e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.598664 47812590760832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:50:26.598671 47958534435712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.601314 47815373513600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.601447 47443652977536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.603868 47812590760832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.603894 47958534435712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:50:26.606345 47815373513600 estimator.py:1111] Calling model_fn.
W0618 11:50:26.606451 47815373513600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:50:26.606497 47443652977536 estimator.py:1111] Calling model_fn.
W0618 11:50:26.606605 47443652977536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.607802 47815373513600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.608171 47919650898816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.607950 47443652977536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.609881 47143636235136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.610532 47369203962752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.610936 47085920072576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.612445 47919650898816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.614148 47143636235136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:50:26.617456 47919650898816 estimator.py:1111] Calling model_fn.
W0618 11:50:26.617565 47919650898816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.617790 47835222967168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.617936 47747694535552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.618129 47254484738944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.618373 47275477308288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:50:26.618919 47919650898816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:50:26.619158 47143636235136 estimator.py:1111] Calling model_fn.
W0618 11:50:26.619266 47143636235136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.620618 47143636235136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.622064 47835222967168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.622690 47275477308288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:50:26.624077 47812590760832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:50:26.624386 47958534435712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:50:26.627050 47835222967168 estimator.py:1111] Calling model_fn.
W0618 11:50:26.627159 47835222967168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880226.592396 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.592850 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.593242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.626576 47813244081024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 11:50:26.627776 47275477308288 estimator.py:1111] Calling model_fn.
W0618 11:50:26.627887 47275477308288 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:50:26.628483 47835222967168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:50:26.627588 47813244081024 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpg6ksg5da
W0618 11:50:26.629239 47275477308288 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:50:26.628596 47813244081024 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpg6ksg5da', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7cac042e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.629009 47813244081024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880226.598417 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.598858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.599209 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.630739 47502289458048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 11:50:26.631689 47502289458048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpy6v9g27u
I0618 11:50:26.632635 47502289458048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpy6v9g27u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3445ad2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:50:26.633027 47502289458048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:50:26.633814 47813244081024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:50:26.637454 47502289458048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880226.570424 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880226.571374 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880226.572235 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:50:26.638964 47536938566528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfre[2019-06-18 11:51:05] moving /lfs/lfs12/gma_akey/results/epb134/models/000004-000002.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb
[2019-06-18 11:51:05] moving /lfs/lfs12/gma_akey/results/epb134/models/000004-000002.index --> /lfs/lfs12/gma_akey/results/epb134/models/000004-000003.index
[2019-06-18 11:51:05] moving /lfs/lfs12/gma_akey/results/epb134/models/000004-000002.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000004-000003.meta
[2019-06-18 11:51:05] moving /lfs/lfs12/gma_akey/results/epb134/models/000004-000002.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000004-000003.data-00000-of-00001
[2019-06-18 11:51:05] iteration time 3: 47.958 seconds
2019-06-18 11:51:05.887626: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880265.408771 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 11:51:09] minmax time: 3.271 seconds
2019-06-18 11:51:09.168285: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:51:09.173815: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:51:09.178246: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880269.188325 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 3}}
[2019-06-18 11:51:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:51:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=5 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=1023779836 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=2047559667 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=3071339498 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=4095119329 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=5118899160 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=6142678991 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=7166458822 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=8190238653 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=9214018484 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=10237798315 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=11261578146 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=12285357977 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=13309137808 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=14332917639 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=15356697470 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=16380477301 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=17404257132 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=18428036963 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=19451816794 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000004-000003 --seed=20475596625 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:51:20] eval finished: 11.396 seconds
[2019-06-18 11:51:20] Win rate 000004-000003 vs 000003-000002: 0.530
:::MLL 1560880280.658735 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 11:51:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=6 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=1023779837 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=2047559668 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=3071339499 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=4095119330 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=5118899161 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=6142678992 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=7166458823 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=8190238654 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=9214018485 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=10237798316 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=11261578147 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=12285357978 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=13309137809 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=14332917640 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=15356697471 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=16380477302 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=17404257133 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000005-000002 --seed=18428036964 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:51:51] selfplay finished: 30.534 seconds
[2019-06-18 11:51:51] selfplay mn: 30.551 seconds
[2019-06-18 11:51:51] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-6-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779837 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559668 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339499 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119330 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899161 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678992 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458823 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238654 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018485 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798316 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578147 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357978 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137809 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917640 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697471 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477302 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257133 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036964 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816795 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596626 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376457 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156288 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:51:53] train finished: 43.928 seconds
:::MLL 1560880274.398279 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.399151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.400012 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.467530 47590653723520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.419136 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.419933 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.420650 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.467832 47763803022208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.468579 47590653723520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpm_giwl9u
W0618 11:51:14.468850 47763803022208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp5lt5w51o
I0618 11:51:14.469586 47590653723520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpm_giwl9u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b48d898de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.469848 47763803022208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp5lt5w51o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7129197dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.469977 47590653723520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.470247 47763803022208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.475160 47590653723520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.475397 47763803022208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.494795 47590653723520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.495244 47763803022208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880274.453219 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.453945 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.454614 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.518552 47703411061632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.446686 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.447561 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.448442 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.518507 47990030480256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.519513 47990030480256 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl96hq5lc
W0618 11:51:14.519540 47703411061632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbmsw5gdr
I0618 11:51:14.520578 47990030480256 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl96hq5lc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5d54e8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.520604 47703411061632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbmsw5gdr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6319754e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.521005 47990030480256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.521026 47703411061632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.526182 47990030480256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.526179 47703411061632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880274.497098 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.497479 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.497804 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.537401 47783869137792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.496081 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.496465 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.496808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.537686 47374344438656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.538399 47783869137792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpoc2qaaw3
W0618 11:51:14.538616 47374344438656 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmsqi79f1
I0618 11:51:14.539396 47783869137792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpoc2qaaw3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75d5221e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.539646 47374344438656 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmsqi79f1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b167b8f1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.539815 47783869137792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.540060 47374344438656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.544580 47783869137792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.544794 47374344438656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.545907 47703411061632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.546380 47990030480256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.548011 47590653723520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880274.482513 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.483281 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.483956 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.549515 47835300631424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.549345 47763803022208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880274.479091 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.479905 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.480577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.549554 47908562604928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.550572 47835300631424 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp31iiuzyi
W0618 11:51:14.550603 47908562604928 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpot7r6kel
I0618 11:51:14.551643 47835300631424 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp31iiuzyi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b81ceb05e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.551653 47908562604928 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpot7r6kel', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b92dd716e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.552063 47835300631424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.552098 47908562604928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.552710 47590653723520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.554039 47763803022208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.557193 47835300631424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.557186 47908562604928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:51:14.558177 47590653723520 estimator.py:1111] Calling model_fn.
W0618 11:51:14.558293 47590653723520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:51:14.559560 47763803022208 estimator.py:1111] Calling model_fn.
W0618 11:51:14.559675 47763803022208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.559747 47590653723520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.561161 47763803022208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.564035 47783869137792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.564719 47374344438656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880274.530621 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.531021 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.531364 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.569516 47856260846464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.530158 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.530564 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.530918 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.569563 47036438266752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.570551 47856260846464 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1tby0sso
W0618 11:51:14.570583 47036438266752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4hs1alfh
I0618 11:51:14.571567 47856260846464 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1tby0sso', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b86b003eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.571613 47036438266752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4hs1alfh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7cec80e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.572056 47856260846464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.572077 47036438266752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.576833 47036438266752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.576835 47856260846464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.577429 47908562604928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.577538 47835300631424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880274.554559 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.555082 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.555461 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.593620 47751779685248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.555386 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.555809 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.556163 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.593725 47424584352640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.593907 47990030480256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.594060 47703411061632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.594673 47751779685248 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphc5a5plo
W0618 11:51:14.594732 47424584352640 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpef0la6_j
I0618 11:51:14.595705 47751779685248 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphc5a5plo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6e5c73ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.595767 47424584352640 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpef0la6_j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b222e175e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.596124 47751779685248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.596185 47424584352640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.596388 47036438266752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.596497 47856260846464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.598164 47990030480256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.598345 47703411061632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.601094 47751779685248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.601109 47424584352640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880274.539694 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.540462 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.541161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.601507 46945854382976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.531314 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.532221 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.533119 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.601553 46922932716416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
I0618 11:51:14.603182 47990030480256 estimator.py:1111] Calling model_fn.
W0618 11:51:14.603290 47990030480256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:51:14.603384 47703411061632 estimator.py:1111] Calling model_fn.
W0618 11:51:14.603491 47703411061632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.602542 46945854382976 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbckanaf1
W0618 11:51:14.602570 46922932716416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1lj5_9eg
I0618 11:51:14.603613 46945854382976 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbckanaf1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2b78fae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.603609 46922932716416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1lj5_9eg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad6152de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.604009 46945854382976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:51:14.604016 46922932716416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.604625 47990030480256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.604842 47703411061632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.609241 46945854382976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.609258 46922932716416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.611447 47783869137792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.612356 47374344438656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880274.539367 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.540277 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.541149 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.612943 47011378185088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880274.544147 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.544895 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.545623 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.612967 46998156030848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
I0618 11:51:14.614024 47011378185088 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac1f9159d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:51:14.614006 46998156030848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz91kwtco
I0618 11:51:14.615028 46998156030848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz91kwtco', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abee4fb8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.615214 47011378185088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.615716 47783869137792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:51:14.615450 46998156030848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:51:14.616712 47374344438656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:51:14.620716 47783869137792 estimator.py:1111] Calling model_fn.
W0618 11:51:14.620823 47783869137792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.620770 47011378185088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.620777 46998156030848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:51:14.622009 47424584352640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.622131 47751779685248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:51:14.621819 47374344438656 estimator.py:1111] Calling model_fn.
W0618 11:51:14.621925 47374344438656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.622161 47783869137792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.623310 47374344438656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.625719 47908562604928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.625904 47835300631424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.628861 46945854382976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.628909 46922932716416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.630055 47908562604928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.630234 47835300631424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:51:14.635152 47908562604928 estimator.py:1111] Calling model_fn.
W0618 11:51:14.635262 47908562604928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:51:14.635302 47835300631424 estimator.py:1111] Calling model_fn.
W0618 11:51:14.635410 47835300631424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.636650 47908562604928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.636760 47835300631424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.643880 47856260846464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.643886 47036438266752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:51:14.643331 46998156030848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.643879 47011378185088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:51:14.648154 47856260846464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:51:14.648170 47036438266752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880274.613450 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.613899 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.614289 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.650519 47418657321856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.651545 47418657321856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa9dunz9e
I0618 11:51:14.653190 47856260846464 estimator.py:1111] Calling model_fn.
I0618 11:51:14.652585 47418657321856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa9dunz9e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20ccd00e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.653202 47036438266752 estimator.py:1111] Calling model_fn.
W0618 11:51:14.653296 47856260846464 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:51:14.653306 47036438266752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880274.614861 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.615304 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.615683 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.653138 47747145081728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
I0618 11:51:14.652996 47418657321856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880274.615101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.615571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.615907 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.653349 47874001343360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.654650 47856260846464 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:51:14.654659 47036438266752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880274.618958 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880274.619446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880274.619871 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:51:14.654104 47406717764480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 11:51:14.654129 47747145081728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxd6agbd0
W0618 11:51:14.654277 47874001343360 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpum08sg4a
I0618 11:51:14.655097 47747145081728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxd6agbd0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d48356e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:51:14.655232 47874001343360 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpum08sg4a', '_tf_random_seed': None[2019-06-18 11:51:54] divide_golden_chunk finished: 3.536 seconds
[2019-06-18 11:51:54] generate golden chunk: 3.550 seconds
[2019-06-18 11:51:54] moving /lfs/lfs12/gma_akey/results/epb134/models/000005-000003.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000005-000004.meta
[2019-06-18 11:51:54] moving /lfs/lfs12/gma_akey/results/epb134/models/000005-000003.index --> /lfs/lfs12/gma_akey/results/epb134/models/000005-000004.index
[2019-06-18 11:51:54] moving /lfs/lfs12/gma_akey/results/epb134/models/000005-000003.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000005-000004.data-00000-of-00001
[2019-06-18 11:51:54] moving /lfs/lfs12/gma_akey/results/epb134/models/000005-000003.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb
[2019-06-18 11:51:54] iteration time 4: 49.393 seconds
2019-06-18 11:51:55.328760: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880314.801656 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 11:51:58] minmax time: 3.230 seconds
2019-06-18 11:51:58.569028: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:51:58.574465: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:51:58.579000: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880318.588952 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 4}}
[2019-06-18 11:51:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:51:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=6 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=1023779837 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=2047559668 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=3071339499 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=4095119330 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=5118899161 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=6142678992 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=7166458823 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=8190238654 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=9214018485 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=10237798316 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=11261578147 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=12285357978 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=13309137809 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=14332917640 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=15356697471 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=16380477302 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=17404257133 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=18428036964 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=19451816795 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000005-000004 --seed=20475596626 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:52:09] eval finished: 10.579 seconds
[2019-06-18 11:52:09] Win rate 000005-000004 vs 000004-000003: 0.540
:::MLL 1560880329.244041 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 11:52:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=7 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=1023779838 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=2047559669 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=3071339500 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=4095119331 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=5118899162 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=6142678993 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=7166458824 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=8190238655 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=9214018486 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=10237798317 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=11261578148 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=12285357979 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=13309137810 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=14332917641 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=15356697472 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=16380477303 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=17404257134 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000006-000003 --seed=18428036965 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:52:39] selfplay finished: 29.941 seconds
[2019-06-18 11:52:39] selfplay mn: 29.958 seconds
[2019-06-18 11:52:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-7-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779838 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559669 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339500 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119331 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899162 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678993 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458824 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238655 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018486 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798317 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578148 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357979 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137810 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917641 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697472 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477303 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257134 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036965 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816796 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596627 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376458 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156289 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:52:42] train finished: 43.707 seconds
:::MLL 1560880323.839380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.840207 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.840937 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.901128 47493353431936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.828365 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.829242 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.830063 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.901287 46983192982400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.902118 47493353431936 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpln187uqc
W0618 11:52:03.902251 46983192982400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvc2w75_o
I0618 11:52:03.903125 47493353431936 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpln187uqc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b32310c3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.903218 46983192982400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvc2w75_o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb691d8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.903525 47493353431936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:03.903612 46983192982400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:03.908481 47493353431936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.908598 46983192982400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.931133 46983192982400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:03.931793 47493353431936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880323.871883 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.872767 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.873491 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.945113 47214604845952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.876058 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.876785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.877487 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.945194 47211495207808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.946112 47214604845952 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpw0vkygzh
W0618 11:52:03.946162 47211495207808 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl4goq4rm
I0618 11:52:03.947111 47214604845952 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpw0vkygzh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af14a565e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.947140 47211495207808 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl4goq4rm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af090fd0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.947513 47214604845952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:03.947529 47211495207808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880323.905905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.906470 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.906846 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.949218 47329407738752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.950212 47329407738752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfczvml86
:::MLL 1560880323.909519 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.909946 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.910318 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.950883 47468399276928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
I0618 11:52:03.951187 47329407738752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfczvml86', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0c051f7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.951578 47329407738752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:03.952577 47211495207808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.952584 47214604845952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.951848 47468399276928 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq1q2llrr
I0618 11:52:03.952806 47468399276928 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq1q2llrr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c61aa0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.953197 47468399276928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:03.956253 47329407738752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.957786 47468399276928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880323.896056 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.896761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.897477 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.965081 47950753055616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.893768 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.894522 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.895262 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.965132 47119080936320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.966097 47950753055616 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmrhb99du
W0618 11:52:03.966124 47119080936320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqn7tc3pe
I0618 11:52:03.967138 47950753055616 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmrhb99du', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9cb0308e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.967140 47119080936320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqn7tc3pe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adb0cab2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.967547 47950753055616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:03.967547 47119080936320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:03.972753 47950753055616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.972752 47119080936320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:03.973047 47214604845952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:03.973234 47211495207808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:03.975903 47329407738752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:03.977184 47468399276928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:03.983352 46983192982400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:03.983554 47493353431936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:03.987900 46983192982400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:03.988075 47493353431936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:03.992876 47119080936320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880323.951265 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.951736 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.952151 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.992977 46989010387840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.993238 47950753055616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:52:03.993100 46983192982400 estimator.py:1111] Calling model_fn.
W0618 11:52:03.993226 46983192982400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:03.993256 47493353431936 estimator.py:1111] Calling model_fn.
W0618 11:52:03.993370 47493353431936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:03.994057 46989010387840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnyg8sari
W0618 11:52:03.994705 46983192982400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:03.995116 46989010387840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnyg8sari', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abcc3dc1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:03.994841 47493353431936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:03.995558 46989010387840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880323.957244 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.957703 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.958101 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:03.997102 47811240489856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:03.998049 47811240489856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpox0ke0o0
I0618 11:52:03.998996 47811240489856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpox0ke0o0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c3497be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:03.999389 47811240489856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:04.000316 46989010387840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.003904 47811240489856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.019940 46989010387840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:04.021356 47214604845952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880323.974987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.975354 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.975672 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.021463 48011574911872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.973043 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.973420 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.973743 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.021564 47604274422656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:04.021907 47211495207808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.022466 48011574911872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2xpddu83
W0618 11:52:04.022502 47604274422656 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl9mt757i
W0618 11:52:04.023234 47811240489856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:52:04.023448 48011574911872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2xpddu83', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baad9747e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.023454 47604274422656 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl9mt757i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4c04743e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.023887 48011574911872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:04.023888 47604274422656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:04.023539 47329407738752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.024351 47468399276928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.025627 47214604845952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:04.026207 47211495207808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:04.027845 47329407738752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:04.028712 48011574911872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.028707 47604274422656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.028648 47468399276928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:52:04.030667 47214604845952 estimator.py:1111] Calling model_fn.
W0618 11:52:04.030776 47214604845952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:04.031283 47211495207808 estimator.py:1111] Calling model_fn.
W0618 11:52:04.031389 47211495207808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.032119 47214604845952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:04.032749 47211495207808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:04.032941 47329407738752 estimator.py:1111] Calling model_fn.
W0618 11:52:04.033046 47329407738752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:04.033680 47468399276928 estimator.py:1111] Calling model_fn.
W0618 11:52:04.033788 47468399276928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.034416 47329407738752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:04.035139 47468399276928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:04.041227 47119080936320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.041979 47950753055616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880323.976119 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.976848 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.977553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.043085 47374460380032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.970948 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.971898 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.972768 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.043048 47175353127808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:04.044079 47374460380032 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzb6pwdbc
W0618 11:52:04.044052 47175353127808 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp07mwghda
I0618 11:52:04.045043 47175353127808 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp07mwghda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae826c0ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.045077 47374460380032 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzb6pwdbc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1682783e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:04.045552 47119080936320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:52:04.045442 47175353127808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:04.045481 47374460380032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:04.046287 47950753055616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:04.048382 47604274422656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:04.048545 48011574911872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:52:04.050642 47119080936320 estimator.py:1111] Calling model_fn.
W0618 11:52:04.050758 47119080936320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.050439 47374460380032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.050487 47175353127808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:52:04.051331 47950753055616 estimator.py:1111] Calling model_fn.
W0618 11:52:04.051437 47950753055616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.052125 47119080936320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:04.052798 47950753055616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880323.982952 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.983714 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.984420 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.056191 47244054233984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880323.980944 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.981735 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880323.982462 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.056336 47163931448192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:04.057291 47244054233984 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcf5a_opd
I0618 11:52:04.057426 47163931448192 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae57df79cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.058367 47244054233984 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcf5a_opd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af825a85e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.058679 47163931448192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:04.058817 47244054233984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:04.064067 47163931448192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.064117 47244054233984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.067364 46989010387840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.070034 47811240489856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:04.070426 47374460380032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:04.070590 47175353127808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:04.071666 46989010387840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880323.999209 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880323.999927 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880324.000633 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.072056 46982680417152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880324.001670 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880324.002437 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880324.003087 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.072329 47968744141696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:04.072996 46982680417152 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpusijpivl
W0618 11:52:04.074308 47811240489856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:52:04.073962 46982680417152 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpusijpivl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb4a905e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:04.073288 47968744141696 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp17zmvs6n
I0618 11:52:04.074279 47968744141696 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp17zmvs6n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0e08acdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:04.074354 46982680417152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:04.074674 47968744141696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:04.076733 46989010387840 estimator.py:1111] Calling model_fn.
W0618 11:52:04.076845 46989010387840 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.078208 46989010387840 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:04.079315 47811240489856 estimator.py:1111] Calling model_fn.
W0618 11:52:04.079421 47811240489856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:04.079288 46982680417152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.079571 47968744141696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:04.080756 47811240489856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880324.040285 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880324.040866 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880324.041303 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.085084 47649332315008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560880324.045847 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880324.046329 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880324.046697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:04.085977 47701193507712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 11:52:04.086445 47163931448192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be remov[2019-06-18 11:52:42] divide_golden_chunk finished: 3.405 seconds
[2019-06-18 11:52:42] generate golden chunk: 3.419 seconds
[2019-06-18 11:52:42] moving /lfs/lfs12/gma_akey/results/epb134/models/000006-000004.index --> /lfs/lfs12/gma_akey/results/epb134/models/000006-000005.index
[2019-06-18 11:52:42] moving /lfs/lfs12/gma_akey/results/epb134/models/000006-000004.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000006-000005.data-00000-of-00001
[2019-06-18 11:52:42] moving /lfs/lfs12/gma_akey/results/epb134/models/000006-000004.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb
[2019-06-18 11:52:42] moving /lfs/lfs12/gma_akey/results/epb134/models/000006-000004.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000006-000005.meta
[2019-06-18 11:52:42] iteration time 5: 47.868 seconds
2019-06-18 11:52:43.353150: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880362.670124 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 11:52:46] minmax time: 3.271 seconds
2019-06-18 11:52:46.634973: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:52:46.640390: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:52:46.644920: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880366.654683 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 5}}
[2019-06-18 11:52:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:52:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=7 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=1023779838 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=2047559669 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=3071339500 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=4095119331 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=5118899162 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=6142678993 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=7166458824 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=8190238655 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=9214018486 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=10237798317 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=11261578148 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=12285357979 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=13309137810 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=14332917641 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=15356697472 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=16380477303 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=17404257134 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=18428036965 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=19451816796 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000006-000005 --seed=20475596627 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:52:57] eval finished: 10.968 seconds
[2019-06-18 11:52:57] Win rate 000006-000005 vs 000005-000004: 0.470
:::MLL 1560880377.695571 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 11:52:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=8 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=1023779839 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=2047559670 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=3071339501 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=4095119332 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=5118899163 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=6142678994 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=7166458825 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=8190238656 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=9214018487 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=10237798318 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=11261578149 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=12285357980 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=13309137811 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=14332917642 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=15356697473 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=16380477304 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=17404257135 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000007-000004 --seed=18428036966 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:53:26] selfplay finished: 28.750 seconds
[2019-06-18 11:53:26] selfplay mn: 28.770 seconds
[2019-06-18 11:53:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-8-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779839 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559670 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339501 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119332 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899163 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678994 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458825 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238656 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018487 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798318 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578149 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357980 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137811 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917642 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697473 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477304 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257135 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036966 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816797 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596628 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376459 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156290 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000007-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:53:29] divide_golden_chunk finished: 3.258 seconds
[2019-06-18 11:53:29] generate golden chunk: 3.276 seconds
[2019-06-18 11:53:30] train finished: 43.918 seconds
:::MLL 1560880371.904542 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.905278 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.906010 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:51.968506 47610403709824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880371.894021 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.894908 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.895755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:51.968529 47001549075328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:51.969559 47001549075328 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkyfg3s4v
W0618 11:52:51.969589 47610403709824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcmm6nxfp
I0618 11:52:51.970546 47001549075328 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkyfg3s4v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abfaf393e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:51.970608 47610403709824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcmm6nxfp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d71c9be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:51.970945 47001549075328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:51.971016 47610403709824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:51.976206 47001549075328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:51.976239 47610403709824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880371.914263 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.915110 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.915863 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:51.989931 47379038749568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880371.915389 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.916186 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.916864 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:51.989915 47688745558912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:51.991008 47379038749568 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe2kc2wj2
W0618 11:52:51.990982 47688745558912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpftnl307c
I0618 11:52:51.991991 47688745558912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpftnl307c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5faf536e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:51.992001 47379038749568 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe2kc2wj2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b17935c9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:51.992396 47379038749568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:51.992396 47688745558912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:51.997113 47001549075328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:51.997646 47688745558912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:51.997650 47379038749568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:51.997618 47610403709824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880371.934435 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.935346 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.936097 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.009310 47982690743168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880371.938214 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.938920 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.939571 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.009308 46951030682496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.010316 47982690743168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj4ok7wo2
W0618 11:52:52.010286 46951030682496 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpy544jaj1
I0618 11:52:52.011252 46951030682496 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpy544jaj1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3ec17be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.011277 47982690743168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj4ok7wo2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba41fd30e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.011644 46951030682496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:52.011665 47982690743168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.016422 47982690743168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.016411 46951030682496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.017415 47688745558912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.017619 47379038749568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880371.985168 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.985608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.985979 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.029316 47464667927424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.030310 47464667927424 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphajbxlns
I0618 11:52:52.031272 47464667927424 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphajbxlns', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2b83421e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.031661 47464667927424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880371.990412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880371.990864 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880371.991265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.031851 47911530619776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.032834 47911530619776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7nyjy7tm
I0618 11:52:52.033815 47911530619776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7nyjy7tm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b938e59ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.034242 47911530619776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.036217 46951030682496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.036268 47982690743168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.036397 47464667927424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.038868 47911530619776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.049978 47001549075328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.050838 47610403709824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.054276 47001549075328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.055223 47610403709824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.055959 47464667927424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.058530 47911530619776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:52:52.059325 47001549075328 estimator.py:1111] Calling model_fn.
W0618 11:52:52.059433 47001549075328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:52.060343 47610403709824 estimator.py:1111] Calling model_fn.
W0618 11:52:52.060454 47610403709824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880372.012655 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.013077 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.013461 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.060911 47791486862208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.060803 47001549075328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880372.011700 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.012246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.012637 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.060962 47382105961344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.012723 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.013172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.013494 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.061322 47676703556480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.012380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.012803 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.013196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.061532 47621135049600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.061829 47610403709824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:52.061902 47791486862208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpu25owr7r
W0618 11:52:52.061929 47382105961344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_k6afbjc
I0618 11:52:52.062855 47791486862208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpu25owr7r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b779b2f5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.062883 47382105961344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_k6afbjc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b184a2e8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:52.062304 47676703556480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmph4r9yn7f
I0618 11:52:52.063250 47791486862208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:52.063271 47382105961344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:52.063296 47676703556480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmph4r9yn7f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ce1911e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:52.062503 47621135049600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8cdsjxre
I0618 11:52:52.063576 47621135049600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8cdsjxre', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4ff16cee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.063691 47676703556480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:52.063977 47621135049600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.065730 47688745558912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.067356 47379038749568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.067996 47382105961344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.068006 47791486862208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.068423 47676703556480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.068516 47621135049600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.070049 47688745558912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.071711 47379038749568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:52:52.075111 47688745558912 estimator.py:1111] Calling model_fn.
W0618 11:52:52.075222 47688745558912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.076570 47688745558912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:52.076830 47379038749568 estimator.py:1111] Calling model_fn.
W0618 11:52:52.076943 47379038749568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.078305 47379038749568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880372.008946 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.009922 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.010782 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.082385 47471995409280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.083860 46951030682496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880372.025061 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.025879 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.026631 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.083515 47166109713280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.084131 47982690743168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.083422 47471995409280 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0aqjgte0
I0618 11:52:52.084472 47471995409280 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0aqjgte0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2d3802ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.084900 47471995409280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.084716 47166109713280 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphdzr35mq
I0618 11:52:52.085933 47166109713280 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphdzr35mq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5ffcd5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.086333 47166109713280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.087595 47382105961344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.087650 47791486862208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.087948 47621135049600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.088034 47676703556480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.088160 46951030682496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.088436 47982690743168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.090048 47471995409280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.091714 47166109713280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:52:52.093240 46951030682496 estimator.py:1111] Calling model_fn.
W0618 11:52:52.093347 46951030682496 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:52.093509 47982690743168 estimator.py:1111] Calling model_fn.
W0618 11:52:52.093615 47982690743168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.094704 46951030682496 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:52.094968 47982690743168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:52.103368 47464667927424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880372.034077 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.034812 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.035505 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.103992 46982834291584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.028477 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.029413 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.030246 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.104059 47081511347072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.106035 47911530619776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.105100 46982834291584 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6axz7hyv
I0618 11:52:52.105184 47081511347072 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad24d58ad30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.106177 46982834291584 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6axz7hyv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb53bc4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.106421 47081511347072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:52:52.106620 46982834291584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.107674 47464667927424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.109841 47471995409280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.110361 47911530619776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.111823 47081511347072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.111830 46982834291584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:52:52.112739 47464667927424 estimator.py:1111] Calling model_fn.
W0618 11:52:52.112846 47464667927424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.114087 47166109713280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.114217 47464667927424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:52.115456 47911530619776 estimator.py:1111] Calling model_fn.
W0618 11:52:52.115564 47911530619776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.116926 47911530619776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:52.135033 47791486862208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.135182 47382105961344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.134471 47081511347072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.135126 47621135049600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:52:52.134598 46982834291584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:52:52.135901 47676703556480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880372.097540 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.097978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.098361 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.138402 47577441497984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.139322 47791486862208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.139397 47621135049600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.139493 47382105961344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.140230 47676703556480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:52:52.139446 47577441497984 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpb43ashkr
I0618 11:52:52.140458 47577441497984 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpb43ashkr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45c5164e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.140884 47577441497984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880372.103583 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.104023 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.104366 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.143203 47045861946240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 11:52:52.144379 47791486862208 estimator.py:1111] Calling model_fn.
I0618 11:52:52.144416 47621135049600 estimator.py:1111] Calling model_fn.
W0618 11:52:52.144485 47791486862208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.144523 47621135049600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:52.144551 47382105961344 estimator.py:1111] Calling model_fn.
W0618 11:52:52.144661 47382105961344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:52:52.145329 47676703556480 estimator.py:1111] Calling model_fn.
W0618 11:52:52.145440 47676703556480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:52:52.144169 47045861946240 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplyc1mn5i
W0618 11:52:52.145855 47791486862208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:52.145119 47045861946240 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplyc1mn5i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca0079fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:52.145888 47621135049600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:52:52.146020 47382105961344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:52:52.145501 47045861946240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:52:52.145642 47577441497984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:52:52.146806 47676703556480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880372.074101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.074888 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.075624 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.146750 47947105141632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.076200 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.076946 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.077648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.146876 47941034718080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.104743 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.105139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.105502 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.147295 46999723324288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560880372.104232 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880372.104689 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880372.105039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:52:52.147345 47308824953728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 11:52:52.147826 47947105141632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6fme4r5v
W0618 11:52:52.147876 47941034718080 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxn860ewh
I0618 11:52:52.148834 47947105141632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6fme4r5v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9bd6c1de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:52:52.148865 47941034718080 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxn860ewh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a6cee7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:52:52.148352 46999723324288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc6[2019-06-18 11:53:30] iteration time 6: 47.924 seconds
2019-06-18 11:53:31.206459: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880410.594600 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 11:53:34] minmax time: 3.242 seconds
2019-06-18 11:53:34.458622: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:53:34.464041: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:53:34.468646: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880414.480636 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 6}}
[2019-06-18 11:53:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000008-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir 
[2019-06-18 11:53:34] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=8 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=1023779839 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=2047559670 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=3071339501 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=4095119332 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=5118899163 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=6142678994 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=7166458825 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=8190238656 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=9214018487 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=10237798318 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=11261578149 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=12285357980 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=13309137811 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=14332917642 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=15356697473 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=16380477304 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=17404257135 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=18428036966 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=19451816797 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000007-000005 --seed=20475596628 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:53:44] eval finished: 10.431 seconds
[2019-06-18 11:53:44] Win rate 000007-000005 vs 000005-000004: 0.560
:::MLL 1560880424.987091 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 11:53:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=9 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=1023779840 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=2047559671 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=3071339502 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=4095119333 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=5118899164 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=6142678995 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=7166458826 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=8190238657 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=9214018488 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=10237798319 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=11261578150 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=12285357981 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=13309137812 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=14332917643 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=15356697474 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=16380477305 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=17404257136 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000008-000004 --seed=18428036967 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/
[2019-06-18 11:54:13] selfplay finished: 28.851 seconds
[2019-06-18 11:54:13] selfplay mn: 28.871 seconds
[2019-06-18 11:54:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-9-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779840 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559671 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339502 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119333 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899164 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678995 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458826 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238657 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018488 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798319 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578150 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357981 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137812 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917643 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697474 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477305 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257136 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036967 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816798 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596629 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376460 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156291 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000008-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_
[2019-06-18 11:54:17] divide_golden_chunk finished: 3.260 seconds
[2019-06-18 11:54:17] generate golden chunk: 3.274 seconds
[2019-06-18 11:54:18] train finished: 44.016 seconds
:::MLL 1560880419.747355 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.748242 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.749072 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.829970 47903594218368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.760597 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.761303 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.761967 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.830104 47267461002112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.830937 47903594218368 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvvmfo6cw
W0618 11:53:39.831035 47267461002112 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwyv2u2l_
I0618 11:53:39.831920 47903594218368 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvvmfo6cw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b91b54dde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880419.766932 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.767737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.768450 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.832062 47268165567360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
I0618 11:53:39.832013 47267461002112 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwyv2u2l_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd98cf4dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880419.754419 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.755300 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.756116 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.832149 47261227819904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
I0618 11:53:39.832338 47903594218368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.832413 47267461002112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.833066 47268165567360 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk1r6ccp_
W0618 11:53:39.833123 47261227819904 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqzlmlvdd
I0618 11:53:39.834066 47268165567360 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk1r6ccp_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afdc2ce0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.834107 47261227819904 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqzlmlvdd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc25487da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.834471 47268165567360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.834501 47261227819904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.837357 47267461002112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.837374 47903594218368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.839660 47261227819904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.839677 47268165567360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880419.778774 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.779430 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.780108 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.847063 47778250875776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.771367 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.772240 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.773063 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.847218 47607434584960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.848165 47778250875776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpo6okxh22
W0618 11:53:39.848207 47607434584960 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0wndyr_y
I0618 11:53:39.849296 47778250875776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpo6okxh22', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7486422e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.849325 47607434584960 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0wndyr_y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4cc0d05e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.849691 47778250875776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.849725 47607434584960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.854735 47607434584960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.854780 47778250875776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.859234 47268165567360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.858972 47903594218368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.859482 47261227819904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.859266 47267461002112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.874544 47778250875776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.874535 47607434584960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880419.840971 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.841416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.841787 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.887511 47118158754688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.888579 47118158754688 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf60a8r0g
I0618 11:53:39.889610 47118158754688 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf60a8r0g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adad5b3ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.889997 47118158754688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880419.846025 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.846488 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.846854 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.890061 46992095785856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.891010 46992095785856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9w47nodk
I0618 11:53:39.891978 46992095785856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9w47nodk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abd7bc38dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.892405 46992095785856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.894825 47118158754688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880419.850461 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.850877 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.851228 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.895393 47764309001088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.852728 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.853134 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.853532 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.896050 47857038558080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.896374 47764309001088 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8eljj72m
I0618 11:53:39.897329 47764309001088 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8eljj72m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7147421e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:53:39.896975 46992095785856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.897018 47857038558080 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2w8a8bzl
I0618 11:53:39.897723 47764309001088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.897971 47857038558080 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2w8a8bzl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b86de5ece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.898368 47857038558080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.902323 47764309001088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.902900 47857038558080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880419.830921 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.831816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.832659 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.902776 47479939900288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.836876 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.837628 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.838286 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.902830 47998804976512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.903828 47998804976512 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp10myhh4x
W0618 11:53:39.903875 47479939900288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1mj8pxyd
I0618 11:53:39.904857 47998804976512 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp10myhh4x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7e04eae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.904916 47479939900288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1mj8pxyd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2f1189fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.905261 47998804976512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.905321 47479939900288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.907444 47261227819904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.907439 47268165567360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880419.864905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.865309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.865668 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.909708 47554976338816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.866087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.866495 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.866848 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.909812 47881666438016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.909885 47903594218368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.910256 47267461002112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.910380 47998804976512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.910394 47479939900288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.910664 47554976338816 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj1z6d8kt
W0618 11:53:39.910756 47881666438016 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9zi5hp19
I0618 11:53:39.911626 47554976338816 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj1z6d8kt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b408a0f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.911725 47881666438016 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9zi5hp19', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c9a4e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:53:39.911770 47261227819904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.911756 47268165567360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:53:39.912016 47554976338816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.912122 47881666438016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.914167 47903594218368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.914530 47267461002112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.914616 47118158754688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.916753 47554976338816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.916849 47881666438016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:53:39.916836 47261227819904 estimator.py:1111] Calling model_fn.
I0618 11:53:39.916821 47268165567360 estimator.py:1111] Calling model_fn.
W0618 11:53:39.916572 46992095785856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.916931 47268165567360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.916944 47261227819904 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.918316 47261227819904 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.918281 47268165567360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:53:39.919219 47903594218368 estimator.py:1111] Calling model_fn.
W0618 11:53:39.919328 47903594218368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:53:39.919608 47267461002112 estimator.py:1111] Calling model_fn.
W0618 11:53:39.919717 47267461002112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.920676 47903594218368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.921092 47267461002112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.922009 47764309001088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.922448 47857038558080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.922684 47607434584960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.923141 47778250875776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.926964 47607434584960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.927467 47778250875776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.930232 47998804976512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.930672 47479939900288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:53:39.931994 47607434584960 estimator.py:1111] Calling model_fn.
W0618 11:53:39.932103 47607434584960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:53:39.932534 47778250875776 estimator.py:1111] Calling model_fn.
W0618 11:53:39.932641 47778250875776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.933447 47607434584960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.933976 47778250875776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.936261 47554976338816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:53:39.936314 47881666438016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880419.880151 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.881068 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.881974 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.954941 47682106106752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560880419.886389 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.887151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.887848 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.954942 47795381101440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.955994 47795381101440 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpylzjyhq0
I0618 11:53:39.956034 47682106106752 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5e23957d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.956993 47795381101440 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpylzjyhq0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78834cae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:53:39.957167 47682106106752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:53:39.957421 47795381101440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880419.916130 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.916624 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.917047 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.960044 47838239019904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.961077 47838239019904 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptqhixzbh
I0618 11:53:39.962100 47838239019904 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptqhixzbh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b827dd4ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:53:39.962123 47118158754688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:53:39.962524 47838239019904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.962527 47682106106752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.962631 47795381101440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.963580 46992095785856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880419.921221 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880419.921668 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880419.922050 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:53:39.964139 46954891854720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 11:53:39.965147 46954891854720 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa6gjh6ev
I0618 11:53:39.966175 46954891854720 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa6gjh6ev', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab4d23c9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:53:39.966412 47118158754688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:53:39.966597 46954891854720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:53:39.967536 47838239019904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:53:39.967881 46992095785856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.969544 47857038558080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.969514 47764309001088 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.971223 46954891854720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:53:39.971450 47118158754688 estimator.py:1111] Calling model_fn.
W0618 11:53:39.971555 47118158754688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.972906 47118158754688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:53:39.972914 46992095785856 estimator.py:1111] Calling model_fn.
W0618 11:53:39.973018 46992095785856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.973825 47857038558080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.973816 47764309001088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:53:39.974359 46992095785856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:53:39.978875 47857038558080 estimator.py:1111] Calling model_fn.
I0618 11:53:39.978862 47764309001088 estimator.py:1111] Calling model_fn.
W0618 11:53:39.978969 47764309001088 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.978978 47857038558080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:53:39.978662 47998804976512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:53:39.980325 47857038558080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.980321 47764309001088 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:53:39.980047 47479939900288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as G[2019-06-18 11:54:18] moving /lfs/lfs12/gma_akey/results/epb134/models/000008-000005.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000008-000006.meta
[2019-06-18 11:54:18] moving /lfs/lfs12/gma_akey/results/epb134/models/000008-000005.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb
[2019-06-18 11:54:18] moving /lfs/lfs12/gma_akey/results/epb134/models/000008-000005.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000008-000006.data-00000-of-00001
[2019-06-18 11:54:18] moving /lfs/lfs12/gma_akey/results/epb134/models/000008-000005.index --> /lfs/lfs12/gma_akey/results/epb134/models/000008-000006.index
[2019-06-18 11:54:18] iteration time 7: 47.970 seconds
2019-06-18 11:54:19.203127: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880458.564661 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 11:54:22] minmax time: 3.291 seconds
2019-06-18 11:54:22.503443: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:54:22.508668: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:54:22.513202: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880462.523206 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 7}}
[2019-06-18 11:54:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000009-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:54:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=9 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=1023779840 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=2047559671 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=3071339502 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=4095119333 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=5118899164 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=6142678995 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=7166458826 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=8190238657 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=9214018488 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=10237798319 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=11261578150 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=12285357981 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=13309137812 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=14332917643 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=15356697474 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=16380477305 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=17404257136 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=18428036967 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=19451816798 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000008-000006 --seed=20475596629 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 11:54:33] eval finished: 10.953 seconds
[2019-06-18 11:54:33] Win rate 000008-000006 vs 000007-000005: 0.520
:::MLL 1560880473.548097 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 11:54:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=10 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=1023779841 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=2047559672 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=3071339503 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=4095119334 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=5118899165 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=6142678996 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=7166458827 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=8190238658 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=9214018489 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=10237798320 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=11261578151 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=12285357982 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=13309137813 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=14332917644 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=15356697475 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=16380477306 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=17404257137 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000009-000005 --seed=18428036968 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:55:02] selfplay finished: 29.232 seconds
[2019-06-18 11:55:02] selfplay mn: 29.250 seconds
[2019-06-18 11:55:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-10-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779841 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559672 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339503 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119334 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899165 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678996 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458827 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238658 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018489 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798320 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578151 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357982 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137813 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917644 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697475 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477306 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257137 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036968 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816799 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596630 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376461 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156292 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000009-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:55:06] train finished: 43.492 seconds
:::MLL 1560880467.781740 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.782633 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.783489 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.859600 47155410527104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.789145 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.789889 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.790554 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.859619 47134173545344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.860642 47155410527104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptd12upwq
W0618 11:54:27.860671 47134173545344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkmwfuy8f
I0618 11:54:27.861634 47155410527104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptd12upwq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae38214be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.861664 47134173545344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkmwfuy8f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ade90420e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.862023 47155410527104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.862061 47134173545344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.866926 47134173545344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.866939 47155410527104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880467.790143 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.791006 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.791874 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.870437 47029137474432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.800466 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.801234 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.801905 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.870424 47499647050624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.871524 47499647050624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2uwu9rze
W0618 11:54:27.871559 47029137474432 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpy64beu5c
I0618 11:54:27.872589 47499647050624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2uwu9rze', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b33a82d2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.872618 47029137474432 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpy64beu5c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac61b9ede10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.873001 47499647050624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.873020 47029137474432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.878029 47499647050624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.878043 47029137474432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880467.803356 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.804278 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.805151 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.880712 47114284557184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.811698 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.812410 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.813071 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.880669 47080448480128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.881757 47080448480128 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpd0m4klsb
W0618 11:54:27.881783 47114284557184 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfjatk83m
I0618 11:54:27.882768 47080448480128 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpd0m4klsb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad20dfeada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.882791 47114284557184 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfjatk83m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad9eec82e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.883177 47080448480128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.883193 47114284557184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.888445 47114284557184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.888441 47080448480128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.889324 47134173545344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.889360 47155410527104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.897720 47499647050624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.897719 47029137474432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.908002 47114284557184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.908021 47080448480128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880467.863304 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.863755 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.864174 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.912760 47645751317376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.913739 47645751317376 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptqhaedrg
:::MLL 1560880467.867965 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.868380 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.868749 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.914383 47259052823424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 11:54:27.914725 47645751317376 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptqhaedrg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55acab6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.915125 47645751317376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.915341 47259052823424 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1mh6jipy
I0618 11:54:27.916326 47259052823424 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1mh6jipy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afba3a4ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.916721 47259052823424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.919794 47645751317376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.921308 47259052823424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.939244 47134173545344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.939297 47645751317376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.939626 47155410527104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880467.892705 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.893132 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.893485 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.940672 47189516174208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.893745 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.894159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.894520 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.940695 48004374012800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.940741 47259052823424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.941655 47189516174208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpb26h8ce4
W0618 11:54:27.941684 48004374012800 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnvn9c9af
I0618 11:54:27.942630 47189516174208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpb26h8ce4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeb72ef9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.942657 48004374012800 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnvn9c9af', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba92c3f7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.943033 47189516174208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.943054 48004374012800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.943506 47134173545344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.943913 47155410527104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.946070 47029137474432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.946044 47499647050624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.947789 47189516174208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.947786 48004374012800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:54:27.948519 47134173545344 estimator.py:1111] Calling model_fn.
W0618 11:54:27.948625 47134173545344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:54:27.948940 47155410527104 estimator.py:1111] Calling model_fn.
W0618 11:54:27.949046 47155410527104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:27.949985 47134173545344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:27.950366 47029137474432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.950321 47499647050624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.950413 47155410527104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880467.906416 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.906817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.907174 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.954229 47650764514176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.906502 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.906907 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.907249 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.954295 47084897510272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 11:54:27.955349 47499647050624 estimator.py:1111] Calling model_fn.
I0618 11:54:27.955416 47029137474432 estimator.py:1111] Calling model_fn.
W0618 11:54:27.955456 47499647050624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:27.955518 47029137474432 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:27.955252 47650764514176 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk3mp_xdu
W0618 11:54:27.955282 47084897510272 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpdm5ojo4l
I0618 11:54:27.956210 47650764514176 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk3mp_xdu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b56d77abda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.956261 47084897510272 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpdm5ojo4l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad3172d7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.956595 47650764514176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.956650 47084897510272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.956795 47499647050624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:27.956867 47029137474432 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:27.956995 47114284557184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.957499 47080448480128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.961228 47084897510272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.961258 47650764514176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.961397 47114284557184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.961904 47080448480128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:54:27.966449 47114284557184 estimator.py:1111] Calling model_fn.
W0618 11:54:27.966574 47114284557184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:54:27.967006 47080448480128 estimator.py:1111] Calling model_fn.
W0618 11:54:27.967118 47080448480128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:27.967308 48004374012800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.967349 47189516174208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.967926 47114284557184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:27.968487 47080448480128 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:27.980703 47084897510272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:27.980896 47650764514176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880467.910709 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.911469 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.912182 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.982830 47660387935104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.907781 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.908530 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.909226 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.982993 47268912436096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.983920 47660387935104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmps7t3rdqk
W0618 11:54:27.984020 47268912436096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8ymykxse
I0618 11:54:27.985003 47660387935104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmps7t3rdqk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5915145e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.985087 47268912436096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8ymykxse', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afdef526e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.985434 47660387935104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.985518 47268912436096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:27.986446 47645751317376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.987908 47259052823424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:27.990322 47660387935104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.990322 47268912436096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:27.990743 47645751317376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:27.992229 47259052823424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:54:27.995780 47645751317376 estimator.py:1111] Calling model_fn.
W0618 11:54:27.995888 47645751317376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880467.916752 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.917655 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.918568 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.996120 47472281269120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.931165 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.931980 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.932736 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:27.996115 47582427542400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:27.997241 47645751317376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:54:27.997300 47259052823424 estimator.py:1111] Calling model_fn.
W0618 11:54:27.997406 47259052823424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:27.997207 47582427542400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8e0ej54m
I0618 11:54:27.997228 47472281269120 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2d490c8d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:27.998196 47582427542400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8e0ej54m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b46ee473dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:54:27.998765 47259052823424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:54:27.998352 47472281269120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:27.998586 47582427542400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:54:28.003386 47472281269120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:28.003460 47582427542400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:54:28.010093 47268912436096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:28.010325 47660387935104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:28.015070 47189516174208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:28.015053 48004374012800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:54:28.019391 47189516174208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:54:28.019373 48004374012800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880467.948767 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.949489 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.950186 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:28.021115 47167717720960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560880467.943350 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.944309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.945156 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:28.021255 47360842687360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:28.022220 47167717720960 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyfypyw_9
W0618 11:54:28.022290 47360842687360 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkt4k35tj
I0618 11:54:28.023216 47167717720960 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyfypyw_9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae65fa59e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:28.023276 47360842687360 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkt4k35tj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1356cabe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:54:28.023615 47167717720960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:28.023669 47360842687360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:54:28.024458 47189516174208 estimator.py:1111] Calling model_fn.
I0618 11:54:28.024418 48004374012800 estimator.py:1111] Calling model_fn.
W0618 11:54:28.024531 48004374012800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:28.024564 47189516174208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:54:28.025911 47189516174208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:28.025883 48004374012800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:54:28.026183 47582427542400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:54:28.026827 47472281269120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880467.978615 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880467.978989 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880467.979313 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:54:28.027210 47937821770624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 11:54:28.027900 47084897510272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It dro[2019-06-18 11:55:06] divide_golden_chunk finished: 3.317 seconds
[2019-06-18 11:55:06] generate golden chunk: 3.331 seconds
[2019-06-18 11:55:06] moving /lfs/lfs12/gma_akey/results/epb134/models/000009-000006.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb
[2019-06-18 11:55:06] moving /lfs/lfs12/gma_akey/results/epb134/models/000009-000006.index --> /lfs/lfs12/gma_akey/results/epb134/models/000009-000007.index
[2019-06-18 11:55:06] moving /lfs/lfs12/gma_akey/results/epb134/models/000009-000006.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000009-000007.data-00000-of-00001
[2019-06-18 11:55:06] moving /lfs/lfs12/gma_akey/results/epb134/models/000009-000006.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000009-000007.meta
[2019-06-18 11:55:06] iteration time 8: 47.610 seconds
2019-06-18 11:55:06.863204: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880506.174565 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 11:55:10] minmax time: 3.266 seconds
2019-06-18 11:55:10.139071: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:55:10.144407: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:55:10.149030: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880510.159011 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 8}}
[2019-06-18 11:55:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:55:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=10 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=1023779841 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=2047559672 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=3071339503 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=4095119334 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=5118899165 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=6142678996 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=7166458827 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=8190238658 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=9214018489 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=10237798320 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=11261578151 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=12285357982 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=13309137813 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=14332917644 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=15356697475 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=16380477306 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=17404257137 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=18428036968 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=19451816799 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000009-000007 --seed=20475596630 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:55:20] eval finished: 10.777 seconds
[2019-06-18 11:55:21] Win rate 000009-000007 vs 000008-000006: 0.420
:::MLL 1560880521.009988 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 11:55:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=11 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=1023779842 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=2047559673 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=3071339504 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=4095119335 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=5118899166 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=6142678997 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=7166458828 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=8190238659 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=9214018490 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=10237798321 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=11261578152 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=12285357983 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=13309137814 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=14332917645 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=15356697476 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=16380477307 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=17404257138 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000010-000006 --seed=18428036969 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:55:50] selfplay finished: 29.661 seconds
[2019-06-18 11:55:50] selfplay mn: 29.678 seconds
[2019-06-18 11:55:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-11-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779842 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559673 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339504 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119335 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899166 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678997 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458828 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238659 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018490 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798321 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578152 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357983 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137814 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917645 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697476 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477307 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257138 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036969 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816800 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596631 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376462 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156293 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000010-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:55:53] train finished: 43.672 seconds
:::MLL 1560880515.429313 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.430231 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.431096 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.518069 47084330980224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.519099 47084330980224 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmposxdygo4
I0618 11:55:15.520100 47084330980224 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmposxdygo4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad2f568de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880515.444688 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.445404 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.446048 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.520141 47189214409600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:55:15.520498 47084330980224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880515.442888 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.443600 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.444292 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.521008 47706622755712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.433954 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.434841 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.435711 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.521096 47012443214720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.521152 47189214409600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl3g_1ijb
I0618 11:55:15.522139 47189214409600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl3g_1ijb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeb60f30dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.522601 47189214409600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.522044 47706622755712 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpou9df559
W0618 11:55:15.522071 47012443214720 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl94kwj_1
I0618 11:55:15.523074 47706622755712 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpou9df559', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b63d8e3de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.523076 47012443214720 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl94kwj_1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac23890ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.523477 47706622755712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.523475 47012443214720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.525450 47084330980224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.527899 47189214409600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.528481 47012443214720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.528482 47706622755712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880515.463018 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.463868 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.464553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.541104 47787947651968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.461806 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.462635 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.463465 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.541376 47011998118784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.542110 47787947651968 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplhekkpth
W0618 11:55:15.542316 47011998118784 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz3idhqgv
I0618 11:55:15.543084 47787947651968 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplhekkpth', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b76c83b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.543283 47011998118784 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz3idhqgv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac21e090da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.543488 47787947651968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.543683 47011998118784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.545927 47084330980224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.548401 47787947651968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.548538 47011998118784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.548570 47012443214720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.548657 47706622755712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.552166 47189214409600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.568246 47787947651968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.568311 47011998118784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880515.529260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.529726 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.530141 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.581433 46933869572992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.582443 46933869572992 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf5q4wlw_
:::MLL 1560880515.533862 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.534309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.534703 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.583106 47661354582912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 11:55:15.583402 46933869572992 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf5q4wlw_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafed35fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.583798 46933869572992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.584061 47661354582912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmps_73bufe
I0618 11:55:15.585065 47661354582912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmps_73bufe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b594eb25e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.585464 47661354582912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880515.535208 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.535598 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.535915 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.587621 47507040039808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.534713 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.535118 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.535457 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.587715 47251211490176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.588322 46933869572992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.588619 47507040039808 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe9mf2gyd
W0618 11:55:15.588695 47251211490176 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphffb1bys
I0618 11:55:15.589604 47507040039808 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe9mf2gyd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3560d53e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.589676 47251211490176 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphffb1bys', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9d0436e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:55:15.589974 47661354582912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:15.589994 47507040039808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.590068 47251211490176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.594794 47507040039808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.594801 47251211490176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880515.546814 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.547241 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.547611 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.596861 47940051854208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.545444 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.545902 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.546425 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.596887 47689291768704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.597364 47012443214720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.597661 47706622755712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.597966 47084330980224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.597982 47940051854208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpdt6vycq3
W0618 11:55:15.598013 47689291768704 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_c9linqx
I0618 11:55:15.599051 47689291768704 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_c9linqx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5fcfe1fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.599050 47940051854208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpdt6vycq3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a32593e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.599466 47689291768704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.599481 47940051854208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.601679 47012443214720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.601964 47706622755712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.602267 47084330980224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.604187 47689291768704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.604247 47940051854208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:55:15.606778 47012443214720 estimator.py:1111] Calling model_fn.
W0618 11:55:15.606890 47012443214720 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:15.607016 47706622755712 estimator.py:1111] Calling model_fn.
W0618 11:55:15.607128 47706622755712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:55:15.607393 47084330980224 estimator.py:1111] Calling model_fn.
W0618 11:55:15.607768 46933869572992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.607499 47084330980224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.607744 47189214409600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.608257 47012443214720 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.608486 47706622755712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.608842 47084330980224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.609544 47661354582912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.612070 47189214409600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.614522 47507040039808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.614500 47251211490176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.616377 47787947651968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.616455 47011998118784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:55:15.617172 47189214409600 estimator.py:1111] Calling model_fn.
W0618 11:55:15.617280 47189214409600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.618631 47189214409600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.620676 47787947651968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.620755 47011998118784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.624291 47689291768704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.624333 47940051854208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:55:15.625807 47011998118784 estimator.py:1111] Calling model_fn.
I0618 11:55:15.625766 47787947651968 estimator.py:1111] Calling model_fn.
W0618 11:55:15.625880 47787947651968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.625916 47011998118784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.627239 47011998118784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.627244 47787947651968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880515.550252 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.551184 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.552082 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.629875 47048954323840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.557036 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.557787 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.558489 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.629970 47110580192128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.630913 47048954323840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp03wxe06u
I0618 11:55:15.631057 47110580192128 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad911fc1d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.631960 47048954323840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp03wxe06u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acab8cbfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.632229 47110580192128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.632384 47048954323840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.637410 47048954323840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.637414 47110580192128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880515.575667 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.576440 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.577140 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.651168 47000166781824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.572900 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.573680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.574443 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.651195 46964118508416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.652177 47000166781824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1_eilqiq
W0618 11:55:15.652149 46964118508416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp75o4r2i8
I0618 11:55:15.653251 47000166781824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1_eilqiq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abf5cd52e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.653255 46964118508416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp75o4r2i8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab6f8302e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.653691 47000166781824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.653690 46964118508416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:55:15.654853 46933869572992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.657182 47661354582912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.658505 47000166781824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.659108 46933869572992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.658507 46964118508416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.658661 47048954323840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:55:15.659036 47110580192128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880515.578483 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.579400 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.580305 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.660456 47203363079040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560880515.584300 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880515.585081 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880515.585800 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:55:15.660456 47777352938368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 11:55:15.661517 47661354582912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.662288 47507040039808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.662324 47251211490176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.661596 47203363079040 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpds046hr3
W0618 11:55:15.661623 47777352938368 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjxavmppt
I0618 11:55:15.662656 47203363079040 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpds046hr3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeeac468e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.662670 47777352938368 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjxavmppt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7450bcbda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:55:15.663081 47203363079040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.663087 47777352938368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:55:15.664137 46933869572992 estimator.py:1111] Calling model_fn.
W0618 11:55:15.664244 46933869572992 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.665613 46933869572992 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:55:15.666607 47661354582912 estimator.py:1111] Calling model_fn.
W0618 11:55:15.666721 47661354582912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.666588 47507040039808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.666634 47251211490176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:55:15.668088 47661354582912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.668231 47203363079040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.668251 47777352938368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:55:15.671670 47689291768704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:55:15.671663 47507040039808 estimator.py:1111] Calling model_fn.
I0618 11:55:15.671707 47251211490176 estimator.py:1111] Calling model_fn.
W0618 11:55:15.671774 47507040039808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.671817 47251211490176 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:55:15.672082 47940051854208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:55:15.673143 47507040039808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:55:15.673182 47251211490176 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/[2019-06-18 11:55:53] divide_golden_chunk finished: 3.308 seconds
[2019-06-18 11:55:54] generate golden chunk: 3.322 seconds
[2019-06-18 11:55:54] iteration time 9: 47.837 seconds
2019-06-18 11:55:54.732797: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880554.011663 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 11:55:57] minmax time: 3.236 seconds
2019-06-18 11:55:57.979158: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:55:57.984463: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:55:57.988972: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880558.000406 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 9}}
[2019-06-18 11:55:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:55:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=11 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=1023779842 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=2047559673 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=3071339504 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=4095119335 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=5118899166 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=6142678997 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=7166458828 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=8190238659 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=9214018490 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=10237798321 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=11261578152 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=12285357983 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=13309137814 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=14332917645 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=15356697476 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=16380477307 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=17404257138 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=18428036969 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=19451816800 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000010-000007 --seed=20475596631 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:56:08] eval finished: 10.583 seconds
[2019-06-18 11:56:08] Win rate 000010-000007 vs 000008-000006: 0.590
:::MLL 1560880568.655869 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 11:56:08] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=12 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=1023779843 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=2047559674 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=3071339505 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=4095119336 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=5118899167 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=6142678998 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=7166458829 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=8190238660 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=9214018491 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=10237798322 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=11261578153 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=12285357984 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=13309137815 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=14332917646 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=15356697477 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=16380477308 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=17404257139 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000011-000006 --seed=18428036970 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:56:37] selfplay finished: 28.627 seconds
[2019-06-18 11:56:37] selfplay mn: 28.643 seconds
[2019-06-18 11:56:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-12-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779843 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559674 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339505 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119336 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899167 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678998 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458829 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238660 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018491 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798322 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578153 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357984 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137815 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917646 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697477 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477308 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257139 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036970 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816801 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596632 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376463 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156294 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:56:40] divide_golden_chunk finished: 3.311 seconds
[2019-06-18 11:56:40] generate golden chunk: 3.325 seconds
[2019-06-18 11:56:41] train finished: 43.754 seconds
:::MLL 1560880563.227162 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.228047 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.228882 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.311427 47148194943872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880563.235781 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.236419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.237096 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.311630 47863239607168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.312440 47148194943872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjg01o2m7
W0618 11:56:03.312614 47863239607168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2es6o8w1
I0618 11:56:03.313491 47148194943872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjg01o2m7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1d3ff7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.313670 47863239607168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2es6o8w1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b884ffb5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.313920 47148194943872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.314100 47863239607168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.318931 47148194943872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.318977 47863239607168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880563.229755 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.230582 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.231426 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.322358 47740616217472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.323438 47740616217472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpg6bn85be
:::MLL 1560880563.229704 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.230536 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.231375 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.324143 47302819255168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 11:56:03.324566 47740616217472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpg6bn85be', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6bc30ede48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.325025 47740616217472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.325221 47302819255168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphgl0vq1h
I0618 11:56:03.326486 47302819255168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphgl0vq1h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b05d4536e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.327001 47302819255168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.330322 47740616217472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.332668 47302819255168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880563.253196 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.254103 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.254951 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.335868 47894009979776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880563.270260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.271102 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.271818 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.335984 47917739504512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.336930 47917739504512 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuygt189p
W0618 11:56:03.336888 47894009979776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpv88rb6no
I0618 11:56:03.337895 47894009979776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpv88rb6no', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8f7a09fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.337917 47917739504512 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuygt189p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b95006dce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.338296 47894009979776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.338315 47917739504512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.338453 47863239607168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.338502 47148194943872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.343289 47894009979776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.343323 47917739504512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.352649 47740616217472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.358029 47302819255168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.363415 47917739504512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.363516 47894009979776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880563.313330 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.313798 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.314204 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.369076 47052709176192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.370144 47052709176192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyhx0i__g
I0618 11:56:03.371174 47052709176192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyhx0i__g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb989a7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.371565 47052709176192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880563.319330 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.319776 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.320182 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.372100 47895402349440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.373130 47895402349440 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl5af50es
I0618 11:56:03.374129 47895402349440 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl5af50es', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8fcd07de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.374524 47895402349440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.376243 47052709176192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.379004 47895402349440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880563.330496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.330890 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.331216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.383997 47620326396800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.385061 47620326396800 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpx4z5tn1k
I0618 11:56:03.386135 47620326396800 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpx4z5tn1k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4fc139ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.386526 47620326396800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.386664 47148194943872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.386806 47863239607168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880563.329878 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.330280 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.330627 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.387250 47747270501248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.388371 47747270501248 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7da8beke
I0618 11:56:03.389636 47747270501248 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7da8beke', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d4faf3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.390096 47747270501248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.390949 47148194943872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.391114 47620326396800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.391131 47863239607168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.395228 47747270501248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.395669 47052709176192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:56:03.395986 47148194943872 estimator.py:1111] Calling model_fn.
W0618 11:56:03.396097 47148194943872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:03.396211 47863239607168 estimator.py:1111] Calling model_fn.
W0618 11:56:03.396322 47863239607168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.397449 47148194943872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.397686 47863239607168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.398428 47895402349440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880563.348558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.349022 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.349425 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.402425 47657000305536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880563.348530 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.348995 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.349400 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.402446 47919172404096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.403473 47657000305536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9lfm9td0
W0618 11:56:03.403504 47919172404096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_adna8xi
I0618 11:56:03.404443 47657000305536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9lfm9td0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b584b295e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.404468 47919172404096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_adna8xi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9555d61e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:03.404368 47740616217472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:56:03.404838 47657000305536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.404854 47919172404096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:03.409010 47740616217472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.409438 47657000305536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.409422 47919172404096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.410660 47620326396800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.412067 47917739504512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.412343 47894009979776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.414330 47302819255168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:56:03.414461 47740616217472 estimator.py:1111] Calling model_fn.
W0618 11:56:03.414577 47740616217472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.416035 47740616217472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.416451 47917739504512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.416691 47894009979776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.418942 47747270501248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.419482 47302819255168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:03.421587 47917739504512 estimator.py:1111] Calling model_fn.
W0618 11:56:03.421698 47917739504512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:03.421813 47894009979776 estimator.py:1111] Calling model_fn.
W0618 11:56:03.421928 47894009979776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.423046 47917739504512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.423299 47894009979776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:56:03.425635 47302819255168 estimator.py:1111] Calling model_fn.
W0618 11:56:03.425751 47302819255168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.427273 47302819255168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.429008 47657000305536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.429099 47919172404096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.442939 47052709176192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.445543 47895402349440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.447220 47052709176192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880563.378560 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.379270 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.379937 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.449428 47135119807360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560880563.367931 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.368847 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.369654 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.449533 47366665540480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 11:56:03.449845 47895402349440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.450491 47135119807360 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnhkgoini
W0618 11:56:03.450516 47366665540480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpm_z1kxde
I0618 11:56:03.451494 47135119807360 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnhkgoini', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adec8a8ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:03.451494 47366665540480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpm_z1kxde', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14b1dc6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880563.368190 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.369002 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.369668 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.451240 47688640566144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 11:56:03.451896 47135119807360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.451895 47366665540480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880563.371771 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.372537 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.373242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:03.451294 47148313936768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 11:56:03.452243 47052709176192 estimator.py:1111] Calling model_fn.
W0618 11:56:03.452351 47052709176192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:03.452243 47688640566144 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5fa9116d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:03.452293 47148313936768 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc3_44gtm
I0618 11:56:03.453282 47148313936768 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpc3_44gtm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1db175e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:03.453686 47052709176192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:56:03.453345 47688640566144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.453675 47148313936768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:03.454870 47895402349440 estimator.py:1111] Calling model_fn.
W0618 11:56:03.454977 47895402349440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.456338 47895402349440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.456702 47135119807360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.456688 47366665540480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.458008 47620326396800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.458104 47688640566144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.458314 47148313936768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:03.462316 47620326396800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:03.467371 47620326396800 estimator.py:1111] Calling model_fn.
W0618 11:56:03.467481 47620326396800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.468848 47620326396800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.474053 47747270501248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.476458 47657000305536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.476138 47366665540480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.476254 47135119807360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.476675 47919172404096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:03.479154 47747270501248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.480749 47657000305536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.481006 47919172404096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:03.480935 47148313936768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:03.481010 47688640566144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:56:03.485357 47747270501248 estimator.py:1111] Calling model_fn.
W0618 11:56:03.485496 47747270501248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:03.485832 47657000305536 estimator.py:1111] Calling model_fn.
W0618 11:56:03.485938 47657000305536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:03.486061 47919172404096 estimator.py:1111] Calling model_fn.
W0618 11:56:03.486167 47919172404096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:03.487008 47747270501248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.487299 47657000305536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:03.487532 47919172404096 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880563.421671 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880563.422622 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880563.423501 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750][2019-06-18 11:56:41] moving /lfs/lfs12/gma_akey/results/epb134/models/000011-000007.index --> /lfs/lfs12/gma_akey/results/epb134/models/000011-000008.index
[2019-06-18 11:56:41] moving /lfs/lfs12/gma_akey/results/epb134/models/000011-000007.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000011-000008.meta
[2019-06-18 11:56:41] moving /lfs/lfs12/gma_akey/results/epb134/models/000011-000007.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb
[2019-06-18 11:56:41] moving /lfs/lfs12/gma_akey/results/epb134/models/000011-000007.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000011-000008.data-00000-of-00001
[2019-06-18 11:56:41] iteration time 10: 47.806 seconds
2019-06-18 11:56:42.580854: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880601.817918 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 11:56:45] minmax time: 3.233 seconds
2019-06-18 11:56:45.824091: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:56:45.829369: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:56:45.833718: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880605.843721 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 10}}
[2019-06-18 11:56:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:56:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=12 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=1023779843 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=2047559674 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=3071339505 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=4095119336 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=5118899167 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=6142678998 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=7166458829 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=8190238660 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=9214018491 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=10237798322 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=11261578153 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=12285357984 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=13309137815 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=14332917646 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=15356697477 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=16380477308 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=17404257139 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=18428036970 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=19451816801 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000011-000008 --seed=20475596632 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:56:57] eval finished: 11.514 seconds
[2019-06-18 11:56:57] Win rate 000011-000008 vs 000010-000007: 0.240
:::MLL 1560880617.431949 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 11:56:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=13 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=1023779844 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=2047559675 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=3071339506 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=4095119337 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=5118899168 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=6142678999 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=7166458830 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=8190238661 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=9214018492 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=10237798323 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=11261578154 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=12285357985 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=13309137816 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=14332917647 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=15356697478 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=16380477309 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=17404257140 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000012-000007 --seed=18428036971 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:57:27] selfplay finished: 30.190 seconds
[2019-06-18 11:57:27] selfplay mn: 30.207 seconds
[2019-06-18 11:57:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-13-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779844 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559675 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339506 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119337 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899168 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142678999 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458830 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238661 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018492 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798323 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578154 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357985 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137816 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917647 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697478 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477309 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257140 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036971 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816802 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596633 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376464 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156295 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:57:29] train finished: 43.722 seconds
:::MLL 1560880611.104736 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.105481 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.106132 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.190493 47116938531712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.102979 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.103723 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.104504 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.190572 47879949783936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.191515 47116938531712 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwqqqt1xb
W0618 11:56:51.191547 47879949783936 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqtj5_yvg
I0618 11:56:51.192522 47116938531712 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwqqqt1xb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada8cf8add8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.192523 47879949783936 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqtj5_yvg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c33fc5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.192915 47116938531712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.192917 47879949783936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.197660 47879949783936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.197692 47116938531712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880611.115380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.116271 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.116954 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.203450 47261390922624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.119298 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.119979 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.120550 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.203502 47406571647872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.204539 47261390922624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptzfehcdi
W0618 11:56:51.204571 47406571647872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnsko0zyz
I0618 11:56:51.205553 47261390922624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptzfehcdi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc2f013da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.205580 47406571647872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnsko0zyz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1dfc733e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.205965 47261390922624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.205982 47406571647872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.211128 47261390922624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.211174 47406571647872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880611.121311 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.122225 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.123008 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.211110 47623726515072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.136433 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.137151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.137810 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.211179 47164399391616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.212172 47164399391616 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpb1z23uou
W0618 11:56:51.212146 47623726515072 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz5_h3e8u
I0618 11:56:51.213204 47623726515072 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz5_h3e8u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b508be39e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.213216 47164399391616 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpb1z23uou', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae599dbeda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.213629 47623726515072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.213644 47164399391616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.218468 47164399391616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.218494 47623726515072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.219679 47879949783936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.220280 47116938531712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.230691 47406571647872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.230798 47261390922624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.237798 47164399391616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.237796 47623726515072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880611.194801 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.195246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.195679 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.253142 47577682924416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.190052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.190491 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.190871 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.253245 47328452633472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.254183 47577682924416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpgewhbyew
W0618 11:56:51.254213 47328452633472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmprvh_yr5l
I0618 11:56:51.255184 47577682924416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpgewhbyew', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45d37a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.255212 47328452633472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmprvh_yr5l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0bcc31ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.255584 47577682924416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.255602 47328452633472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.260368 47577682924416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.260370 47328452633472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880611.183731 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.184695 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.185591 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.269341 47353956012928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.190474 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.191229 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.191901 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.269500 47620592849792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.270573 47879949783936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.271671 47116938531712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:56:51.270440 47353956012928 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11bc504d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:51.270562 47620592849792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptoz2wvx0
I0618 11:56:51.271578 47620592849792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptoz2wvx0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4fd11bae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.271599 47353956012928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.271980 47620592849792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.274861 47879949783936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.276004 47116938531712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.276592 47353956012928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.276847 47620592849792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.278894 47406571647872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880611.216761 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.217151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.217481 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.278836 47312635036544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.218604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.219012 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.219331 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.279011 47531361014656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.219487 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.219923 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.220310 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.279379 47193599701888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.219641 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.220088 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.220463 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.279436 47883156095872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.279568 47261390922624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.279582 47328452633472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.279806 47577682924416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:56:51.279919 47879949783936 estimator.py:1111] Calling model_fn.
W0618 11:56:51.280031 47879949783936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:51.279830 47312635036544 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvnu9k85l
W0618 11:56:51.280009 47531361014656 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi6ekh_ba
I0618 11:56:51.280879 47312635036544 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvnu9k85l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b081d645e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.281044 47531361014656 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi6ekh_ba', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3b0a79ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:56:51.280373 47193599701888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwxy4am3n
W0618 11:56:51.280410 47883156095872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyv4pfpwc
I0618 11:56:51.281293 47312635036544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.281341 47193599701888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwxy4am3n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aec66553e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.281401 47883156095872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyv4pfpwc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8cf318de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.281473 47531361014656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.281172 47116938531712 estimator.py:1111] Calling model_fn.
W0618 11:56:51.281284 47116938531712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:51.281394 47879949783936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:56:51.281732 47193599701888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.281804 47883156095872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.282691 47116938531712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:51.283221 47406571647872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.283934 47261390922624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.285771 47623726515072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.285915 47312635036544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.286019 47531361014656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.286436 47193599701888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.286480 47883156095872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.286489 47164399391616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:56:51.288299 47406571647872 estimator.py:1111] Calling model_fn.
W0618 11:56:51.288408 47406571647872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:51.289087 47261390922624 estimator.py:1111] Calling model_fn.
W0618 11:56:51.289198 47261390922624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:51.289776 47406571647872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:51.290055 47623726515072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.290574 47261390922624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:51.290777 47164399391616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:51.295098 47623726515072 estimator.py:1111] Calling model_fn.
W0618 11:56:51.295203 47623726515072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:51.295859 47164399391616 estimator.py:1111] Calling model_fn.
W0618 11:56:51.295969 47164399391616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:51.296545 47623726515072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:51.297315 47164399391616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880611.214707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.215460 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.216156 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.297762 47203079500672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.208487 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.209440 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.210280 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.297909 47819392795520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.297591 47353956012928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.297732 47620592849792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.298829 47203079500672 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp75ucg330
W0618 11:56:51.298885 47819392795520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9oq2wcb2
I0618 11:56:51.299822 47203079500672 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp75ucg330', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee9b5f8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.299880 47819392795520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9oq2wcb2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7e1a821e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.300221 47203079500672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.300280 47819392795520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.305167 47312635036544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.305266 47531361014656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.305189 47203079500672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.305188 47819392795520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.305786 47883156095872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.305839 47193599701888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.324777 47203079500672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.324883 47819392795520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:56:51.326949 47328452633472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.327322 47577682924416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.331246 47328452633472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:56:51.331626 47577682924416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:56:51.336322 47328452633472 estimator.py:1111] Calling model_fn.
W0618 11:56:51.336429 47328452633472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:56:51.336733 47577682924416 estimator.py:1111] Calling model_fn.
W0618 11:56:51.336842 47577682924416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:56:51.337782 47328452633472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:56:51.338215 47577682924416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880611.282332 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.282714 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.283039 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.338406 47283071169408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560880611.283620 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.283991 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.284313 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.338524 47387046761344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.339405 47283071169408 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplnfldes_
W0618 11:56:51.339474 47387046761344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpawxof46e
I0618 11:56:51.340388 47283071169408 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplnfldes_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b013b3f8dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.340443 47387046761344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpawxof46e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1970ad1dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:56:51.340792 47283071169408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:56:51.340836 47387046761344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:56:51.345418 47387046761344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.345448 47283071169408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:56:51.350769 47353956012928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.351099 47620592849792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.352481 47312635036544 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.352563 47531361014656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880611.297616 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880611.298084 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880611.298505 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:56:51.352652 47622317454208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 11:56:51.353296 47193599701888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:56:51.353422 47883156095872 deprecation.py:323] From ./preprocessing.py:[2019-06-18 11:57:30] divide_golden_chunk finished: 3.303 seconds
[2019-06-18 11:57:30] generate golden chunk: 3.317 seconds
[2019-06-18 11:57:30] iteration time 11: 49.140 seconds
2019-06-18 11:57:31.759088: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880650.958327 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 11:57:35] minmax time: 3.239 seconds
2019-06-18 11:57:35.008529: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:57:35.013926: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:57:35.018466: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880655.030467 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 11}}
[2019-06-18 11:57:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:57:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=13 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=1023779844 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=2047559675 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=3071339506 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=4095119337 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=5118899168 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=6142678999 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=7166458830 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=8190238661 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=9214018492 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=10237798323 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=11261578154 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=12285357985 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=13309137816 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=14332917647 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=15356697478 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=16380477309 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=17404257140 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=18428036971 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=19451816802 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000012-000008 --seed=20475596633 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:57:45] eval finished: 10.797 seconds
[2019-06-18 11:57:45] Win rate 000012-000008 vs 000010-000007: 0.520
:::MLL 1560880665.899611 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 11:57:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=14 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=1023779845 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=2047559676 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=3071339507 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=4095119338 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=5118899169 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=6142679000 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=7166458831 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=8190238662 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=9214018493 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=10237798324 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=11261578155 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=12285357986 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=13309137817 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=14332917648 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=15356697479 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=16380477310 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=17404257141 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000013-000007 --seed=18428036972 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:58:17] selfplay finished: 31.142 seconds
[2019-06-18 11:58:17] selfplay mn: 31.159 seconds
[2019-06-18 11:58:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-14-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779845 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559676 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339507 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119338 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899169 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679000 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458831 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238662 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018493 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798324 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578155 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357986 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137817 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917648 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697479 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477310 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257141 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036972 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816803 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596634 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376465 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156296 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:58:18] train finished: 43.782 seconds
:::MLL 1560880660.310186 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.311059 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.311882 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.395471 47759797502848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.317396 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.318141 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.318794 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.395719 47893399753600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.396511 47759797502848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp61_wzogo
W0618 11:57:40.396740 47893399753600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsgievpln
I0618 11:57:40.397518 47759797502848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp61_wzogo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b703a5a1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.397705 47893399753600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsgievpln', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8f55aaae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.397920 47759797502848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.398108 47893399753600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.402756 47893399753600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.402756 47759797502848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880660.334652 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.335368 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.336085 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.415827 47577538077568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.330897 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.331817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.332574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.415827 47326479606656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.416872 47577538077568 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz_wq64bo
W0618 11:57:40.416844 47326479606656 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf389e2wx
I0618 11:57:40.417837 47577538077568 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz_wq64bo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45cad7fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.417834 47326479606656 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf389e2wx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b56979da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.418237 47326479606656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.418243 47577538077568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.423110 47577538077568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.423096 47326479606656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.425126 47893399753600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.425215 47759797502848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.442760 47577538077568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.442741 47326479606656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880660.394627 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.394987 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.395303 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.459651 47278921749376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.392307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.392681 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.393001 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.459707 47676994233216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.460675 47676994233216 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_jw9a1a0
W0618 11:57:40.460632 47278921749376 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfmf4oltg
I0618 11:57:40.461625 47278921749376 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfmf4oltg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0043ec5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.461636 47676994233216 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_jw9a1a0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5cf2e46dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.462014 47278921749376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.462024 47676994233216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.466837 47278921749376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.466842 47676994233216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880660.395240 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.395962 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.396637 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.469737 47740699108224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.388260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.389163 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.390005 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.469815 47341853324160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.470821 47740699108224 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpeigi7bq9
W0618 11:57:40.470862 47341853324160 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplbil493b
I0618 11:57:40.471896 47740699108224 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpeigi7bq9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6bc7ffae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880660.414797 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.415197 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.415537 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.471552 47915204137856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.413209 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.413604 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.413961 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.471634 47605345121152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 11:57:40.471938 47341853324160 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplbil493b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0eeaf00e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.472290 47740699108224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.472337 47341853324160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.472543 47915204137856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxe7ocvn9
W0618 11:57:40.472624 47605345121152 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq5tys9yu
I0618 11:57:40.473515 47915204137856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxe7ocvn9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b94694f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.473603 47605345121152 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq5tys9yu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4c4445ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.473921 47915204137856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.474009 47605345121152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.476564 47759797502848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.477042 47740699108224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.477046 47341853324160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.476958 47893399753600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.478659 47915204137856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.478709 47605345121152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.480862 47759797502848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.481298 47893399753600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:57:40.485979 47759797502848 estimator.py:1111] Calling model_fn.
W0618 11:57:40.486088 47759797502848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:40.486159 47676994233216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.486256 47278921749376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:57:40.486416 47893399753600 estimator.py:1111] Calling model_fn.
W0618 11:57:40.486526 47893399753600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:40.487453 47759797502848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.487880 47893399753600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.490531 47326479606656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.490993 47577538077568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.494846 47326479606656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.495343 47577538077568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.496797 47740699108224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.497092 47341853324160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.498028 47915204137856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.498257 47605345121152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:57:40.499914 47326479606656 estimator.py:1111] Calling model_fn.
W0618 11:57:40.500021 47326479606656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:40.500481 47577538077568 estimator.py:1111] Calling model_fn.
W0618 11:57:40.500591 47577538077568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:40.501381 47326479606656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.501951 47577538077568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880660.429490 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.430254 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.430968 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.506148 47853181395840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.423504 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.424409 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.425249 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.506171 47172678095744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 11:57:40.507207 47172678095744 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae7874eecc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:57:40.507200 47853181395840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphz3ie99w
I0618 11:57:40.508149 47853181395840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphz3ie99w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b85f8772e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.508304 47172678095744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.508543 47853181395840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.513532 47172678095744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.513710 47853181395840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880660.467606 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.468112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.468483 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.524711 47410850624384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.470805 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.471255 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.471647 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.526242 47593319228288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.525720 47410850624384 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpiswx2foy
I0618 11:57:40.526732 47410850624384 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpiswx2foy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1efb7f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.527133 47410850624384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.527221 47593319228288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpved3jupn
I0618 11:57:40.528191 47593319228288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpved3jupn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4977792e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.528586 47593319228288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.531838 47410850624384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.533159 47593319228288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.533898 47278921749376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.533999 47676994233216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.535770 47853181395840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.535797 47172678095744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.538220 47278921749376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.538310 47676994233216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:57:40.543297 47278921749376 estimator.py:1111] Calling model_fn.
W0618 11:57:40.543402 47278921749376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:40.543415 47676994233216 estimator.py:1111] Calling model_fn.
W0618 11:57:40.543520 47676994233216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:40.544809 47740699108224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.545074 47341853324160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.544749 47278921749376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.544898 47676994233216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.545311 47915204137856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.545712 47605345121152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.549109 47740699108224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.549393 47341853324160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.549587 47915204137856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.550035 47605345121152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:57:40.551229 47410850624384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:57:40.552665 47593319228288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:57:40.554192 47740699108224 estimator.py:1111] Calling model_fn.
W0618 11:57:40.554298 47740699108224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:40.554504 47341853324160 estimator.py:1111] Calling model_fn.
W0618 11:57:40.554612 47341853324160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:40.554706 47915204137856 estimator.py:1111] Calling model_fn.
W0618 11:57:40.554811 47915204137856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:57:40.555162 47605345121152 estimator.py:1111] Calling model_fn.
W0618 11:57:40.555272 47605345121152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:57:40.555629 47740699108224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.555961 47341853324160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.556167 47915204137856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:57:40.556643 47605345121152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880660.479897 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.480858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.481755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.562276 47688062505856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.502667 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.503498 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.504281 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.562906 47418310161280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 11:57:40.563306 47688062505856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe415s9tg
I0618 11:57:40.564399 47688062505856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe415s9tg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f869cee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:57:40.563954 47418310161280 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl81r7fc1
I0618 11:57:40.564829 47688062505856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880660.506875 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.507270 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.507595 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.564181 47317471327104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560880660.508687 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880660.509098 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880660.509424 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:57:40.564193 47745049043840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 11:57:40.565001 47418310161280 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl81r7fc1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20b81ebe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.565418 47418310161280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.565230 47317471327104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_2r_edlv
W0618 11:57:40.565257 47745049043840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbui89tpl
I0618 11:57:40.566197 47317471327104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_2r_edlv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b093da84e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.566222 47745049043840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbui89tpl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ccb467e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:57:40.566605 47317471327104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:57:40.566613 47745049043840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:57:40.569886 47688062505856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.570364 47418310161280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.571274 47317471327104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.571315 47745049043840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:57:40.585208 47172678095744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:57:40.585781 47853181395840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_funct[2019-06-18 11:58:20] divide_golden_chunk finished: 3.535 seconds
[2019-06-18 11:58:20] generate golden chunk: 3.549 seconds
[2019-06-18 11:58:20] moving /lfs/lfs12/gma_akey/results/epb134/models/000013-000008.index --> /lfs/lfs12/gma_akey/results/epb134/models/000013-000009.index
[2019-06-18 11:58:20] moving /lfs/lfs12/gma_akey/results/epb134/models/000013-000008.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000013-000009.data-00000-of-00001
[2019-06-18 11:58:20] moving /lfs/lfs12/gma_akey/results/epb134/models/000013-000008.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb
[2019-06-18 11:58:20] moving /lfs/lfs12/gma_akey/results/epb134/models/000013-000008.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000013-000009.meta
[2019-06-18 11:58:20] iteration time 12: 49.692 seconds
2019-06-18 11:58:21.483967: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880700.650790 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 11:58:24] minmax time: 3.255 seconds
2019-06-18 11:58:24.748725: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:58:24.754038: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:58:24.758561: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880704.768989 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 12}}
[2019-06-18 11:58:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:58:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=14 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=1023779845 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=2047559676 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=3071339507 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=4095119338 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=5118899169 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=6142679000 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=7166458831 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=8190238662 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=9214018493 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=10237798324 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=11261578155 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=12285357986 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=13309137817 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=14332917648 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=15356697479 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=16380477310 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=17404257141 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=18428036972 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=19451816803 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000013-000009 --seed=20475596634 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:58:36] eval finished: 11.536 seconds
[2019-06-18 11:58:36] Win rate 000013-000009 vs 000012-000008: 0.660
:::MLL 1560880716.376924 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 11:58:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=15 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=1023779846 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=2047559677 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=3071339508 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=4095119339 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=5118899170 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=6142679001 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=7166458832 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=8190238663 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=9214018494 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=10237798325 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=11261578156 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=12285357987 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=13309137818 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=14332917649 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=15356697480 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=16380477311 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=17404257142 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000014-000008 --seed=18428036973 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:59:05] selfplay finished: 29.425 seconds
[2019-06-18 11:59:05] selfplay mn: 29.442 seconds
[2019-06-18 11:59:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-15-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779846 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559677 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339508 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119339 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899170 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679001 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458832 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238663 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018494 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798325 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578156 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357987 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137818 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917649 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697480 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477311 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257142 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036973 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816804 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596635 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376466 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156297 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:59:08] train finished: 43.791 seconds
:::MLL 1560880710.033483 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.034224 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.034894 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.113252 47488451916672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.020896 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.021769 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.022590 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.113345 47485139399552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.114280 47488451916672 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0ycwd9ko
W0618 11:58:30.114335 47485139399552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcf0y2y9n
I0618 11:58:30.115306 47488451916672 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0ycwd9ko', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b310ce50e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.115339 47485139399552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcf0y2y9n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3047740e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.115724 47488451916672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.115749 47485139399552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.120929 47485139399552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.120949 47488451916672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.140429 47488451916672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.140433 47485139399552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880710.065905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.066811 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.067634 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.160925 48002219512704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.071949 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.072695 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.073428 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.161047 47145296802688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.162075 48002219512704 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmo1o4gzu
W0618 11:58:30.162133 47145296802688 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsqmeksm9
I0618 11:58:30.163191 48002219512704 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmo1o4gzu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba8abd46e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.163272 47145296802688 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsqmeksm9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae127418e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.163627 48002219512704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.163723 47145296802688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.168891 48002219512704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.169038 47145296802688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880710.079333 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.080227 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.081051 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.176219 47410502652800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.079215 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.080062 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.080901 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.176403 47377841316736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.177290 47410502652800 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpv6145kvz
W0618 11:58:30.177422 47377841316736 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjmswk7e5
I0618 11:58:30.178287 47410502652800 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpv6145kvz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ee6c1ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.178411 47377841316736 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjmswk7e5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b174bfd2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.178674 47410502652800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.178800 47377841316736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.183401 47410502652800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.183510 47377841316736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880710.126307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.126786 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.127207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.187096 47092104872832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.130642 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.131086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.131467 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.188417 46981751300992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.188103 47092104872832 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2p6m0wv2
W0618 11:58:30.188866 47488451916672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.189007 47485139399552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:58:30.189107 47092104872832 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2p6m0wv2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad4c4c4fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.189520 47092104872832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.189406 46981751300992 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_cqgo9em
I0618 11:58:30.190416 46981751300992 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_cqgo9em', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb132f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.190810 46981751300992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.190960 48002219512704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.191379 47145296802688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.193189 47488451916672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.193331 47485139399552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.194178 47092104872832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.195286 46981751300992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:30.198243 47488451916672 estimator.py:1111] Calling model_fn.
W0618 11:58:30.198354 47488451916672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:30.198387 47485139399552 estimator.py:1111] Calling model_fn.
W0618 11:58:30.198498 47485139399552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.199720 47488451916672 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.199871 47485139399552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.202927 47410502652800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.203089 47377841316736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.213446 47092104872832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.214602 46981751300992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880710.145480 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.146033 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.146519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.218515 47186063012736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.152518 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.152984 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.153378 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.218608 47367739175808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.155822 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.156204 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.156523 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.218657 47523634262912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.158223 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.158594 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.158969 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.218991 47630276916096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.219575 47186063012736 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp3xux2t2g
W0618 11:58:30.219640 47367739175808 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq75w2nik
I0618 11:58:30.220607 47186063012736 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp3xux2t2g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeaa51c8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:30.219649 47523634262912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjlqafn54
I0618 11:58:30.220687 47367739175808 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq75w2nik', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14f1dabda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.220674 47523634262912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjlqafn54', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b393decfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.220999 47186063012736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.219985 47630276916096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe8o9iwpz
I0618 11:58:30.221085 47367739175808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.221025 47630276916096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe8o9iwpz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b521252ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.221101 47523634262912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.221467 47630276916096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880710.149604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.150381 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.151216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.222889 47551300412288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.133971 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.134874 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.135775 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.223063 47492837008256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.223971 47551300412288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplqhn14tg
W0618 11:58:30.224046 47492837008256 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxeao_3pj
I0618 11:58:30.224981 47551300412288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplqhn14tg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3faef4ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.225033 47492837008256 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxeao_3pj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3212441da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:30.225701 47186063012736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:30.225425 47551300412288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.225782 47367739175808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:30.225480 47492837008256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.225780 47523634262912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.226077 47630276916096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.230520 47551300412288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.230508 47492837008256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.243163 47145296802688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.243220 48002219512704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.245107 47186063012736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.245171 47367739175808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.245078 47523634262912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.245350 47630276916096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.247804 47145296802688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.247868 48002219512704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.250236 47492837008256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.250683 47410502652800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.250450 47551300412288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.251084 47377841316736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:58:30.253255 47145296802688 estimator.py:1111] Calling model_fn.
I0618 11:58:30.253301 48002219512704 estimator.py:1111] Calling model_fn.
W0618 11:58:30.253369 47145296802688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.253415 48002219512704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.254993 47410502652800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.254827 47145296802688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.254860 48002219512704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.255433 47377841316736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:58:30.260037 47410502652800 estimator.py:1111] Calling model_fn.
W0618 11:58:30.260147 47410502652800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:30.260521 47377841316736 estimator.py:1111] Calling model_fn.
W0618 11:58:30.260628 47377841316736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.260634 47092104872832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.261503 47410502652800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880710.171788 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.172696 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.173427 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.260704 47176218575744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.171070 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.171934 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.172771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.260711 47790889403264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.262009 47377841316736 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.261974 46981751300992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:58:30.261801 47176218575744 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae85a565d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:30.261789 47790889403264 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpewbyxkuy
I0618 11:58:30.262812 47790889403264 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpewbyxkuy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b777792cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.262995 47176218575744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.263232 47790889403264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.264940 47092104872832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.266320 46981751300992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.268405 47790889403264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.268492 47176218575744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:58:30.269973 47092104872832 estimator.py:1111] Calling model_fn.
W0618 11:58:30.270082 47092104872832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:30.271382 46981751300992 estimator.py:1111] Calling model_fn.
W0618 11:58:30.271425 47092104872832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:58:30.271492 46981751300992 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.272835 46981751300992 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880710.222190 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.222656 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.223038 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.279833 46943479833472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.280941 46943479833472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp3wt2dpe7
I0618 11:58:30.282027 46943479833472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp3wt2dpe7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab22a06fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.282449 46943479833472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880710.230821 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.231274 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.231672 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.286060 47220018160512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.185426 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.186382 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.187295 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.285937 47793218597760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
:::MLL 1560880710.190337 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880710.191079 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880710.191801 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:58:30.285911 47834345878400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000004-000001.tfrecord.zz_0_0
W0618 11:58:30.287149 46943479833472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.287043 47220018160512 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2l55ko31
I0618 11:58:30.288012 47220018160512 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2l55ko31', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af28cfefda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:58:30.286941 47793218597760 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplchfbbcz
W0618 11:58:30.286910 47834345878400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmps44r9w51
I0618 11:58:30.287919 47834345878400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmps44r9w51', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8195c80e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.287957 47793218597760 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplchfbbcz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7802678e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:58:30.288414 47220018160512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.288361 47834345878400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:58:30.288407 47793218597760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:58:30.290672 47790889403264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.291576 47176218575744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:58:30.292485 47186063012736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.292401 47630276916096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.292804 47367739175808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.292481 47523634262912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.292979 47220018160512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.293177 47834345878400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.293200 47793218597760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:58:30.296796 47186063012736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.296700 47630276916096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.297126 47367739175808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.296799 47523634262912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:58:30.298371 47492837008256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:58:30.299045 47551300412288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 11:58:30.301845 47186063012736 estimator.py:1111] Calling model_fn.
W0618 11:58:30.301954 47186063012736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:30.301733 47630276916096 estimator.py:1111] Calling model_fn.
W0618 11:58:30.301841 47630276916096 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:58:30.301827 47523634262912 estimator.py:1111] Calling model_fn.
I0618 11:58:30.302187 47367739175808 estimator.py:1111] Calling model_fn.
W0618 11:58:30.301930 47523634262912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:58:30.302298 47367739175808 deprecation.py:323] From /global/panfs01/users/gma/sub[2019-06-18 11:59:09] divide_golden_chunk finished: 3.317 seconds
[2019-06-18 11:59:09] generate golden chunk: 3.331 seconds
[2019-06-18 11:59:09] moving /lfs/lfs12/gma_akey/results/epb134/models/000014-000009.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000014-000010.data-00000-of-00001
[2019-06-18 11:59:09] moving /lfs/lfs12/gma_akey/results/epb134/models/000014-000009.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb
[2019-06-18 11:59:09] moving /lfs/lfs12/gma_akey/results/epb134/models/000014-000009.index --> /lfs/lfs12/gma_akey/results/epb134/models/000014-000010.index
[2019-06-18 11:59:09] moving /lfs/lfs12/gma_akey/results/epb134/models/000014-000009.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000014-000010.meta
[2019-06-18 11:59:09] iteration time 13: 48.543 seconds
2019-06-18 11:59:10.092025: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880749.194015 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 11:59:13] minmax time: 3.237 seconds
2019-06-18 11:59:13.338522: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:59:13.345299: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:59:13.350339: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880753.361191 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 13}}
[2019-06-18 11:59:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:59:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=15 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=1023779846 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=2047559677 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=3071339508 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=4095119339 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=5118899170 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=6142679001 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=7166458832 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=8190238663 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=9214018494 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=10237798325 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=11261578156 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=12285357987 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=13309137818 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=14332917649 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=15356697480 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=16380477311 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=17404257142 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=18428036973 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=19451816804 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000014-000010 --seed=20475596635 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 11:59:25] eval finished: 12.220 seconds
[2019-06-18 11:59:25] Win rate 000014-000010 vs 000013-000009: 0.380
:::MLL 1560880765.656106 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 11:59:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=16 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=1023779847 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=2047559678 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=3071339509 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=4095119340 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=5118899171 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=6142679002 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=7166458833 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=8190238664 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=9214018495 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=10237798326 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=11261578157 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=12285357988 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=13309137819 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=14332917650 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=15356697481 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=16380477312 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=17404257143 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000015-000009 --seed=18428036974 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 11:59:55] selfplay finished: 30.300 seconds
[2019-06-18 11:59:55] selfplay mn: 30.320 seconds
[2019-06-18 11:59:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-16-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779847 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559678 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339509 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119340 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899171 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679002 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458833 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238664 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018495 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798326 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578157 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357988 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137819 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917650 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697481 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477312 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257143 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036974 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816805 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596636 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376467 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156298 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 11:59:56] train finished: 43.621 seconds
:::MLL 1560880758.602865 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.603733 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.604555 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.693631 47288338617216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.694652 47288338617216 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvnqdutkc
I0618 11:59:18.695663 47288338617216 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvnqdutkc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0275366e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880758.631849 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.632648 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.633389 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.695729 47436868391808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
I0618 11:59:18.696076 47288338617216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.696717 47436868391808 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4q_jvyoi
I0618 11:59:18.697727 47436868391808 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4q_jvyoi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b250a46ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.698132 47436868391808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.701054 47288338617216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.703083 47436868391808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.720319 47288338617216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.722594 47436868391808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880758.668520 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.669450 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.670311 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.760603 46979098448768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.675052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.675743 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.676402 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.760786 47082851521408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.761652 46979098448768 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz1yomi9a
W0618 11:59:18.761828 47082851521408 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2qsrnvb8
I0618 11:59:18.762714 46979098448768 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz1yomi9a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba750feda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.762890 47082851521408 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2qsrnvb8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad29d3a2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.763147 46979098448768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.763314 47082851521408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.768033 46979098448768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.768141 47082851521408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.776111 47288338617216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.776477 47436868391808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880758.693889 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.694595 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.695328 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.779109 47505828524928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.689451 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.690342 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.691162 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.779314 47579096097664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.780134 47505828524928 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp60ih6i7i
W0618 11:59:18.780291 47579096097664 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj_g_ngrp
I0618 11:59:18.781141 47505828524928 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp60ih6i7i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b35189f0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:18.780850 47288338617216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:18.781280 47579096097664 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj_g_ngrp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4627b57da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:18.781211 47436868391808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:18.781536 47505828524928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.781676 47579096097664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880758.720442 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.720873 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.721202 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.783667 48011772228480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.718722 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.719105 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.719437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.783748 47263764349824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.784730 47263764349824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9omvkq9j
W0618 11:59:18.784681 48011772228480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp80keiplk
I0618 11:59:18.785665 48011772228480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp80keiplk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baae5374e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.785689 47263764349824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9omvkq9j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afcbc78ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.785892 47288338617216 estimator.py:1111] Calling model_fn.
W0618 11:59:18.786006 47288338617216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:18.786055 48011772228480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.786076 47263764349824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.786378 47505828524928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.786401 47579096097664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:18.786288 47436868391808 estimator.py:1111] Calling model_fn.
W0618 11:59:18.786400 47436868391808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:18.787678 46979098448768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.787410 47288338617216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.787959 47082851521408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.787763 47436868391808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.790715 47263764349824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.790747 48011772228480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.805785 47505828524928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.805951 47579096097664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.810022 47263764349824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.810242 48011772228480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880758.749862 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.750351 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.750777 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.819460 47118765437824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.730687 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.731600 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.732472 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.819467 47092594877312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.735766 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.736483 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.737146 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.819539 47849363260288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.820516 47118765437824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwfny7jto
I0618 11:59:18.821468 47118765437824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwfny7jto', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adaf9dd0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.821864 47118765437824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.820607 47092594877312 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad4e1f9fd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:18.820643 47849363260288 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpy5zm46xp
I0618 11:59:18.821726 47849363260288 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpy5zm46xp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8514e31e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.821843 47092594877312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.822178 47849363260288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880758.764239 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.764724 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.765139 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.823205 47591665132416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.824205 47591665132416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqhoff5dt
I0618 11:59:18.825178 47591665132416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqhoff5dt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4914e18e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.825565 47591665132416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.826604 47118765437824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.827145 47092594877312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.827316 47849363260288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880758.769883 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.770377 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.770803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.829174 47876186706816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.830128 47591665132416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.830232 47876186706816 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnrf15hr4
I0618 11:59:18.831313 47876186706816 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnrf15hr4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b53b06e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.831755 47876186706816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880758.741873 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.742839 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.743747 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.832293 47832646009728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.759760 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.760566 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.761346 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.832409 47822987690880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.833328 47832646009728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwq7qde6g
W0618 11:59:18.833404 47822987690880 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpug7tzvq_
I0618 11:59:18.834339 47832646009728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwq7qde6g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8130761e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.834401 47822987690880 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpug7tzvq_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ef0c7ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.834743 47832646009728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:18.834789 47822987690880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.836088 46979098448768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880758.778410 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.778861 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.779242 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.836017 47943449731968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.836489 47082851521408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.836640 47876186706816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.837025 47943449731968 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpy5otvrgb
I0618 11:59:18.838007 47943449731968 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpy5otvrgb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9afce0ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.838411 47943449731968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.839638 47832646009728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.839642 47822987690880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.840385 46979098448768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.840800 47082851521408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.842937 47943449731968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:18.845441 46979098448768 estimator.py:1111] Calling model_fn.
W0618 11:59:18.845552 46979098448768 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:18.845863 47082851521408 estimator.py:1111] Calling model_fn.
W0618 11:59:18.845976 47082851521408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:18.846108 47118765437824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.846910 46979098448768 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.847343 47082851521408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.849454 47591665132416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.849228 47092594877312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.849580 47849363260288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880758.799928 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.800442 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.800890 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.851928 47496854791040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.853492 47505828524928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.853769 47579096097664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.852992 47496854791040 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp75kuiyle
I0618 11:59:18.854068 47496854791040 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp75kuiyle', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3301beae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.854497 47496854791040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.856001 47876186706816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.857233 47263764349824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.857819 47505828524928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.858083 47579096097664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.858101 48011772228480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.859270 47822987690880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.859261 47832646009728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.859714 47496854791040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.861523 47263764349824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.862546 47943449731968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.862465 48011772228480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:18.862906 47505828524928 estimator.py:1111] Calling model_fn.
W0618 11:59:18.863013 47505828524928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:18.863132 47579096097664 estimator.py:1111] Calling model_fn.
W0618 11:59:18.863243 47579096097664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:18.864382 47505828524928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.864602 47579096097664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:18.866563 47263764349824 estimator.py:1111] Calling model_fn.
W0618 11:59:18.866670 47263764349824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:18.867576 48011772228480 estimator.py:1111] Calling model_fn.
W0618 11:59:18.867687 48011772228480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:18.868029 47263764349824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:18.869035 48011772228480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880758.817040 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.817506 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.818023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.873342 47309719978880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.874310 47309719978880 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_ezxcok9
I0618 11:59:18.875279 47309719978880 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_ezxcok9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b076fa41dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:18.875680 47309719978880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:18.880296 47309719978880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:18.880372 47496854791040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:18.893716 47118765437824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:18.896509 47591665132416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880758.832982 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.833485 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.833887 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.897194 47199391814528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560880758.833775 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880758.834205 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880758.834617 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:18.897218 47494903894912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 11:59:18.898039 47118765437824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:18.898177 47199391814528 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpu94bqvyj
W0618 11:59:18.898210 47494903894912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpv61qi6_1
I0618 11:59:18.899136 47199391814528 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpu94bqvyj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aedbf91ee10>, '_task_type': 'worker', '_task_id': 0, '_globa[2019-06-18 11:59:59] divide_golden_chunk finished: 3.322 seconds
[2019-06-18 11:59:59] generate golden chunk: 3.336 seconds
[2019-06-18 11:59:59] iteration time 14: 50.121 seconds
2019-06-18 12:00:00.235947: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880799.314767 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:00:03] minmax time: 3.236 seconds
2019-06-18 12:00:03.481952: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:00:03.487205: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:00:03.491821: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880803.503943 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 14}}
[2019-06-18 12:00:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000016-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:00:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=16 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=1023779847 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=2047559678 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=3071339509 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=4095119340 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=5118899171 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=6142679002 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=7166458833 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=8190238664 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=9214018495 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=10237798326 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=11261578157 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=12285357988 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=13309137819 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=14332917650 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=15356697481 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=16380477312 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=17404257143 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=18428036974 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=19451816805 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000015-000010 --seed=20475596636 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:00:13] eval finished: 9.951 seconds
[2019-06-18 12:00:13] Win rate 000015-000010 vs 000013-000009: 0.790
:::MLL 1560880813.528143 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:00:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=17 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=1023779848 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=2047559679 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=3071339510 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=4095119341 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=5118899172 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=6142679003 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=7166458834 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=8190238665 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=9214018496 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=10237798327 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=11261578158 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=12285357989 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=13309137820 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=14332917651 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=15356697482 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=16380477313 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=17404257144 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000016-000009 --seed=18428036975 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:00:44] selfplay finished: 31.234 seconds
[2019-06-18 12:00:44] selfplay mn: 31.251 seconds
[2019-06-18 12:00:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-17-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779848 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559679 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339510 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119341 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899172 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679003 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458834 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238665 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018496 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798327 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578158 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357989 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137820 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917651 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697482 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477313 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257144 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036975 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816806 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596637 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376468 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156299 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000016-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:00:46] train finished: 43.324 seconds
:::MLL 1560880808.794491 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.795226 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.795911 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.874423 47361230455680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.783009 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.783879 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.784711 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.874571 47183254922112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.875489 47361230455680 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9uaao1bj
W0618 12:00:08.875614 47183254922112 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvrnqojqr
I0618 12:00:08.876523 47361230455680 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9uaao1bj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b136de76e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.876620 47183254922112 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvrnqojqr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9fdbc6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.876927 47361230455680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.877015 47183254922112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.882031 47361230455680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.882040 47183254922112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.902264 47183254922112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.902384 47361230455680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880808.813139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.814003 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.814795 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.905876 47677988938624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.813487 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.814355 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.815111 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.905910 47375408911232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.906945 47375408911232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpn2uwc6bi
W0618 12:00:08.906920 47677988938624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuk847s_k
I0618 12:00:08.907905 47677988938624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuk847s_k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5d2e2e7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.907916 47375408911232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpn2uwc6bi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b16bb01ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.908304 47677988938624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.908313 47375408911232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.913263 47375408911232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.913235 47677988938624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.932828 47375408911232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.932823 47677988938624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880808.856195 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.856896 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.857534 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.943879 47618468758400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.852309 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.853215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.853996 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.943894 46998869246848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.944960 47618468758400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4dsv3owe
W0618 12:00:08.944931 46998869246848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2alqg_pv
I0618 12:00:08.945898 46998869246848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2alqg_pv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abf0f7e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.945954 47618468758400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4dsv3owe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f52809e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.946337 46998869246848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.946378 47618468758400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.951135 46998869246848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.951167 47618468758400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.955680 47361230455680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:08.955799 47183254922112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:08.960192 47361230455680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:08.960331 47183254922112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880808.900197 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.900640 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.901194 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.962842 47220139041664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.884106 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.884487 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.884847 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.962979 47763244245888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.884987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.885360 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.885682 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.963006 47041249551232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.963851 47220139041664 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmcl4p7li
I0618 12:00:08.964882 47220139041664 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmcl4p7li', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af294337e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:08.964105 47763244245888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpgglwqpla
I0618 12:00:08.965315 47220139041664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.964069 47041249551232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmppg_ekrmy
I0618 12:00:08.965175 47041249551232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmppg_ekrmy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8ed8e7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.965205 47763244245888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpgglwqpla', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7107cb3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.965574 47041249551232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.965610 47763244245888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.965730 47361230455680 estimator.py:1111] Calling model_fn.
W0618 12:00:08.965841 47361230455680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:08.965818 47183254922112 estimator.py:1111] Calling model_fn.
W0618 12:00:08.965929 47183254922112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880808.905862 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.906338 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.906738 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.966840 47945483572096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.967221 47361230455680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:08.967294 47183254922112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:08.967807 47945483572096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj_h_mvnc
I0618 12:00:08.968798 47945483572096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj_h_mvnc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b761a9da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.969188 47945483572096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.969974 47220139041664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.970313 46998869246848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.970514 47618468758400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.970215 47763244245888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.970235 47041249551232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.973716 47945483572096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880808.882262 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.883183 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.884006 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.974071 47580001469312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.882650 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.883534 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.884318 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.974128 47168985461632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.975114 47168985461632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpx3jmysgt
W0618 12:00:08.975083 47580001469312 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmppx351dtb
I0618 12:00:08.976107 47168985461632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpx3jmysgt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae6ab35ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.976098 47580001469312 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmppx351dtb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b465dac4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.976505 47168985461632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.976507 47580001469312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:08.981164 47677988938624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:08.981811 47375408911232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:08.981465 47580001469312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.981545 47168985461632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:08.985426 47677988938624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:08.986144 47375408911232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:08.989152 47220139041664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.989300 47763244245888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:08.989591 47041249551232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:08.990483 47677988938624 estimator.py:1111] Calling model_fn.
W0618 12:00:08.990588 47677988938624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:08.991250 47375408911232 estimator.py:1111] Calling model_fn.
W0618 12:00:08.991359 47375408911232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:08.992002 47677988938624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:08.992880 47375408911232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:08.993147 47945483572096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880808.931746 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.932180 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.932502 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.995219 47409880761216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.933230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.933598 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.933928 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:08.995272 47934792893312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:08.996219 47409880761216 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpddue9ecv
W0618 12:00:08.996267 47934792893312 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnf60_2_5
I0618 12:00:08.997194 47409880761216 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpddue9ecv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ec1b05e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.997250 47934792893312 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnf60_2_5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98f8e3de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:08.997587 47409880761216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:08.997643 47934792893312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:09.000964 47580001469312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.001059 47168985461632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.002261 47934792893312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:09.002226 47409880761216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:09.018217 46998869246848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.018813 47618468758400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880808.956799 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.957299 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.957726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:09.020749 47467049399168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:09.021430 47409880761216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.021595 47934792893312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.022509 46998869246848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:09.021751 47467049399168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1k7a45k4
I0618 12:00:09.022739 47467049399168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1k7a45k4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c11348e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880808.959793 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.960239 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.960629 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:09.022621 47388659225472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:00:09.023140 47618468758400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:09.023137 47467049399168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:09.023582 47388659225472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq0g8l0_j
I0618 12:00:09.024551 47388659225472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq0g8l0_j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b19d0c96e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:09.024939 47388659225472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880808.932934 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.933780 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.934601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:09.025460 47797117514624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560880808.934412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880808.935243 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880808.936030 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:09.025491 47556728230784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000006-000003.tfrecord.zz_0_0
I0618 12:00:09.027483 46998869246848 estimator.py:1111] Calling model_fn.
W0618 12:00:09.027603 46998869246848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:09.027764 47467049399168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:00:09.028188 47618468758400 estimator.py:1111] Calling model_fn.
W0618 12:00:09.026583 47556728230784 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp65dfjq17
I0618 12:00:09.026597 47797117514624 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78eacc2cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:09.028299 47618468758400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:09.027668 47556728230784 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp65dfjq17', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40f27aee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:09.027831 47797117514624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:09.028102 47556728230784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:09.028961 46998869246848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:09.029647 47618468758400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:09.029423 47388659225472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:09.033054 47797117514624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:09.033215 47556728230784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:09.036637 47220139041664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.036742 47763244245888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.036858 47041249551232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.040515 47945483572096 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.040938 47220139041664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:09.041035 47763244245888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:09.041160 47041249551232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:09.044843 47945483572096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:09.045982 47220139041664 estimator.py:1111] Calling model_fn.
W0618 12:00:09.046094 47220139041664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:09.046070 47763244245888 estimator.py:1111] Calling model_fn.
W0618 12:00:09.046189 47763244245888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:09.046206 47041249551232 estimator.py:1111] Calling model_fn.
W0618 12:00:09.046314 47041249551232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:09.047448 47220139041664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:09.047391 47467049399168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.047545 47763244245888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:09.047678 47041249551232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:09.048966 47388659225472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:09.049421 47580001469312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:09.049614 47168985461632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an[2019-06-18 12:00:48] divide_golden_chunk finished: 3.333 seconds
[2019-06-18 12:00:48] generate golden chunk: 3.346 seconds
[2019-06-18 12:00:48] moving /lfs/lfs12/gma_akey/results/epb134/models/000016-000010.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000016-000011.meta
[2019-06-18 12:00:48] moving /lfs/lfs12/gma_akey/results/epb134/models/000016-000010.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000016-000011.data-00000-of-00001
[2019-06-18 12:00:48] moving /lfs/lfs12/gma_akey/results/epb134/models/000016-000010.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb
[2019-06-18 12:00:48] moving /lfs/lfs12/gma_akey/results/epb134/models/000016-000010.index --> /lfs/lfs12/gma_akey/results/epb134/models/000016-000011.index
[2019-06-18 12:00:48] iteration time 15: 48.849 seconds
2019-06-18 12:00:49.151698: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880848.164007 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:00:52] minmax time: 3.273 seconds
2019-06-18 12:00:52.435072: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:00:52.440454: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:00:52.444999: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880852.455515 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 15}}
[2019-06-18 12:00:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:00:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=17 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=1023779848 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=2047559679 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=3071339510 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=4095119341 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=5118899172 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=6142679003 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=7166458834 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=8190238665 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=9214018496 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=10237798327 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=11261578158 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=12285357989 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=13309137820 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=14332917651 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=15356697482 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=16380477313 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=17404257144 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=18428036975 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=19451816806 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000016-000011 --seed=20475596637 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:01:03] eval finished: 10.718 seconds
[2019-06-18 12:01:03] Win rate 000016-000011 vs 000015-000010: 0.480
:::MLL 1560880863.246052 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:01:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=18 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=1023779849 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=2047559680 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=3071339511 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=4095119342 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=5118899173 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=6142679004 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=7166458835 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=8190238666 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=9214018497 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=10237798328 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=11261578159 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=12285357990 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=13309137821 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=14332917652 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=15356697483 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=16380477314 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=17404257145 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000017-000010 --seed=18428036976 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:01:33] selfplay finished: 29.880 seconds
[2019-06-18 12:01:33] selfplay mn: 29.897 seconds
[2019-06-18 12:01:33] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-18-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779849 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559680 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339511 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119342 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899173 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679004 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458835 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238666 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018497 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798328 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578159 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357990 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137821 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917652 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697483 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477314 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257145 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036976 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816807 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596638 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376469 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156300 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:01:36] train finished: 43.666 seconds
:::MLL 1560880857.735139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.735844 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.736487 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.821628 47608543802240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.726414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.727252 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.728071 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.821627 47427511710592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.822651 47608543802240 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkkgizwkt
W0618 12:00:57.822677 47427511710592 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl1yzm6sy
I0618 12:00:57.823693 47608543802240 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkkgizwkt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d02edce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.823703 47427511710592 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl1yzm6sy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b22dc934dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.824097 47608543802240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.824108 47427511710592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.828941 47427511710592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.828958 47608543802240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.849954 47427511710592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.850031 47608543802240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880857.787522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.788440 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.789311 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.880333 47118369170304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.807589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.808477 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.809301 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.880993 47543568970624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.881341 47118369170304 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbb37ad1e
I0618 12:00:57.882335 47118369170304 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbb37ad1e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adae23e7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:57.881969 47543568970624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplj74js4m
I0618 12:00:57.882743 47118369170304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.882946 47543568970624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplj74js4m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3de2207e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.883344 47543568970624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.887676 47118369170304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.888170 47543568970624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880857.808529 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.809379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.810179 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.899688 47631874888576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.809772 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.810597 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.811313 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.899732 47960562570112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.900705 47631874888576 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcowt5pt1
W0618 12:00:57.900730 47960562570112 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_rn7j9wk
I0618 12:00:57.901774 47631874888576 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcowt5pt1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b527191ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.901774 47960562570112 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_rn7j9wk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ef8e1ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.902209 47631874888576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.902217 47960562570112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880857.828014 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.828473 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.828828 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.902682 47155358634880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.825189 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.825628 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.825969 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.902665 47642973852544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.903006 47427511710592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.903320 47608543802240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.903730 47155358634880 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptyib3n0n
W0618 12:00:57.903759 47642973852544 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpswho1p7c
I0618 12:00:57.904723 47155358634880 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptyib3n0n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae37efcee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.904724 47642973852544 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpswho1p7c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55071e9dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.905117 47155358634880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.905122 47642973852544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.906951 47631874888576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.907007 47960562570112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.907208 47118369170304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.907409 47427511710592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.907934 47543568970624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.907757 47608543802240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.909745 47155358634880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.909722 47642973852544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880857.826316 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.827059 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.827700 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.912238 47908543812480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.818401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.819287 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.820083 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.912217 47205620376448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
I0618 12:00:57.912552 47427511710592 estimator.py:1111] Calling model_fn.
W0618 12:00:57.912664 47427511710592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:57.912940 47608543802240 estimator.py:1111] Calling model_fn.
W0618 12:00:57.913050 47608543802240 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.914009 47427511710592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.913286 47205620376448 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpudth6afe
W0618 12:00:57.913315 47908543812480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9rub51vj
I0618 12:00:57.914314 47205620376448 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpudth6afe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aef32d23da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.914334 47908543812480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9rub51vj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b92dc52ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:57.914408 47608543802240 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:00:57.914726 47205620376448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.914735 47908543812480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.919793 47205620376448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.919801 47908543812480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.926191 47631874888576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.926915 47960562570112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.928891 47642973852544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.928965 47155358634880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880857.849063 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.849770 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.850430 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.931353 47162169344896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.838959 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.839844 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.840693 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.931402 47765627593600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
I0618 12:00:57.932465 47162169344896 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae514efed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:57.932453 47765627593600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmr09xaxp
I0618 12:00:57.933491 47765627593600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmr09xaxp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7195da2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.933664 47162169344896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.933900 47765627593600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.938552 47162169344896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.938614 47765627593600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.939155 47908543812480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.939195 47205620376448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880857.887326 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.887765 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.888143 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.955444 47726271640448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.955604 47118369170304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.955898 47543568970624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880857.889637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.890070 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.890450 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.956084 47607375692672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.956489 47726271640448 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp49dvmn1i
I0618 12:00:57.957494 47726271640448 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp49dvmn1i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b686c0e0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:57.957111 47607375692672 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6ld8u4_y
I0618 12:00:57.957893 47726271640448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.958094 47607375692672 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6ld8u4_y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4cbd4dde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880857.890286 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.890671 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.890991 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.957980 46974630495104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
I0618 12:00:57.958481 47607375692672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880857.891586 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.891992 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.892344 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:57.958668 47940652852096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:57.959044 46974630495104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpit_r84xj
W0618 12:00:57.959889 47118369170304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:57.960081 46974630495104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpit_r84xj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab96ac05e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:57.960190 47543568970624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.959679 47940652852096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpahb1rxxb
I0618 12:00:57.960504 46974630495104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:57.960713 47940652852096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpahb1rxxb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a562b9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:57.961135 47940652852096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:57.960329 47765627593600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.960365 47162169344896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.962458 47726271640448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.963000 47607375692672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:00:57.964940 47118369170304 estimator.py:1111] Calling model_fn.
W0618 12:00:57.965050 47118369170304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.965118 46974630495104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:00:57.965246 47543568970624 estimator.py:1111] Calling model_fn.
W0618 12:00:57.965355 47543568970624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.965605 47940652852096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:57.966417 47118369170304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.966714 47543568970624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.973993 47631874888576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.974438 47960562570112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.976415 47642973852544 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.976595 47155358634880 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.978278 47631874888576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.978739 47960562570112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.980708 47642973852544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.980895 47155358634880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.981572 47726271640448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.982131 47607375692672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:57.983343 47631874888576 estimator.py:1111] Calling model_fn.
W0618 12:00:57.983455 47631874888576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:57.983779 47960562570112 estimator.py:1111] Calling model_fn.
W0618 12:00:57.983894 47960562570112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.984315 46974630495104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.984814 47631874888576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.985013 47940652852096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:57.985250 47960562570112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:00:57.985748 47642973852544 estimator.py:1111] Calling model_fn.
W0618 12:00:57.985858 47642973852544 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:57.985954 47155358634880 estimator.py:1111] Calling model_fn.
W0618 12:00:57.986060 47155358634880 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.987218 47642973852544 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.987424 47155358634880 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.987809 47908543812480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.987917 47205620376448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:57.992600 47908543812480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:57.992613 47205620376448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:57.997820 47908543812480 estimator.py:1111] Calling model_fn.
I0618 12:00:57.997864 47205620376448 estimator.py:1111] Calling model_fn.
W0618 12:00:57.997932 47908543812480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.997974 47205620376448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:57.999421 47205620376448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:57.999428 47908543812480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880857.932522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.932929 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.933274 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:58.003912 47867463844736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
:::MLL 1560880857.932297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.932715 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.933089 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:58.004005 47213909296000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
W0618 12:00:58.004914 47867463844736 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpdlwi0boz
W0618 12:00:58.004958 47213909296000 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2yiwy9e6
:::MLL 1560880857.938262 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880857.938696 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880857.939070 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:58.004964 47862413575040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000007-000004.tfrecord.zz_0_0
I0618 12:00:58.005885 47867463844736 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpdlwi0boz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b894bc40e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:58.005923 47213909296000 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2yiwy9e6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af120e11e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:58.006288 47867463844736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:58.006330 47213909296000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:58.006026 47862413575040 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp44mzr862
I0618 12:00:58.007001 47862413575040 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp44mzr862', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tenso[2019-06-18 12:01:36] divide_golden_chunk finished: 3.319 seconds
[2019-06-18 12:01:36] generate golden chunk: 3.333 seconds
[2019-06-18 12:01:36] iteration time 16: 48.313 seconds
2019-06-18 12:01:37.467281: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
Got 343572 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000009.tfrecord.zz: 13.306 seconds
Got 377794 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000004.tfrecord.zz: 14.762 seconds
Got 380569 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000005.tfrecord.zz: 14.098 seconds
Got 347710 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz: 0.312 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000006.tfrecord.zz: 14.466 seconds
Got 391566 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000000.tfrecord.zz: 14.198 seconds
Got 383396 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000002.tfrecord.zz: 12.511 seconds
Got 347701 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000007.tfrecord.zz: 13.662 seconds
Got 346317 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz: 0.308 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000008.tfrecord.zz: 13.810 seconds
Got 389066 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz: 0.317 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000003.tfrecord.zz: 14.812 seconds
Got 388250 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000000-000001.tfrecord.zz: 15.526 seconds
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/checkpoint_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/checkpointlog.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000001-000001_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000001-000001log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000002-000002_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000002-000002log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000003-000002_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000003-000002log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000004-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000004-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000005-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000005-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000006-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000006-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000007-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000007-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000008-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000008-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000009-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000009-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000010-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000010-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000011-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000011-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000012-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000012-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000013-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000013-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000014-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000014-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000015-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000015-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000016-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000016-000011log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000017-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000017-000011log.txt['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880896.477394 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:01:40] minmax time: 3.245 seconds
2019-06-18 12:01:40.721802: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:01:40.726928: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:01:40.731391: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880900.743596 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 16}}
[2019-06-18 12:01:40] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000018-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:01:40] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=18 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=1023779849 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=2047559680 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=3071339511 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=4095119342 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=5118899173 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=6142679004 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=7166458835 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=8190238666 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=9214018497 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=10237798328 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=11261578159 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=12285357990 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=13309137821 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=14332917652 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=15356697483 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=16380477314 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=17404257145 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=18428036976 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=19451816807 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000017-000011 --seed=20475596638 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:01:51] eval finished: 10.999 seconds
[2019-06-18 12:01:51] Win rate 000017-000011 vs 000015-000010: 0.650
:::MLL 1560880911.814900 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:01:51] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=19 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=1023779850 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=2047559681 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=3071339512 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=4095119343 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=5118899174 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=6142679005 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=7166458836 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=8190238667 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=9214018498 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=10237798329 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=11261578160 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=12285357991 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=13309137822 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=14332917653 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=15356697484 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=16380477315 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=17404257146 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000018-000010 --seed=18428036977 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:02:22] selfplay finished: 30.250 seconds
[2019-06-18 12:02:22] selfplay mn: 30.266 seconds
[2019-06-18 12:02:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-19-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779850 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559681 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339512 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119343 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899174 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679005 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458836 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238667 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018498 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798329 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578160 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357991 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137822 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917653 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697484 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477315 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257146 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036977 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816808 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596639 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376470 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156301 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000018-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:02:24] train finished: 43.810 seconds
:::MLL 1560880905.992426 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880905.993119 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880905.993808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.086708 47920464327552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880905.986482 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880905.987333 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880905.988148 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.086881 47424279524224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.087837 47920464327552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpoo0pu5u7
W0618 12:01:46.087961 47424279524224 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa5pq3wia
I0618 12:01:46.088957 47920464327552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpoo0pu5u7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b95a2d73e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.089098 47424279524224 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa5pq3wia', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b221bec0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.089410 47920464327552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.089542 47424279524224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.094834 47920464327552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.094858 47424279524224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880906.012270 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.013033 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.013697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.102138 47593488860032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.006019 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.006951 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.007798 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.102327 47823649940352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.103219 47593488860032 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp12yozdu_
W0618 12:01:46.103425 47823649940352 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpd6se5ooj
I0618 12:01:46.104324 47593488860032 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp12yozdu_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4981958e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.104511 47823649940352 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpd6se5ooj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7f1840ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.104723 47593488860032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.104916 47823649940352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.109697 47593488860032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.109789 47823649940352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.116709 47920464327552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.117339 47424279524224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.128945 47593488860032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.129156 47823649940352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880906.041307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.042226 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.043023 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.136549 47960121987968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.041369 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.042298 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.043112 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.136611 47633400578944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.137720 47960121987968 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpboetm2of
W0618 12:01:46.137749 47633400578944 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9weofuqn
I0618 12:01:46.138810 47960121987968 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpboetm2of', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ede9f2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.138822 47633400578944 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9weofuqn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b52cc821e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.139216 47960121987968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.139221 47633400578944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.143966 47960121987968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.143987 47633400578944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880906.070810 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.071220 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.071657 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.143725 47054401811328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.071880 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.072292 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.072642 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.143961 47449619137408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.144742 47054401811328 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt76wpc2v
W0618 12:01:46.144939 47449619137408 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpv5097ogk
I0618 12:01:46.145744 47054401811328 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt76wpc2v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acbfd7e0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.145915 47449619137408 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpv5097ogk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b280247de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.146148 47054401811328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.146306 47449619137408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.150756 47054401811328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.150861 47449619137408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880906.088209 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.088706 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.089141 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.150953 47859984372608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.152041 47859984372608 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxzo2tzkh
I0618 12:01:46.153115 47859984372608 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxzo2tzkh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b878df45e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.153555 47859984372608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.158780 47859984372608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.163546 47960121987968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.163552 47633400578944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880906.104488 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.104956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.105391 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.165289 47139376731008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.166489 47920464327552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.166265 47139376731008 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe6b2cnmg
I0618 12:01:46.167245 47139376731008 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe6b2cnmg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adfc6645e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:46.167319 47424279524224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:01:46.167647 47139376731008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.169857 47054401811328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.169901 47449619137408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.170782 47920464327552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.171683 47424279524224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.172225 47139376731008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:46.175831 47920464327552 estimator.py:1111] Calling model_fn.
W0618 12:01:46.175939 47920464327552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:46.176890 47424279524224 estimator.py:1111] Calling model_fn.
W0618 12:01:46.177101 47593488860032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.177008 47424279524224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.177492 47823649940352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.177312 47920464327552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.178423 47859984372608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.178396 47424279524224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880906.117515 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.118067 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.118510 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.179723 47343608230784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.181393 47593488860032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.180879 47343608230784 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvy8vmeg4
W0618 12:01:46.181821 47823649940352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:46.181961 47343608230784 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvy8vmeg4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0f5389ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.182406 47343608230784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.186460 47593488860032 estimator.py:1111] Calling model_fn.
W0618 12:01:46.186568 47593488860032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:46.186912 47823649940352 estimator.py:1111] Calling model_fn.
W0618 12:01:46.187023 47823649940352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.187613 47343608230784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.187939 47593488860032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.188368 47823649940352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.191386 47139376731008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880906.132295 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.132744 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.133138 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.199263 46966913676160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.200230 46966913676160 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpww9rd2_8
I0618 12:01:46.201198 46966913676160 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpww9rd2_8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab79ecaee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.201594 46966913676160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880906.110584 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.111326 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.112031 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.203859 47051202929536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.108899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.109630 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.110423 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.203937 47014013113216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.204893 47051202929536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmporhqjl_6
W0618 12:01:46.204921 47014013113216 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_e1xtn_v
I0618 12:01:46.205888 47051202929536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmporhqjl_6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb3ed2ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.205925 47014013113216 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_e1xtn_v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac296236e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:46.206297 46966913676160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:46.206290 47051202929536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.206319 47014013113216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.207785 47343608230784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.211184 47014013113216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.211203 47051202929536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.211863 47960121987968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.212068 47633400578944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.216197 47960121987968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.216399 47633400578944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.217281 47054401811328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.217352 47449619137408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:01:46.221337 47960121987968 estimator.py:1111] Calling model_fn.
W0618 12:01:46.221448 47960121987968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:46.221547 47633400578944 estimator.py:1111] Calling model_fn.
W0618 12:01:46.221656 47633400578944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.221577 47054401811328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.221642 47449619137408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:46.222809 47960121987968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.223020 47633400578944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.225559 46966913676160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.225821 47859984372608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:01:46.226665 47054401811328 estimator.py:1111] Calling model_fn.
I0618 12:01:46.226721 47449619137408 estimator.py:1111] Calling model_fn.
W0618 12:01:46.226774 47054401811328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.226831 47449619137408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560880906.131095 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.131842 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.132588 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.227227 47837842514816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.133336 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.134080 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.134758 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.227353 47461482582912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.228124 47054401811328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.228202 47449619137408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:01:46.228357 47837842514816 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8266327d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:46.228558 47461482582912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmppormp_ti
W0618 12:01:46.230120 47859984372608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:46.229629 47837842514816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.229649 47461482582912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmppormp_ti', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ac565ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:46.230518 47051202929536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.230572 47014013113216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:46.230088 47461482582912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:46.235144 47859984372608 estimator.py:1111] Calling model_fn.
W0618 12:01:46.235254 47859984372608 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.234942 47837842514816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.235225 47461482582912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:46.236603 47859984372608 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.238552 47139376731008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:46.242857 47139376731008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:46.247926 47139376731008 estimator.py:1111] Calling model_fn.
W0618 12:01:46.248037 47139376731008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:46.249389 47139376731008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:46.255153 47343608230784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880906.175236 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.176020 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.176699 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.256361 47405080986496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
:::MLL 1560880906.163602 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880906.164589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880906.165448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:46.256580 47195804574592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000008-000004.tfrecord.zz_0_0
W0618 12:01:46.256853 47837842514816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.257263 47461482582912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:46.257379 47405080986496 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpecj8xb4l
W0618 12:01:46.257553 47195804574592 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp28wxlvf9
I0618 12:01:46.258386 47405080986496 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpecj8xb4l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1da3998e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.258559 47195804574592 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp28wxlvf9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aece9c0fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:46.258791 47405080986496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:46.259440 47343608230784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:46.258975 47195804574592 train.py:201] [2019-06-18 12:02:25] divide_golden_chunk finished: 3.421 seconds
[2019-06-18 12:02:25] generate golden chunk: 3.435 seconds
[2019-06-18 12:02:25] moving /lfs/lfs12/gma_akey/results/epb134/models/000018-000011.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000018-000012.meta
[2019-06-18 12:02:25] moving /lfs/lfs12/gma_akey/results/epb134/models/000018-000011.index --> /lfs/lfs12/gma_akey/results/epb134/models/000018-000012.index
[2019-06-18 12:02:25] moving /lfs/lfs12/gma_akey/results/epb134/models/000018-000011.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000018-000012.data-00000-of-00001
[2019-06-18 12:02:25] moving /lfs/lfs12/gma_akey/results/epb134/models/000018-000011.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb
[2019-06-18 12:02:25] iteration time 17: 49.087 seconds
2019-06-18 12:02:26.596852: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880945.564175 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:02:29] minmax time: 3.244 seconds
2019-06-18 12:02:29.851001: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:02:29.856330: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:02:29.860906: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880949.871608 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 17}}
[2019-06-18 12:02:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:02:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=19 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=1023779850 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=2047559681 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=3071339512 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=4095119343 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=5118899174 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=6142679005 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=7166458836 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=8190238667 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=9214018498 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=10237798329 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=11261578160 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=12285357991 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=13309137822 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=14332917653 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=15356697484 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=16380477315 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=17404257146 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=18428036977 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=19451816808 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000018-000012 --seed=20475596639 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:02:41] eval finished: 11.155 seconds
[2019-06-18 12:02:41] Win rate 000018-000012 vs 000017-000011: 0.430
:::MLL 1560880961.099073 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:02:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=20 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=1023779851 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=2047559682 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=3071339513 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=4095119344 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=5118899175 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=6142679006 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=7166458837 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=8190238668 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=9214018499 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=10237798330 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=11261578161 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=12285357992 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=13309137823 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=14332917654 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=15356697485 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=16380477316 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=17404257147 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000019-000011 --seed=18428036978 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:03:11] selfplay finished: 30.230 seconds
[2019-06-18 12:03:11] selfplay mn: 30.247 seconds
[2019-06-18 12:03:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-20-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779851 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559682 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339513 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119344 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899175 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679006 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458837 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238668 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018499 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798330 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578161 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357992 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137823 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917654 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697485 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477316 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257147 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036978 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816809 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596640 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376471 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156302 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000019-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:03:13] train finished: 43.691 seconds
:::MLL 1560880955.151655 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.152304 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.152967 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.243094 46969994150784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.146588 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.147496 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.148367 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.243048 48000975963008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.244112 46969994150784 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpoykn201i
W0618 12:02:35.244081 48000975963008 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk3xhrthw
I0618 12:02:35.245098 48000975963008 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk3xhrthw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba861b55e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.245131 46969994150784 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpoykn201i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab856675da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.245506 48000975963008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.245525 46969994150784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.250308 48000975963008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.250344 46969994150784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880955.175178 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.175932 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.176570 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.268977 47629534057344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.167724 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.168574 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.169414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.269203 46952379081600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.269991 48000975963008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.270158 46969994150784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.270000 47629534057344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpteb4f17i
W0618 12:02:35.270194 46952379081600 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzo083z38
:::MLL 1560880955.152387 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.153306 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.154135 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.270432 47947296002944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:02:35.271018 47629534057344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpteb4f17i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b51e60bae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.271186 46952379081600 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzo083z38', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab43c76be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.271417 47629534057344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.271578 46952379081600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880955.169260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.169930 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.170607 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.271392 47040460870528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.271594 47947296002944 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqt4tecux
I0618 12:02:35.272713 47947296002944 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqt4tecux', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9be2222e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.273174 47947296002944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.272480 47040460870528 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpm2o0a10z
I0618 12:02:35.273617 47040460870528 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpm2o0a10z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8be8c1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.274070 47040460870528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.276356 47629534057344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.276465 46952379081600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.278535 47947296002944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.279343 47040460870528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.295622 47629534057344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.295732 46952379081600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.300611 47947296002944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.301510 47040460870528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880955.219399 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.220028 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.220765 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.316749 47105579467648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.221482 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.222217 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.222936 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.316939 47584875869056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.220414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.220809 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.221159 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.317319 47349473551232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.317799 47105579467648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzn2zzxrg
W0618 12:02:35.318794 48000975963008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.317964 47584875869056 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnzg3d4ql
W0618 12:02:35.318852 46969994150784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:02:35.318799 47105579467648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzn2zzxrg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad7e7eb1da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880955.222970 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.223376 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.223723 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.318750 47109357519744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:02:35.318989 47584875869056 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnzg3d4ql', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b478035ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:35.318345 47349473551232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6722i5g6
I0618 12:02:35.319200 47105579467648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.319356 47349473551232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6722i5g6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b10b1237e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.319403 47584875869056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.319753 47349473551232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880955.242760 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.243309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.243800 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.320306 47113268593536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.247175 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.247652 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.248183 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.320405 47129542685568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.319756 47109357519744 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt7nbmus9
I0618 12:02:35.320731 47109357519744 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt7nbmus9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8c91b9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.321135 47109357519744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.321331 47113268593536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf_17hrxt
W0618 12:02:35.321397 47129542685568 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsrzevs_l
I0618 12:02:35.322306 47113268593536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf_17hrxt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad9b239de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.322376 47129542685568 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsrzevs_l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2add7c3cae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.322701 47113268593536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880955.249686 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.250193 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.250598 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.322425 47919771034496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
I0618 12:02:35.322769 47129542685568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880955.249675 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.250190 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.250604 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.322727 47896069104512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.323131 48000975963008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.323177 46969994150784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.323460 47919771034496 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmponqbj291
W0618 12:02:35.323994 47105579467648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.324132 47584875869056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.323705 47896069104512 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfvedh1tb
I0618 12:02:35.324444 47919771034496 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmponqbj291', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9579847e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.324694 47896069104512 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfvedh1tb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8ff4c5ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:35.324402 47349473551232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:35.324856 47919771034496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.325088 47896069104512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.325679 47109357519744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.327350 47129542685568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.327351 47113268593536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:35.328263 46969994150784 estimator.py:1111] Calling model_fn.
I0618 12:02:35.328271 48000975963008 estimator.py:1111] Calling model_fn.
W0618 12:02:35.328373 46969994150784 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:35.328380 48000975963008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:35.329495 47919771034496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880955.229020 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.229772 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.230442 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.328665 47581081465728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.329638 47896069104512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880955.232033 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.232779 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.233466 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.328722 47641712042880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.329755 46969994150784 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:35.329754 48000975963008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:35.329791 47581081465728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuq8b25de
I0618 12:02:35.329872 47641712042880 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54bbe8ed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.330872 47581081465728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuq8b25de', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b469e0bce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.331140 47641712042880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.331323 47581081465728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.336495 47641712042880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.336486 47581081465728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.343511 47629534057344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.343585 47105579467648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.343896 46952379081600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.343647 47349473551232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.344175 47584875869056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.345005 47109357519744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.346306 47113268593536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.346407 47129542685568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.347818 47629534057344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.348214 46952379081600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.348644 47896069104512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.348710 47919771034496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.349554 47947296002944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.350678 47040460870528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:02:35.352905 47629534057344 estimator.py:1111] Calling model_fn.
W0618 12:02:35.353013 47629534057344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:35.353345 46952379081600 estimator.py:1111] Calling model_fn.
W0618 12:02:35.353450 46952379081600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:35.353872 47947296002944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.354372 47629534057344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:35.354794 46952379081600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:35.355055 47040460870528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:35.358226 47581081465728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:02:35.359023 47947296002944 estimator.py:1111] Calling model_fn.
W0618 12:02:35.358584 47641712042880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:35.359134 47947296002944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:35.360227 47040460870528 estimator.py:1111] Calling model_fn.
W0618 12:02:35.360337 47040460870528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:35.360519 47947296002944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:35.361719 47040460870528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880955.281410 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.282327 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.283191 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.377776 47924034245504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.289743 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.290505 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.291228 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.377817 47616522388352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.378782 47924034245504 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyqi9bpkl
W0618 12:02:35.378812 47616522388352 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpokgezd3m
I0618 12:02:35.379785 47924034245504 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyqi9bpkl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96779fee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.379810 47616522388352 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpokgezd3m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4ede7d5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.380185 47924034245504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.380205 47616522388352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.385087 47616522388352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:35.385284 47924034245504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880955.315504 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.315910 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.316276 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.388555 47939895440256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.314088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.314501 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.314857 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.388655 47867212088192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.313978 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.314361 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.314694 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.388064 47247132054400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
:::MLL 1560880955.315751 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880955.316176 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880955.316496 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:35.388261 47508858954624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000009-000005.tfrecord.zz_0_0
W0618 12:02:35.389574 47939895440256 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp41672w__
W0618 12:02:35.389646 47867212088192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptui9surg
W0618 12:02:35.389155 47247132054400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpd9poj60w
I0618 12:02:35.390551 47939895440256 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp41672w__', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a29067da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:35.389275 47508858954624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9lcs_w0a
I0618 12:02:35.390142 47247132054400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpd9poj60w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af8dd1c2dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.390614 47867212088192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptui9surg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b893cc28da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.390249 47508858954624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9lcs_w0a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b35cd3fbdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:35.390946 47939895440256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.391006 47867212088192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.390538 47247132054400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:35.390642 47508858954624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:35.391598 47349473551232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.392050 47105579467648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.392938 47584875869056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:35.393127 4[2019-06-18 12:03:14] divide_golden_chunk finished: 3.519 seconds
[2019-06-18 12:03:14] generate golden chunk: 3.533 seconds
[2019-06-18 12:03:14] iteration time 18: 49.317 seconds
2019-06-18 12:03:15.969558: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880994.880973 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:03:19] minmax time: 3.270 seconds
2019-06-18 12:03:19.249396: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:19.254599: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:19.259089: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880999.271528 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 18}}
[2019-06-18 12:03:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000020-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:03:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=20 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=1023779851 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=2047559682 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=3071339513 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=4095119344 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=5118899175 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=6142679006 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=7166458837 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=8190238668 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=9214018499 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=10237798330 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=11261578161 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=12285357992 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=13309137823 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=14332917654 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=15356697485 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=16380477316 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=17404257147 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=18428036978 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=19451816809 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000019-000012 --seed=20475596640 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:03:29] eval finished: 10.434 seconds
[2019-06-18 12:03:29] Win rate 000019-000012 vs 000017-000011: 0.510
:::MLL 1560881009.777781 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:03:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=21 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=1023779852 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=2047559683 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=3071339514 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=4095119345 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=5118899176 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=6142679007 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=7166458838 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=8190238669 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=9214018500 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=10237798331 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=11261578162 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=12285357993 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=13309137824 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=14332917655 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=15356697486 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=16380477317 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=17404257148 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000020-000011 --seed=18428036979 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:03:59] selfplay finished: 29.532 seconds
[2019-06-18 12:03:59] selfplay mn: 29.553 seconds
[2019-06-18 12:03:59] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-21-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779852 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559683 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339514 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119345 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899176 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679007 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458838 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238669 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018500 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798331 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578162 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357993 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137824 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917655 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697486 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477317 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257148 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036979 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816810 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596641 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376472 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156303 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000020-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:04:02] divide_golden_chunk finished: 3.320 seconds
[2019-06-18 12:04:02] generate golden chunk: 3.335 seconds
[2019-06-18 12:04:02] train finished: 43.394 seconds
:::MLL 1560881004.509610 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.510512 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.511318 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.607483 47940647232384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.608561 47940647232384 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfvg270xf
I0618 12:03:24.609645 47940647232384 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfvg270xf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a55d5ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.610073 47940647232384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.615005 47940647232384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881004.530828 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.531682 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.532442 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.618289 47932429935488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.619265 47932429935488 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnm6hhxn7
I0618 12:03:24.620380 47932429935488 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnm6hhxn7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b986c0bfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.620808 47932429935488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.625742 47932429935488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881004.533327 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.534089 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.534769 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.628453 47122738090880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.528133 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.529008 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.529855 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.628679 47516310770560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.629532 47122738090880 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpg5q19rjh
W0618 12:03:24.629722 47516310770560 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzjeewv2n
I0618 12:03:24.630527 47122738090880 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpg5q19rjh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adbe6a6ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.630713 47516310770560 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzjeewv2n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3789696e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.630931 47122738090880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.631108 47516310770560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.635807 47122738090880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.635889 47516310770560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.635731 47940647232384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881004.543187 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.543928 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.544609 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.641095 47151753876352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.541281 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.542021 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.542752 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.641177 47373649781632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.642127 47151753876352 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4i4gjv4t
W0618 12:03:24.642175 47373649781632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuf13z0xw
I0618 12:03:24.643108 47151753876352 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4i4gjv4t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae2a8209e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.643182 47373649781632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuf13z0xw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1652276e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.643505 47151753876352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.643585 47373649781632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.647672 47932429935488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.648250 47151753876352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.648410 47373649781632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.655230 47122738090880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.655415 47516310770560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.667357 47151753876352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.667757 47373649781632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.688797 47940647232384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881004.615458 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.615909 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.616260 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.692658 47034582758272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.618419 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.618816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.619172 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.692711 47376712909696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.693378 47940647232384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881004.615122 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.615497 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.615819 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.693429 47657746584448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.612393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.612839 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.613163 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.693515 47998502835072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.693648 47034582758272 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpu3s5od1e
W0618 12:03:24.693721 47376712909696 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmc0hwhtr
:::MLL 1560881004.609206 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.609671 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.610090 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.694289 46973025461120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.603093 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.603632 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.604115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.694506 47862337307520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
I0618 12:03:24.694628 47034582758272 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpu3s5od1e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7602f4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.694696 47376712909696 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmc0hwhtr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1708bb1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.695031 47034582758272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.695094 47376712909696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.694511 47998502835072 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwxap561m
W0618 12:03:24.694481 47657746584448 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0cyu5rmd
I0618 12:03:24.695477 47657746584448 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0cyu5rmd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5877a4ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.695493 47998502835072 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwxap561m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7ce4c5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.695878 47657746584448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.695330 46973025461120 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjjafw6cb
I0618 12:03:24.695886 47998502835072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.695488 47862337307520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmprgv4i4fp
I0618 12:03:24.696302 46973025461120 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjjafw6cb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab90b157da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.696455 47862337307520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmprgv4i4fp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b881a335e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.696696 46973025461120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.696852 47862337307520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.698507 47940647232384 estimator.py:1111] Calling model_fn.
W0618 12:03:24.698621 47940647232384 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:24.699693 47376712909696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.699676 47034582758272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.699928 47932429935488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881004.601429 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.602351 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.603226 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.699785 47331311571840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.699987 47940647232384 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:24.700664 47998502835072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.700698 47657746584448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881004.622867 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.623680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.624470 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.700868 47872082035584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.701425 46973025461120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.701449 47862337307520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.700838 47331311571840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptw30s2i5
I0618 12:03:24.701841 47331311571840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptw30s2i5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0c7699ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.702249 47331311571840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.701858 47872082035584 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp43kuuf7h
I0618 12:03:24.702855 47872082035584 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp43kuuf7h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a5f07ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:24.703098 47516310770560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.703224 47122738090880 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:03:24.703259 47872082035584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.704297 47932429935488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.707211 47331311571840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.707399 47516310770560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.707543 47122738090880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.708106 47872082035584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:24.709410 47932429935488 estimator.py:1111] Calling model_fn.
W0618 12:03:24.709521 47932429935488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:24.710889 47932429935488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:03:24.712435 47516310770560 estimator.py:1111] Calling model_fn.
W0618 12:03:24.712546 47516310770560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:24.712599 47122738090880 estimator.py:1111] Calling model_fn.
W0618 12:03:24.712711 47122738090880 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:24.713927 47516310770560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:24.714068 47122738090880 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:24.714875 47151753876352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.715614 47373649781632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.718842 47376712909696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.718970 47034582758272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.719161 47151753876352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.719959 47373649781632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.719730 47998502835072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.719767 47657746584448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.720711 46973025461120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.720732 47862337307520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:24.724215 47151753876352 estimator.py:1111] Calling model_fn.
W0618 12:03:24.724327 47151753876352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:24.725037 47373649781632 estimator.py:1111] Calling model_fn.
W0618 12:03:24.725146 47373649781632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:24.725682 47151753876352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:24.726503 47373649781632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:24.726709 47331311571840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.727796 47872082035584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881004.637126 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.638046 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.638949 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.735215 47209603720064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
:::MLL 1560881004.646847 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.647582 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.648281 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.735352 47788605088640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.736352 47209603720064 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj8m5u079
I0618 12:03:24.736517 47788605088640 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b76ef6aed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.737469 47209603720064 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj8m5u079', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af0203f2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.737791 47788605088640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.737902 47209603720064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:24.743219 47209603720064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.743337 47788605088640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:24.765104 47209603720064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.765920 47376712909696 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.766103 47034582758272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.766050 47788605088640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:24.766921 47998502835072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.767107 47657746584448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.768027 47862337307520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.768320 46973025461120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881004.692642 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.693087 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.693473 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.768610 47477561615232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.770194 47376712909696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.770397 47034582758272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.769631 47477561615232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpeeti9r0p
I0618 12:03:24.770623 47477561615232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpeeti9r0p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2e83c82e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.771026 47477561615232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881004.697717 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881004.698162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881004.698553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:24.770888 47613377635200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000010-000006.tfrecord.zz_0_0
W0618 12:03:24.771216 47998502835072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.771379 47657746584448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.772321 47862337307520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.772634 46973025461120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:24.771851 47613377635200 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpklm2e5ar
I0618 12:03:24.772822 47613377635200 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpklm2e5ar', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4e230c3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:24.773222 47613377635200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:24.775225 47376712909696 estimator.py:1111] Calling model_fn.
W0618 12:03:24.775331 47376712909696 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:24.775429 47034582758272 estimator.py:1111] Calling model_fn.
W0618 12:03:24.775537 47034582758272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:24.775383 47331311571840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:24.775731 47477561615232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/trai[2019-06-18 12:04:02] moving /lfs/lfs12/gma_akey/results/epb134/models/000020-000012.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000020-000013.meta
[2019-06-18 12:04:02] moving /lfs/lfs12/gma_akey/results/epb134/models/000020-000012.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000020-000013.data-00000-of-00001
[2019-06-18 12:04:02] moving /lfs/lfs12/gma_akey/results/epb134/models/000020-000012.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb
[2019-06-18 12:04:02] moving /lfs/lfs12/gma_akey/results/epb134/models/000020-000012.index --> /lfs/lfs12/gma_akey/results/epb134/models/000020-000013.index
[2019-06-18 12:04:02] iteration time 19: 47.852 seconds
2019-06-18 12:04:03.857736: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881042.733445 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:04:07] minmax time: 3.311 seconds
2019-06-18 12:04:07.179124: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:04:07.184536: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:04:07.189166: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881047.200505 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 19}}
[2019-06-18 12:04:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:04:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=21 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=1023779852 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=2047559683 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=3071339514 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=4095119345 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=5118899176 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=6142679007 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=7166458838 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=8190238669 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=9214018500 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=10237798331 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=11261578162 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=12285357993 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=13309137824 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=14332917655 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=15356697486 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=16380477317 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=17404257148 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=18428036979 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=19451816810 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000020-000013 --seed=20475596641 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:04:16] eval finished: 9.637 seconds
[2019-06-18 12:04:16] Win rate 000020-000013 vs 000019-000012: 0.620
:::MLL 1560881056.912642 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:04:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=22 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=1023779853 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=2047559684 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=3071339515 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=4095119346 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=5118899177 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=6142679008 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=7166458839 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=8190238670 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=9214018501 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=10237798332 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=11261578163 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=12285357994 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=13309137825 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=14332917656 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=15356697487 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=16380477318 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=17404257149 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000021-000012 --seed=18428036980 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:04:46] selfplay finished: 29.629 seconds
[2019-06-18 12:04:46] selfplay mn: 29.647 seconds
[2019-06-18 12:04:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-22-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779853 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559684 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339515 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119346 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899177 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679008 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458839 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238670 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018501 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798332 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578163 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357994 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137825 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917656 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697487 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477318 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257149 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036980 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816811 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596642 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376473 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156304 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:04:50] divide_golden_chunk finished: 3.554 seconds
[2019-06-18 12:04:50] generate golden chunk: 3.569 seconds
[2019-06-18 12:04:50] train finished: 43.735 seconds
:::MLL 1560881052.462195 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.463112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.463946 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.567735 47037624992640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.568785 47037624992640 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmph_lh84v4
I0618 12:04:12.569798 47037624992640 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmph_lh84v4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac815841e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881052.492099 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.492978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.493755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.569906 47323956683648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:04:12.570214 47037624992640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.570909 47323956683648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwqv8uq11
I0618 12:04:12.571962 47323956683648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwqv8uq11', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0ac036fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881052.462068 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.463000 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.463840 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.571478 46938704544640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.469300 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.470048 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.470723 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.571519 47439105786752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:04:12.572352 47323956683648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.572586 46938704544640 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1x2t4qee
W0618 12:04:12.572619 47439105786752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvsk8cbaw
I0618 12:04:12.573667 46938704544640 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1x2t4qee', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab10d65de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.573722 47439105786752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvsk8cbaw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b258fa2ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.574110 46938704544640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:12.574160 47439105786752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.575084 47037624992640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.576976 47323956683648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.579409 46938704544640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.579463 47439105786752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881052.482102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.483035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.483938 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.583261 47385590731648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.489918 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.490663 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.491335 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.583292 47061138396032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.584317 47385590731648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1pfuljhw
W0618 12:04:12.584348 47061138396032 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz6pav52x
I0618 12:04:12.585334 47385590731648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1pfuljhw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1919e3cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.585349 47061138396032 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz6pav52x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acd8f061da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.585760 47061138396032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:12.585759 47385590731648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.590546 47385590731648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.590579 47061138396032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.594454 47037624992640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.596137 47323956683648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.601492 46938704544640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.602133 47439105786752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.609983 47385590731648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.610372 47061138396032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881052.556610 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.557137 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.557569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.636422 47177661535104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.552855 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.553234 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.553558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.637567 47352196068224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.637470 47177661535104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpdaxfm_et
I0618 12:04:12.638491 47177661535104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpdaxfm_et', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8b0580e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.638891 47177661535104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.638639 47352196068224 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpori9dx19
I0618 12:04:12.639677 47352196068224 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpori9dx19', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b115369ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.640073 47352196068224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.642750 47037624992640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.643327 47323956683648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.643562 47177661535104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881052.563520 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.563935 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.564283 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.643765 47866519180160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.644737 47352196068224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.644758 47866519180160 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwzr_izb4
I0618 12:04:12.645747 47866519180160 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwzr_izb4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8913759e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.646146 47866519180160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881052.554513 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.554913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.555265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.646562 47325624853376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.647083 47037624992640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.647608 47323956683648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.647549 47325624853376 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj85i8959
I0618 12:04:12.648522 47325624853376 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj85i8959', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b23a53e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.648912 47325624853376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.650677 47866519180160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:12.652202 47037624992640 estimator.py:1111] Calling model_fn.
W0618 12:04:12.652315 47037624992640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:12.652637 47323956683648 estimator.py:1111] Calling model_fn.
W0618 12:04:12.652744 47323956683648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.653074 46938704544640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881052.556788 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.557552 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.558236 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.652393 47083180303232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.547128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.548077 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.548974 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.652662 47592593240960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.653684 47037624992640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.653469 47325624853376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.654086 47323956683648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.653762 47439105786752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.653548 47083180303232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8wegx0_m
I0618 12:04:12.653807 47592593240960 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b494c338d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.654640 47083180303232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8wegx0_m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad2b0d2fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.655095 47592593240960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:12.655104 47083180303232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881052.578068 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.578446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.578769 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.655906 47934668632960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.574586 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.575131 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.575556 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.656118 46926989173632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.657372 46938704544640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.656947 47934668632960 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpz20foi9i
W0618 12:04:12.657128 46926989173632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp95gqh4s6
I0618 12:04:12.657938 47934668632960 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpz20foi9i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98f17bbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.658105 46926989173632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp95gqh4s6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae531b5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.658338 47934668632960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.658105 47439105786752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:12.658502 46926989173632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.658642 47385590731648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.659012 47061138396032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.660459 47083180303232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.660490 47592593240960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:12.662436 46938704544640 estimator.py:1111] Calling model_fn.
W0618 12:04:12.662808 47177661535104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.662544 46938704544640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.662961 47385590731648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.662965 47934668632960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.663153 46926989173632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.663315 47061138396032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:12.663184 47439105786752 estimator.py:1111] Calling model_fn.
W0618 12:04:12.663293 47439105786752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.663872 47352196068224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.663899 46938704544640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881052.554414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.555174 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.555877 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.663866 47946289787776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.552131 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.552894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.553634 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.664093 47846077416320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.664657 47439105786752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.664917 47946289787776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfweq3rxk
W0618 12:04:12.665072 47846077416320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpf2dp3i52
I0618 12:04:12.665914 47946289787776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfweq3rxk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ba6288e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.666053 47846077416320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpf2dp3i52', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8451091e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.666320 47946289787776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:12.666475 47846077416320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:12.668025 47385590731648 estimator.py:1111] Calling model_fn.
W0618 12:04:12.668135 47385590731648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:12.668355 47061138396032 estimator.py:1111] Calling model_fn.
W0618 12:04:12.668465 47061138396032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.669498 47385590731648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.669835 47061138396032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.669931 47866519180160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.671190 47946289787776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.671218 47846077416320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.672622 47325624853376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.682209 47934668632960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.682529 46926989173632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.682528 47083180303232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.683008 47592593240960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.690520 47946289787776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:12.690571 47846077416320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881052.630766 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.631203 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.631584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.708404 47511098577792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.710045 47177661535104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.709420 47511098577792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2ec10ry5
:::MLL 1560881052.635307 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.635724 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.636081 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.710069 47585645970304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:04:12.710400 47511098577792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2ec10ry5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3652bdae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.710800 47511098577792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.711417 47352196068224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.711044 47585645970304 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt7mtovni
I0618 12:04:12.712016 47585645970304 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt7mtovni', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47ae1c9dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.712405 47585645970304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.714333 47177661535104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.715746 47352196068224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:12.715494 47511098577792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:12.716944 47866519180160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.716987 47585645970304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:12.719379 47177661535104 estimator.py:1111] Calling model_fn.
W0618 12:04:12.719487 47177661535104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.719908 47325624853376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:12.720859 47177661535104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:04:12.720802 47352196068224 estimator.py:1111] Calling model_fn.
W0618 12:04:12.720910 47352196068224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:12.721231 47866519180160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881052.638754 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.639312 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.639808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.721411 47347994309504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881052.644401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881052.644836 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881052.645210 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:12.722169 46964256695168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:04:12.722266 47352196068224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:12.722482 47347994309504 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp599kh73w
I0618 12:04:12.723539 47347994309504 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp599kh73w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1058f80e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:12.723941 47347994309504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:12.723200 46964256695168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvsam4axv
I0618 12:04:12.724200 46964256695168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvsam4axv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs':[2019-06-18 12:04:50] moving /lfs/lfs12/gma_akey/results/epb134/models/000021-000013.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb
[2019-06-18 12:04:50] moving /lfs/lfs12/gma_akey/results/epb134/models/000021-000013.index --> /lfs/lfs12/gma_akey/results/epb134/models/000021-000014.index
[2019-06-18 12:04:50] moving /lfs/lfs12/gma_akey/results/epb134/models/000021-000013.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000021-000014.data-00000-of-00001
[2019-06-18 12:04:50] moving /lfs/lfs12/gma_akey/results/epb134/models/000021-000013.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000021-000014.meta
[2019-06-18 12:04:50] iteration time 20: 48.263 seconds
2019-06-18 12:04:52.339166: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881090.996402 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:04:55] minmax time: 3.240 seconds
2019-06-18 12:04:55.589054: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:04:55.594483: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:04:55.599057: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881095.609710 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 20}}
[2019-06-18 12:04:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:04:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=22 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=1023779853 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=2047559684 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=3071339515 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=4095119346 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=5118899177 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=6142679008 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=7166458839 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=8190238670 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=9214018501 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=10237798332 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=11261578163 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=12285357994 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=13309137825 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=14332917656 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=15356697487 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=16380477318 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=17404257149 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=18428036980 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=19451816811 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000021-000014 --seed=20475596642 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:05:05] eval finished: 10.209 seconds
[2019-06-18 12:05:05] Win rate 000021-000014 vs 000020-000013: 0.690
:::MLL 1560881105.893539 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:05:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=23 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=1023779854 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=2047559685 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=3071339516 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=4095119347 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=5118899178 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=6142679009 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=7166458840 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=8190238671 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=9214018502 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=10237798333 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=11261578164 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=12285357995 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=13309137826 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=14332917657 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=15356697488 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=16380477319 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=17404257150 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000022-000013 --seed=18428036981 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:05:35] selfplay finished: 29.725 seconds
[2019-06-18 12:05:35] selfplay mn: 29.742 seconds
[2019-06-18 12:05:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-23-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=23 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779854 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559685 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339516 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119347 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899178 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679009 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458840 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238671 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018502 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798333 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578164 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357995 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137826 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917657 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697488 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477319 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257150 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036981 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816812 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596643 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376474 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156305 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:05:39] divide_golden_chunk finished: 3.537 seconds
[2019-06-18 12:05:39] generate golden chunk: 3.553 seconds
[2019-06-18 12:05:39] train finished: 43.759 seconds
:::MLL 1560881100.890453 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.891312 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.891993 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:00.994561 47857908761472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881100.894297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.895036 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.895751 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:00.994605 47304839435136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:00.995636 47857908761472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpxibm_ibq
W0618 12:05:00.995668 47304839435136 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmbatt9q_
I0618 12:05:00.996654 47857908761472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpxibm_ibq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b87123d1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:00.996671 47304839435136 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmbatt9q_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b064cbcee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:00.997069 47857908761472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:00.997100 47304839435136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.001879 47857908761472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.001962 47304839435136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881100.927888 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.928612 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.929249 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.020520 47537795539840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.020959 47857908761472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881100.914528 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.915377 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.916231 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.020549 47298537288576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.021804 47304839435136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.021630 47537795539840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9950d3jx
W0618 12:05:01.021670 47298537288576 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt9kj0qyv
I0618 12:05:01.022638 47537795539840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9950d3jx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c8a00fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.022679 47298537288576 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt9kj0qyv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04d519be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.023041 47537795539840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.023081 47298537288576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.028115 47537795539840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.028212 47298537288576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881100.920953 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.921713 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.922380 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.034781 47692311806848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881100.916286 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.917194 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.918056 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.034870 46930082558848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.035955 47692311806848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpswo2v6i7
W0618 12:05:01.036145 46930082558848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpe8s85pd9
I0618 12:05:01.036981 47692311806848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpswo2v6i7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6083e40e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.037148 46930082558848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpe8s85pd9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaf0b7cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.037384 47692311806848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.037549 46930082558848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.042124 47692311806848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.042194 46930082558848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.050119 47298537288576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.050179 47537795539840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.061329 47692311806848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.061550 46930082558848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.068939 47857908761472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.070385 47304839435136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881100.980216 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.980668 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.981062 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.070502 47209607377792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881100.983872 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.984246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.984562 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.070531 47791640404864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.071648 47791640404864 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjxeoccpb
W0618 12:05:01.071618 47209607377792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj1879yp_
I0618 12:05:01.072659 47209607377792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj1879yp_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af020770e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.072685 47791640404864 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjxeoccpb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b77a4562e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.073087 47209607377792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.073115 47791640404864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.073187 47857908761472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.074688 47304839435136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.077821 47209607377792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881100.977912 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.978681 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.979364 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.077494 47595895706496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.077846 47791640404864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881100.975465 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.976202 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.976880 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.077736 47191824982912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
I0618 12:05:01.078213 47857908761472 estimator.py:1111] Calling model_fn.
W0618 12:05:01.078323 47857908761472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:01.078583 47595895706496 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpzx8lorto
W0618 12:05:01.079669 47857908761472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:01.079741 47304839435136 estimator.py:1111] Calling model_fn.
W0618 12:05:01.079852 47304839435136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:01.078819 47191824982912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1k1wr43u
I0618 12:05:01.079738 47595895706496 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpzx8lorto', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4a110b2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.079922 47191824982912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1k1wr43u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aebfc8d3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.080135 47595895706496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.080328 47191824982912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.081207 47304839435136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.084846 47595895706496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.084936 47191824982912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881101.007884 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.008359 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.008765 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.091789 47760523764608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881101.009591 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.010172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.010585 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.091934 47329315058560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.092809 47760523764608 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4blj7yjx
W0618 12:05:01.092916 47329315058560 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8di02pox
I0618 12:05:01.093776 47760523764608 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4blj7yjx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7065a3fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.093904 47329315058560 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8di02pox', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0bff994e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.094168 47760523764608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.094295 47329315058560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.097419 47791640404864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.097433 47209607377792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.098822 47760523764608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.098869 47329315058560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881100.995607 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881100.996539 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881100.997255 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.100037 46974501495680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881100.999491 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.000222 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.000926 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.100121 47096085111680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.101144 46974501495680 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp3qe0fgn6
I0618 12:05:01.101254 47096085111680 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5b202bd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.102224 46974501495680 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp3qe0fgn6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab9630ffe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.102527 47096085111680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.102670 46974501495680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881101.021130 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.021542 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.021888 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.103296 47503735509888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.103666 47298537288576 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881101.021985 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.022378 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.022724 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.103594 47073820193664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.104116 47595895706496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.104328 47191824982912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.104599 47537795539840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.104327 47503735509888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_nujgqil
W0618 12:05:01.104563 47073820193664 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq2j1_h4g
I0618 12:05:01.105319 47503735509888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_nujgqil', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b349bde0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.105546 47073820193664 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq2j1_h4g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad082eb0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.105717 47503735509888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:01.105936 47073820193664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.108303 47298537288576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.107838 47096085111680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.107862 46974501495680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.108942 47692311806848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.109263 47537795539840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.109592 46930082558848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.110330 47503735509888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.110562 47073820193664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.113228 47692311806848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.113899 46930082558848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:01.113756 47298537288576 estimator.py:1111] Calling model_fn.
W0618 12:05:01.113872 47298537288576 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:01.114773 47537795539840 estimator.py:1111] Calling model_fn.
W0618 12:05:01.114890 47537795539840 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:01.115331 47298537288576 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.116374 47537795539840 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.117789 47329315058560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.117954 47760523764608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:05:01.118297 47692311806848 estimator.py:1111] Calling model_fn.
W0618 12:05:01.118405 47692311806848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:01.118956 46930082558848 estimator.py:1111] Calling model_fn.
W0618 12:05:01.119065 46930082558848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:01.119782 47692311806848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.120443 46930082558848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.129249 47503735509888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.129819 47073820193664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.129662 47096085111680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:01.129748 46974501495680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881101.061461 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.061898 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.062278 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.142735 47747387859840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881101.064126 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.064546 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.064945 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.143454 47125374477184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.143747 47747387859840 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplmqfpmla
I0618 12:05:01.144740 47747387859840 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplmqfpmla', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6d56adde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:01.145141 47747387859840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.144435 47125374477184 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpaomawvol
I0618 12:05:01.145418 47125374477184 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpaomawvol', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adc83caee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:01.145658 47209607377792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:01.145823 47125374477184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:01.146111 47791640404864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.149729 47747387859840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.150244 47209607377792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.150373 47125374477184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:01.150705 47791640404864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:01.152318 47595895706496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:01.152272 47191824982912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:01.155680 47209607377792 estimator.py:1111] Calling model_fn.
W0618 12:05:01.155794 47209607377792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:01.156190 47791640404864 estimator.py:1111] Calling model_fn.
W0618 12:05:01.156308 47791640404864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:01.156607 47595895706496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881101.073749 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.074316 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.074682 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.155865 47030516949888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.156564 47191824982912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881101.071004 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881101.071435 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881101.071799 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:01.156054 47875548042112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:05:01.157231 47209607377792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.157735 47791640404864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:01.156900 47030516949888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvn6otcko
W0618 12:05:01.157047 47875548042112 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpef2m3ceb
I0618 12:05:01.157882 47030516949888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvn6otcko', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.p[2019-06-18 12:05:39] moving /lfs/lfs12/gma_akey/results/epb134/models/000022-000014.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000022-000015.data-00000-of-00001
[2019-06-18 12:05:39] moving /lfs/lfs12/gma_akey/results/epb134/models/000022-000014.index --> /lfs/lfs12/gma_akey/results/epb134/models/000022-000015.index
[2019-06-18 12:05:39] moving /lfs/lfs12/gma_akey/results/epb134/models/000022-000014.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000022-000015.meta
[2019-06-18 12:05:39] moving /lfs/lfs12/gma_akey/results/epb134/models/000022-000014.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb
[2019-06-18 12:05:39] iteration time 21: 48.439 seconds
2019-06-18 12:05:40.668178: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881139.435536 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:05:43] minmax time: 3.235 seconds
2019-06-18 12:05:43.913755: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:05:43.919175: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:05:43.923665: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881143.934752 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 21}}
[2019-06-18 12:05:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:05:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=23 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=1023779854 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=2047559685 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=3071339516 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=4095119347 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=5118899178 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=6142679009 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=7166458840 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=8190238671 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=9214018502 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=10237798333 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=11261578164 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=12285357995 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=13309137826 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=14332917657 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=15356697488 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=16380477319 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=17404257150 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=18428036981 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=19451816812 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000022-000015 --seed=20475596643 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:05:54] eval finished: 10.542 seconds
[2019-06-18 12:05:54] Win rate 000022-000015 vs 000021-000014: 0.640
:::MLL 1560881154.551670 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:05:54] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=24 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=1023779855 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=2047559686 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=3071339517 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=4095119348 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=5118899179 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=6142679010 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=7166458841 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=8190238672 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=9214018503 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=10237798334 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=11261578165 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=12285357996 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=13309137827 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=14332917658 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=15356697489 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=16380477320 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=17404257151 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000023-000014 --seed=18428036982 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:06:24] selfplay finished: 29.710 seconds
[2019-06-18 12:06:24] selfplay mn: 29.728 seconds
[2019-06-18 12:06:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-24-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=24 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779855 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559686 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339517 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119348 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899179 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679010 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458841 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238672 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018503 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798334 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578165 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357996 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137827 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917658 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697489 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477320 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257151 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036982 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816813 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596644 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376475 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156306 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:06:27] divide_golden_chunk finished: 3.493 seconds
[2019-06-18 12:06:27] generate golden chunk: 3.507 seconds
[2019-06-18 12:06:27] train finished: 43.857 seconds
:::MLL 1560881149.193003 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.193768 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.194481 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.288606 47111965807488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.179637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.180543 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.181381 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.288877 46954907505536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.289748 47111965807488 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvv_uefm7
W0618 12:05:49.289980 46954907505536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvq37u48t
I0618 12:05:49.290869 47111965807488 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvv_uefm7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad96492de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.291075 46954907505536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvq37u48t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab4d32b6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.291330 47111965807488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.291539 46954907505536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.296673 47111965807488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.296821 46954907505536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.318722 47111965807488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.318947 46954907505536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881149.213186 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.214011 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.214699 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.319510 47966324159360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.212395 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.213282 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.214099 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.319580 47487549797248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.320586 47487549797248 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp15rznqrd
W0618 12:05:49.320554 47966324159360 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphqsku1ah
I0618 12:05:49.321580 47487549797248 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp15rznqrd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b30d71fce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.321581 47966324159360 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphqsku1ah', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0504cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.321982 47487549797248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.321996 47966324159360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.326885 47487549797248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.326893 47966324159360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881149.217087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.217903 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.218597 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.339248 46950231663488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.215880 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.216706 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.217492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.339500 47071625782144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.340287 46950231663488 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwer0_m5j
W0618 12:05:49.340504 47071625782144 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkj1g6fo4
I0618 12:05:49.341278 46950231663488 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwer0_m5j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3bc77ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.341502 47071625782144 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkj1g6fo4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0001efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.341678 46950231663488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.341901 47071625782144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.346068 47966324159360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.346197 47487549797248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.346614 46950231663488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.346692 47071625782144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881149.272727 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.273112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.273444 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.359909 47433065063296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.274408 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.274779 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.275109 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.360928 47279089128320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.360940 47433065063296 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwx1n9lmp
I0618 12:05:49.361932 47433065063296 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwx1n9lmp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b242794ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.362334 47433065063296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.361920 47279089128320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsjxz7yng
I0618 12:05:49.362910 47279089128320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsjxz7yng', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b004de65e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.363314 47279089128320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.365903 46950231663488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.366083 47071625782144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.366931 47433065063296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.367974 47279089128320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.371273 46954907505536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.371743 47111965807488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.375834 46954907505536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.376362 47111965807488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:49.381026 46954907505536 estimator.py:1111] Calling model_fn.
W0618 12:05:49.381159 46954907505536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:49.381605 47111965807488 estimator.py:1111] Calling model_fn.
W0618 12:05:49.381723 47111965807488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:49.382622 46954907505536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:49.383193 47111965807488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:49.385910 47433065063296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.387797 47279089128320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881149.306466 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.306901 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.307268 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.389186 47222174819200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.289342 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.290104 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.290767 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.389878 47882184266624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.309818 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.310242 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.310625 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.390193 47861324395392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.282234 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.283127 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.284000 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.390073 47048472830848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.390207 47222174819200 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_92jqpj8
I0618 12:05:49.391191 47222174819200 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_92jqpj8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af30d8aee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.391585 47222174819200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.390917 47882184266624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnkkegcl9
W0618 12:05:49.391171 47861324395392 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptzzf75p5
W0618 12:05:49.391074 47048472830848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp85_1ubmz
I0618 12:05:49.391920 47882184266624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnkkegcl9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8cb92bee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.392144 47861324395392 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptzzf75p5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b87ddd38e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.392074 47048472830848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp85_1ubmz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca9c18fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.392360 47882184266624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.392536 47861324395392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.392518 47048472830848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.394284 47966324159360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.394634 47487549797248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881149.290582 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.291168 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.291688 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.394959 47736622187392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.310205 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.310662 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.311065 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.395118 47753412346752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.396161 47222174819200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.395980 47736622187392 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpib6r_n_1
W0618 12:05:49.396110 47753412346752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp07bznk98
I0618 12:05:49.396954 47736622187392 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpib6r_n_1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ad4fece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.397070 47753412346752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp07bznk98', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ebdc45da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:49.397070 47861324395392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:05:49.397364 47736622187392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.397464 47753412346752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.397288 47882184266624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.397299 47048472830848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.398627 47966324159360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.398987 47487549797248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.401951 47736622187392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.402007 47753412346752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:05:49.403729 47966324159360 estimator.py:1111] Calling model_fn.
W0618 12:05:49.403840 47966324159360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:49.404058 47487549797248 estimator.py:1111] Calling model_fn.
W0618 12:05:49.404169 47487549797248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:49.405226 47966324159360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:49.405511 47487549797248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:49.413509 46950231663488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.413948 47071625782144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.415147 47222174819200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.416193 47861324395392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.417150 47048472830848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.417182 47882184266624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.417799 46950231663488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.418264 47071625782144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.420999 47736622187392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.421142 47753412346752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:05:49.422814 46950231663488 estimator.py:1111] Calling model_fn.
W0618 12:05:49.422933 46950231663488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:49.423321 47071625782144 estimator.py:1111] Calling model_fn.
W0618 12:05:49.423429 47071625782144 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:49.424287 46950231663488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:49.424783 47071625782144 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881149.316953 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.317829 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.318708 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.424939 48008238777216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881149.317072 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.318003 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.318860 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.425073 47128817685376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.426076 48008238777216 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpw9c1dwo9
I0618 12:05:49.426222 47128817685376 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2add51062d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.427176 48008238777216 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpw9c1dwo9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baa129b1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.427453 47128817685376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:49.427634 48008238777216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.432772 47128817685376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.432857 48008238777216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.433514 47433065063296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.436405 47279089128320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.437817 47433065063296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.440839 47279089128320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:49.442883 47433065063296 estimator.py:1111] Calling model_fn.
W0618 12:05:49.442991 47433065063296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:49.444356 47433065063296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:49.446078 47279089128320 estimator.py:1111] Calling model_fn.
W0618 12:05:49.446189 47279089128320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:49.447548 47279089128320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881149.376088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.376487 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.376816 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.453950 47439261635456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.454710 48008238777216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.454783 47128817685376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:49.455005 47439261635456 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwhl1q1wg
I0618 12:05:49.456040 47439261635456 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwhl1q1wg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2598ecde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:49.456488 47439261635456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.461285 47439261635456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:49.462207 47222174819200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.463274 47861324395392 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881149.377899 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881149.378304 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881149.378759 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:49.463504 47976511918976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:05:49.464500 47976511918976 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpny5p_nt2
I0618 12:05:49.465465 47976511918976 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpny5p_nt2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba2af89ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:49.465718 47882184266624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:05:49.465862 47976511918976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:49.465857 47048472830848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:49.466484 47222174819200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.467571 47861324395392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:49.468309 47736622187392 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:[2019-06-18 12:06:27] moving /lfs/lfs12/gma_akey/results/epb134/models/000023-000015.index --> /lfs/lfs12/gma_akey/results/epb134/models/000023-000016.index
[2019-06-18 12:06:27] moving /lfs/lfs12/gma_akey/results/epb134/models/000023-000015.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000023-000016.data-00000-of-00001
[2019-06-18 12:06:27] moving /lfs/lfs12/gma_akey/results/epb134/models/000023-000015.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb
[2019-06-18 12:06:27] moving /lfs/lfs12/gma_akey/results/epb134/models/000023-000015.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000023-000016.meta
[2019-06-18 12:06:27] iteration time 22: 48.423 seconds
2019-06-18 12:06:29.121202: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881187.859106 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:06:32] minmax time: 3.264 seconds
2019-06-18 12:06:32.394592: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:06:32.400117: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:06:32.404707: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881192.415782 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 22}}
[2019-06-18 12:06:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:06:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=24 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=1023779855 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=2047559686 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=3071339517 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=4095119348 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=5118899179 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=6142679010 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=7166458841 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=8190238672 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=9214018503 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=10237798334 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=11261578165 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=12285357996 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=13309137827 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=14332917658 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=15356697489 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=16380477320 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=17404257151 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=18428036982 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=19451816813 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000023-000016 --seed=20475596644 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:06:43] eval finished: 10.930 seconds
[2019-06-18 12:06:43] Win rate 000023-000016 vs 000022-000015: 0.500
:::MLL 1560881203.421235 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:06:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=25 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=1023779856 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=2047559687 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=3071339518 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=4095119349 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=5118899180 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=6142679011 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=7166458842 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=8190238673 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=9214018504 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=10237798335 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=11261578166 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=12285357997 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=13309137828 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=14332917659 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=15356697490 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=16380477321 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=17404257152 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000024-000015 --seed=18428036983 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:07:13] selfplay finished: 30.078 seconds
[2019-06-18 12:07:13] selfplay mn: 30.095 seconds
[2019-06-18 12:07:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-25-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=25 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779856 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559687 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339518 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119349 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899180 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679011 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458842 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238673 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018504 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798335 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578166 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357997 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137828 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917659 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697490 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477321 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257152 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036983 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816814 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596645 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376476 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156307 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:07:16] train finished: 43.805 seconds
:::MLL 1560881197.621060 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.621832 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.622590 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.736375 48008072561536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.622327 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.623121 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.623806 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.736447 47150283346816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.737494 48008072561536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa7p8mmsd
W0618 12:06:37.737558 47150283346816 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4d79bef4
I0618 12:06:37.738590 48008072561536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa7p8mmsd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baa08b2de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.738678 47150283346816 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4d79bef4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae2507a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.739033 48008072561536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.739148 47150283346816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.744332 48008072561536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.744408 47150283346816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.766330 48008072561536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.766788 47150283346816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881197.662326 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.663138 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.663821 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.770783 47837867357056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.661085 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.661921 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.662748 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.770840 47772190548864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.771805 47837867357056 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc7yixfod
W0618 12:06:37.771845 47772190548864 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpw1qq2suj
I0618 12:06:37.772798 47837867357056 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpc7yixfod', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8267ad8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.772966 47772190548864 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpw1qq2suj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b731d08ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.773275 47837867357056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.773433 47772190548864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.778082 47837867357056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.778155 47772190548864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881197.677255 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.678174 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.679047 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.782926 47336055038848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.683369 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.684095 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.684785 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.783124 47965951046528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.783976 47336055038848 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi51tiqo9
W0618 12:06:37.784097 47965951046528 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvg2jgthm
I0618 12:06:37.784976 47336055038848 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi51tiqo9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d91553e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.785093 47965951046528 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvg2jgthm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba03a0f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.785376 47336055038848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.785492 47965951046528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.790242 47336055038848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.790293 47965951046528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.797406 47837867357056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.797417 47772190548864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881197.709589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.709992 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.710351 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.798995 47350668342144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.706525 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.707010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.707409 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.798996 47080969823104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.800026 47350668342144 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpgzp50i47
W0618 12:06:37.799998 47080969823104 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsvlh04ll
I0618 12:06:37.800979 47080969823104 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsvlh04ll', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad22d11be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.801002 47350668342144 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpgzp50i47', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b10f85a8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.801383 47080969823104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.801402 47350668342144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.806043 47350668342144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.806036 47080969823104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.809551 47336055038848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.809797 47965951046528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.816386 48008072561536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.816813 47150283346816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.820704 48008072561536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.821142 47150283346816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.824947 47080969823104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.825130 47350668342144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:37.825792 48008072561536 estimator.py:1111] Calling model_fn.
W0618 12:06:37.825904 48008072561536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:37.826262 47150283346816 estimator.py:1111] Calling model_fn.
W0618 12:06:37.826370 47150283346816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.827259 48008072561536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.827745 47150283346816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881197.740114 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.740663 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.741147 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.827780 47847951098752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.828792 47847951098752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpapn3nur4
I0618 12:06:37.829775 47847951098752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpapn3nur4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b84c0b73e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.830180 47847951098752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881197.756357 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.756865 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.757265 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.833982 47887534801792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.834836 47847951098752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.834989 47887534801792 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpohkj2cet
I0618 12:06:37.835961 47887534801792 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpohkj2cet', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8df8169e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.836356 47887534801792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.840869 47887534801792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.845721 47772190548864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.845923 47837867357056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.850020 47772190548864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.850267 47837867357056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.853956 47847951098752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:37.855131 47772190548864 estimator.py:1111] Calling model_fn.
W0618 12:06:37.855243 47772190548864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881197.769208 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.769622 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.769975 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.855103 47830193279872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
I0618 12:06:37.855381 47837867357056 estimator.py:1111] Calling model_fn.
:::MLL 1560881197.770484 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.770890 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.771288 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.855343 47608613475200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.855494 47837867357056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.856609 47772190548864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.856142 47830193279872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpx5s0_awz
W0618 12:06:37.856874 47837867357056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.856328 47608613475200 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp2p38eajo
I0618 12:06:37.857128 47830193279872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpx5s0_awz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b809e446e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.857300 47608613475200 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp2p38eajo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d0714de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.857526 47830193279872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.857686 47608613475200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.857956 47336055038848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.858069 47965951046528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.860023 47887534801792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.862139 47830193279872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.862262 47608613475200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.862275 47336055038848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.862358 47965951046528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:06:37.867371 47336055038848 estimator.py:1111] Calling model_fn.
I0618 12:06:37.867448 47965951046528 estimator.py:1111] Calling model_fn.
W0618 12:06:37.867483 47336055038848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.867558 47965951046528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.868834 47336055038848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.868919 47965951046528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.872210 47080969823104 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.872419 47350668342144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.876484 47080969823104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.876713 47350668342144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.881130 47830193279872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.881443 47608613475200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:37.881522 47080969823104 estimator.py:1111] Calling model_fn.
W0618 12:06:37.881630 47080969823104 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:37.881791 47350668342144 estimator.py:1111] Calling model_fn.
W0618 12:06:37.881899 47350668342144 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.882978 47080969823104 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.883259 47350668342144 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881197.801020 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.801751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.802451 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.897126 47066088342400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.772206 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.773152 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.774064 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.897261 47494424400768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.898136 47066088342400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi1k48sig
W0618 12:06:37.898269 47494424400768 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp6ijjns7u
I0618 12:06:37.899116 47066088342400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi1k48sig', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aceb6105e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.899262 47494424400768 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp6ijjns7u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3270e1eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.899517 47066088342400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.899661 47494424400768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:37.901035 47847951098752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.904284 47066088342400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.904290 47494424400768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.905309 47847951098752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.907196 47887534801792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881197.804592 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.805579 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.806335 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.909435 47031773721472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
I0618 12:06:37.910386 47847951098752 estimator.py:1111] Calling model_fn.
W0618 12:06:37.910491 47847951098752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881197.808383 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.809133 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.809848 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.909726 47010078507904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.911500 47887534801792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.911862 47847951098752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.910598 47031773721472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp4mns2wc_
I0618 12:06:37.910878 47010078507904 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac1ab9e1cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.911690 47031773721472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp4mns2wc_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac6b8c0ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.912130 47010078507904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.912148 47031773721472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:37.916590 47887534801792 estimator.py:1111] Calling model_fn.
W0618 12:06:37.916702 47887534801792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:37.918071 47887534801792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:37.917673 47031773721472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.917750 47010078507904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:37.923544 47066088342400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.923704 47494424400768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:37.928436 47830193279872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.928816 47608613475200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:37.932709 47830193279872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:37.933146 47608613475200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881197.795490 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.796279 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.797003 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.933341 47620256637824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881197.786160 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881197.787120 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881197.788019 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:37.933386 47871277597568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:06:37.934366 47620256637824 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8gnu5t0v
W0618 12:06:37.934395 47871277597568 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpdsdywt8j
I0618 12:06:37.935448 47871277597568 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpdsdywt8j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a2f155e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:37.935455 47620256637824 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8gnu5t0v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_prot[2019-06-18 12:07:17] divide_golden_chunk finished: 3.533 seconds
[2019-06-18 12:07:17] generate golden chunk: 3.547 seconds
[2019-06-18 12:07:17] moving /lfs/lfs12/gma_akey/results/epb134/models/000024-000016.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000024-000017.data-00000-of-00001
[2019-06-18 12:07:17] moving /lfs/lfs12/gma_akey/results/epb134/models/000024-000016.index --> /lfs/lfs12/gma_akey/results/epb134/models/000024-000017.index
[2019-06-18 12:07:17] moving /lfs/lfs12/gma_akey/results/epb134/models/000024-000016.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000024-000017.meta
[2019-06-18 12:07:17] moving /lfs/lfs12/gma_akey/results/epb134/models/000024-000016.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb
[2019-06-18 12:07:17] iteration time 23: 49.249 seconds
2019-06-18 12:07:18.425697: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881237.108364 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:07:21] minmax time: 3.247 seconds
2019-06-18 12:07:21.682484: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:21.687775: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:21.692219: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881241.703042 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 23}}
[2019-06-18 12:07:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:07:21] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=25 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=1023779856 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=2047559687 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=3071339518 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=4095119349 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=5118899180 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=6142679011 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=7166458842 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=8190238673 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=9214018504 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=10237798335 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=11261578166 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=12285357997 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=13309137828 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=14332917659 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=15356697490 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=16380477321 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=17404257152 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=18428036983 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=19451816814 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000024-000017 --seed=20475596645 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:07:32] eval finished: 10.591 seconds
[2019-06-18 12:07:32] Win rate 000024-000017 vs 000023-000016: 0.380
:::MLL 1560881252.368956 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:07:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=26 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=1023779857 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=2047559688 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=3071339519 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=4095119350 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=5118899181 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=6142679012 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=7166458843 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=8190238674 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=9214018505 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=10237798336 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=11261578167 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=12285357998 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=13309137829 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=14332917660 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=15356697491 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=16380477322 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=17404257153 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000025-000016 --seed=18428036984 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:08:03] selfplay finished: 31.061 seconds
[2019-06-18 12:08:03] selfplay mn: 31.079 seconds
[2019-06-18 12:08:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-26-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=26 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779857 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559688 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339519 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119350 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899181 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679012 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458843 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238674 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018505 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798336 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578167 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357998 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137829 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917660 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697491 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477322 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257153 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036984 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816815 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596646 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376477 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156308 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:08:05] train finished: 43.930 seconds
:::MLL 1560881246.985922 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881246.986739 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881246.987418 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.100688 47885092766592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881246.984931 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881246.985747 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881246.986558 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.100677 47766593794944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.101755 47766593794944 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpr8yrldmc
W0618 12:07:27.101785 47885092766592 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjadbq8bk
I0618 12:07:27.102846 47766593794944 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpr8yrldmc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b71cf714e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.102872 47885092766592 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjadbq8bk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d66880e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.103331 47885092766592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.103323 47766593794944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.108647 47885092766592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.108655 47766593794944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.130298 47885092766592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.130484 47766593794944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881247.030287 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.031177 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.032057 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.143585 47962646152064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.038539 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.039301 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.039961 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.143620 47974520611712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.144663 47974520611712 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyfd4pxci
W0618 12:07:27.144633 47962646152064 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpybpdyj3p
I0618 12:07:27.145639 47962646152064 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpybpdyj3p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9f7512de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.145662 47974520611712 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyfd4pxci', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba238d8be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.146033 47962646152064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.146059 47974520611712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.150915 47974520611712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.150919 47962646152064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881247.043468 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.044335 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.045171 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.156123 47272319050624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.054032 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.054754 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.055437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.156292 47753868149632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.157227 47272319050624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpetl2_41r
I0618 12:07:27.158356 47272319050624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpetl2_41r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afeba5f3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:27.157625 47753868149632 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmplejmdx2f
I0618 12:07:27.158736 47753868149632 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmplejmdx2f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ed8ef4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.158789 47272319050624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.159138 47753868149632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881247.072739 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.073254 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.073698 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.162306 46987268318080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.163528 47272319050624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.163782 47753868149632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.163381 46987268318080 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp36u4cfo3
I0618 12:07:27.164375 46987268318080 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp36u4cfo3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc5c062e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.164765 46987268318080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881247.082068 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.082509 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.082865 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.167574 47249771975552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.168564 47249771975552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpgd26cz3g
I0618 12:07:27.169542 47249771975552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpgd26cz3g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af97a762e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:27.169575 46987268318080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:27.169940 47249771975552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.170145 47962646152064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.170253 47974520611712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.174461 47249771975552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.182622 47272319050624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.182403 47885092766592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.182986 47753868149632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.182885 47766593794944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.186993 47885092766592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.187516 47766593794944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.188541 46987268318080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:27.192417 47885092766592 estimator.py:1111] Calling model_fn.
W0618 12:07:27.192552 47885092766592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:27.193016 47766593794944 estimator.py:1111] Calling model_fn.
W0618 12:07:27.193137 47766593794944 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.193451 47249771975552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.194014 47885092766592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.194576 47766593794944 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.218068 47962646152064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.218517 47974520611712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881247.116905 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.117444 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.117947 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.219456 47014961038208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.123469 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.123900 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.124270 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.219604 47581547000704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.121385 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.121865 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.122325 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.220422 47887883240320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.122488 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.123035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.123441 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.220567 47170752340864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.220499 47014961038208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvy_xjws8
W0618 12:07:27.220589 47581547000704 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpo7nj3aj6
I0618 12:07:27.221489 47014961038208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvy_xjws8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac2cea39da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.221585 47581547000704 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpo7nj3aj6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b46b9cb3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.221890 47014961038208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.221989 47581547000704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.221436 47887883240320 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt_gyfjtu
W0618 12:07:27.221562 47170752340864 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa0t110gk
W0618 12:07:27.222363 47962646152064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:27.222416 47887883240320 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt_gyfjtu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e0cdb5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.222539 47170752340864 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa0t110gk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae714863da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:27.222802 47974520611712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:27.222818 47887883240320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.222940 47170752340864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.226506 47014961038208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.226559 47581547000704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:27.227397 47962646152064 estimator.py:1111] Calling model_fn.
W0618 12:07:27.227508 47962646152064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.227432 47887883240320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.227505 47170752340864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:07:27.227873 47974520611712 estimator.py:1111] Calling model_fn.
W0618 12:07:27.227983 47974520611712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.228858 47962646152064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.229338 47974520611712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.230861 47272319050624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.231037 47753868149632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.235196 47272319050624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.235360 47753868149632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.235846 46987268318080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:27.240281 47272319050624 estimator.py:1111] Calling model_fn.
W0618 12:07:27.240129 46987268318080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.240394 47272319050624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:27.240441 47753868149632 estimator.py:1111] Calling model_fn.
W0618 12:07:27.240251 47249771975552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.240549 47753868149632 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.241767 47272319050624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.241910 47753868149632 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.244534 47249771975552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:27.245156 46987268318080 estimator.py:1111] Calling model_fn.
W0618 12:07:27.245264 46987268318080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.245465 47014961038208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.245536 47581547000704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.246360 47887883240320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.246428 47170752340864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.246614 46987268318080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:07:27.249573 47249771975552 estimator.py:1111] Calling model_fn.
W0618 12:07:27.249680 47249771975552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.251035 47249771975552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881247.115486 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.116352 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.117194 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.254239 47447186457472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.116009 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.116917 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.117688 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.254389 47255261442944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.255390 47447186457472 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1z4nwn9t
I0618 12:07:27.255562 47255261442944 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afac1a8cd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.256503 47447186457472 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1z4nwn9t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2771481e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.256790 47255261442944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.256956 47447186457472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.262217 47255261442944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.262223 47447186457472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881247.169125 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.169865 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.170587 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.272625 47601870623616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.153443 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.154409 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.155305 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.272680 47993618277248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.273677 47601870623616 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmputkhgvry
W0618 12:07:27.273704 47993618277248 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmpvf8uva
I0618 12:07:27.274701 47601870623616 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmputkhgvry', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4b752d2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.274710 47993618277248 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmpvf8uva', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6ab27ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.275102 47601870623616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.275107 47993618277248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.280272 47601870623616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.280251 47993618277248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.284046 47447186457472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.284553 47255261442944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.292683 47014961038208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.292804 47581547000704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.293468 47887883240320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.293742 47170752340864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.296987 47014961038208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.297103 47581547000704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.297749 47887883240320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.298038 47170752340864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:27.299430 47993618277248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.299706 47601870623616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:27.302035 47014961038208 estimator.py:1111] Calling model_fn.
W0618 12:07:27.302142 47014961038208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:27.302180 47581547000704 estimator.py:1111] Calling model_fn.
W0618 12:07:27.302289 47581547000704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:27.302796 47887883240320 estimator.py:1111] Calling model_fn.
W0618 12:07:27.302904 47887883240320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:27.303093 47170752340864 estimator.py:1111] Calling model_fn.
W0618 12:07:27.303204 47170752340864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:27.303482 47014961038208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.303653 47581547000704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.304275 47887883240320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:27.304555 47170752340864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881247.185787 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.186292 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.186706 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.304532 47819056890752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881247.191752 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.192158 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.192512 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.304677 47352076194688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.305555 47819056890752 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1yrcrppa
W0618 12:07:27.305653 47352076194688 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa7url5p0
I0618 12:07:27.306534 47819056890752 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1yrcrppa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7e067c9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.306622 47352076194688 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa7url5p0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b114c448e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:27.306938 47819056890752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:27.307019 47352076194688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:27.311605 47819056890752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.311664 47352076194688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:27.330648 47352076194688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:27.330611 47819056890752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881247.241552 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.242133 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.242675 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.332594 47627581436800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:07:27.332799 47255261442944 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.332799 47447186457472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:27.333620 47627581436800 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcl41l47a
:::MLL 1560881247.140432 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881247.141381 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881247.142328 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:27.333667 47703502619520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000015-000009.tfrecord.zz_0_0
I0618 12:07:27.334617 47627581436800 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcl41l47a', '_tf_random[2019-06-18 12:08:06] divide_golden_chunk finished: 3.513 seconds
[2019-06-18 12:08:06] generate golden chunk: 3.527 seconds
[2019-06-18 12:08:06] iteration time 24: 49.868 seconds
2019-06-18 12:08:08.348159: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881286.976063 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:08:11] minmax time: 3.240 seconds
2019-06-18 12:08:11.598486: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:08:11.603894: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:08:11.608318: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881291.620691 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 24}}
[2019-06-18 12:08:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:08:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=26 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=1023779857 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=2047559688 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=3071339519 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=4095119350 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=5118899181 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=6142679012 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=7166458843 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=8190238674 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=9214018505 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=10237798336 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=11261578167 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=12285357998 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=13309137829 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=14332917660 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=15356697491 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=16380477322 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=17404257153 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=18428036984 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=19451816815 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000025-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000025-000017 --seed=20475596646 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:22] eval finished: 10.818 seconds
[2019-06-18 12:08:22] Win rate 000025-000017 vs 000023-000016: 0.480
:::MLL 1560881302.512728 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:08:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=27 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=1023779858 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=2047559689 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=3071339520 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=4095119351 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=5118899182 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=6142679013 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=7166458844 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=8190238675 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=9214018506 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=10237798337 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=11261578168 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=12285357999 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=13309137830 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=14332917661 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=15356697492 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=16380477323 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=17404257154 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000026-000016 --seed=18428036985 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:08:52] selfplay finished: 29.725 seconds
[2019-06-18 12:08:52] selfplay mn: 29.746 seconds
[2019-06-18 12:08:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-27-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=27 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779858 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559689 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339520 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119351 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899182 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679013 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458844 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238675 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018506 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798337 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578168 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285357999 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137830 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917661 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697492 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477323 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257154 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036985 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816816 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596647 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376478 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156309 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000026-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:08:55] train finished: 43.972 seconds
:::MLL 1560881296.894063 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.894898 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.895723 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.009769 47698583786368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881296.898787 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.899491 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.900160 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.009919 47417105273728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.010943 47698583786368 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpx_1ykqoc
W0618 12:08:17.011006 47417105273728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_b_7yz2g
I0618 12:08:17.012074 47698583786368 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpx_1ykqoc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b61f9bade48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.012105 47417105273728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_b_7yz2g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20704dae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.012543 47698583786368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:17.012551 47417105273728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.017982 47698583786368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.017949 47417105273728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.039760 47698583786368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.039987 47417105273728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881296.949909 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.950756 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.951519 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.074433 46919576109952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881296.950794 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.951642 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.952345 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.075697 47637741577088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.075486 46919576109952 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt459reaj
I0618 12:08:17.076467 46919576109952 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt459reaj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aac99410e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.076867 46919576109952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.076690 47637741577088 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp5w1_bpyx
I0618 12:08:17.077661 47637741577088 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp5w1_bpyx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53cf407e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.078062 47637741577088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.081647 46919576109952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.082764 47637741577088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881296.993401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.993837 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.994180 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.087732 47474146587520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881296.996467 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.996840 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.997196 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.089124 47326697173888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.088770 47474146587520 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpb2rdew9w
I0618 12:08:17.089765 47474146587520 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpb2rdew9w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2db83b0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.090168 47474146587520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.090097 47326697173888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa61gwsrb
I0618 12:08:17.091074 47326697173888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa61gwsrb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b638f8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.091480 47326697173888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.092246 47698583786368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.092382 47417105273728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.094945 47474146587520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.096217 47326697173888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.096557 47698583786368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.096711 47417105273728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.100980 46919576109952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:17.101620 47698583786368 estimator.py:1111] Calling model_fn.
W0618 12:08:17.101729 47698583786368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:17.101787 47417105273728 estimator.py:1111] Calling model_fn.
W0618 12:08:17.101894 47417105273728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:17.102214 47637741577088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.103100 47698583786368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.103247 47417105273728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.114032 47474146587520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.115350 47326697173888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881296.951644 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.952536 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.953400 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.125484 48007829209984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881296.957724 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881296.958455 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881296.959107 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.125629 47011432506240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.126590 48007829209984 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpw0821i7d
W0618 12:08:17.126777 47011432506240 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpitnsfvsq
I0618 12:08:17.127668 48007829209984 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpw0821i7d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba9fa319da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.127820 47011432506240 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpitnsfvsq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac1fc527e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.128075 48007829209984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:17.128213 47011432506240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.132909 47011432506240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.132910 48007829209984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881297.023689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.024198 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.024653 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.134895 46990549906304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881297.024494 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.024980 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.025398 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.135561 47906380006272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.135977 46990549906304 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmput23n_0o
I0618 12:08:17.136965 46990549906304 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmput23n_0o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abd1f9f2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:17.136551 47906380006272 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpccvul0ax
I0618 12:08:17.137362 46990549906304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:17.137531 47906380006272 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpccvul0ax', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b925b598da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.137944 47906380006272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.142152 46990549906304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.142625 47906380006272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881297.044657 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.045390 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.046108 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.144258 47199602389888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881297.028749 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.029681 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.030542 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.144289 47542776492928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.145315 47199602389888 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1j82r49m
W0618 12:08:17.145367 47542776492928 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi8tdcntw
I0618 12:08:17.146480 47199602389888 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1j82r49m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aedcc1efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.146486 47542776492928 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi8tdcntw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3db2e42e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.146912 47199602389888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:17.146919 47542776492928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.148919 46919576109952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.149658 47637741577088 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.151793 47199602389888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.151808 47542776492928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.152450 48007829209984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.152727 47011432506240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.153263 46919576109952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.153975 47637741577088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:17.158339 46919576109952 estimator.py:1111] Calling model_fn.
W0618 12:08:17.158450 46919576109952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:17.159039 47637741577088 estimator.py:1111] Calling model_fn.
W0618 12:08:17.159145 47637741577088 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:17.159806 46919576109952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.160485 47637741577088 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.161326 46990549906304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.161627 47906380006272 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.161509 47474146587520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.162234 47326697173888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881297.053647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.054363 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.055057 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.161861 47180448711552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881297.047962 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.048890 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.049801 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.162474 47244772299648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
I0618 12:08:17.163049 47180448711552 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae956791d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.164331 47180448711552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.163617 47244772299648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpr6a996ty
I0618 12:08:17.164742 47244772299648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpr6a996ty', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af850752dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.165218 47244772299648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.165810 47474146587520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.166531 47326697173888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881297.024824 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.025350 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.025841 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.169227 46999946376064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881297.030042 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.030532 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.030964 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.169666 46963176952704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.169803 47180448711552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.170236 46999946376064 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmper8s9ted
I0618 12:08:17.170850 47474146587520 estimator.py:1111] Calling model_fn.
I0618 12:08:17.171228 46999946376064 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmper8s9ted', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abf4fb20e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:17.170961 47474146587520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:17.170474 47244772299648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.170636 46963176952704 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp161fd7p_
W0618 12:08:17.171215 47199602389888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:17.171607 46963176952704 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp161fd7p_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab6c0112e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:17.171622 46999946376064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.171522 47542776492928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:17.171572 47326697173888 estimator.py:1111] Calling model_fn.
W0618 12:08:17.171679 47326697173888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:17.172008 46963176952704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:17.172317 47474146587520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.173033 47326697173888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.176272 46999946376064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.176601 46963176952704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:17.191895 47180448711552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.192531 47244772299648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.195407 46999946376064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.195760 46963176952704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:17.200304 48007829209984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.200597 47011432506240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.204620 48007829209984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.204936 47011432506240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.208651 47906380006272 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.208883 46990549906304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:08:17.209681 48007829209984 estimator.py:1111] Calling model_fn.
W0618 12:08:17.209798 48007829209984 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:17.209996 47011432506240 estimator.py:1111] Calling model_fn.
W0618 12:08:17.210105 47011432506240 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:17.211161 48007829209984 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.211467 47011432506240 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.212946 47906380006272 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:17.213201 46990549906304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:17.217984 47906380006272 estimator.py:1111] Calling model_fn.
W0618 12:08:17.218092 47906380006272 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:17.218265 46990549906304 estimator.py:1111] Calling model_fn.
W0618 12:08:17.218376 46990549906304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:17.219446 47906380006272 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.219746 46990549906304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:17.219717 47199602389888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:17.220549 47542776492928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881297.121857 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.122316 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.122690 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.222055 47473249960832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
:::MLL 1560881297.121295 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881297.121762 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881297.122158 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:17.222011 47550347154304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000016-000009.tfrecord.zz_0_0
W0618 12:08:17.223099 47473249960832 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmb159lfc
W0618 12:08:17.223068 47550347154304 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsnbsphhb
I0618 12:08:17.224042 47550347154304 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsnbsphhb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f76236da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:17.224004 47199602389888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:17.224083 47473249960832 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmb159lfc', '_tf_random_seed': None, '_save_summary_steps': 1[2019-06-18 12:08:55] divide_golden_chunk finished: 3.529 seconds
[2019-06-18 12:08:55] generate golden chunk: 3.542 seconds
[2019-06-18 12:08:55] iteration time 25: 48.826 seconds
2019-06-18 12:08:57.181923: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881335.802603 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:09:00] minmax time: 3.230 seconds
2019-06-18 12:09:00.421146: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:09:00.426639: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:09:00.431192: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881340.443587 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 25}}
[2019-06-18 12:09:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000027-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:09:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=27 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=1023779858 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=2047559689 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=3071339520 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=4095119351 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=5118899182 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=6142679013 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=7166458844 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=8190238675 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=9214018506 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=10237798337 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=11261578168 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=12285357999 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=13309137830 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=14332917661 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=15356697492 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=16380477323 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=17404257154 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=18428036985 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=19451816816 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000026-000017 --seed=20475596647 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:09:11] eval finished: 11.189 seconds
[2019-06-18 12:09:11] Win rate 000026-000017 vs 000023-000016: 0.570
:::MLL 1560881351.707430 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:09:11] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=28 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=1023779859 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=2047559690 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=3071339521 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=4095119352 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=5118899183 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=6142679014 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=7166458845 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=8190238676 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=9214018507 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=10237798338 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=11261578169 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=12285358000 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=13309137831 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=14332917662 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=15356697493 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=16380477324 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=17404257155 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000027-000016 --seed=18428036986 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:09:42] selfplay finished: 30.463 seconds
[2019-06-18 12:09:42] selfplay mn: 30.483 seconds
[2019-06-18 12:09:42] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-28-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=28 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779859 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559690 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339521 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119352 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899183 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679014 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458845 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238676 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018507 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798338 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578169 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285358000 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137831 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917662 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697493 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477324 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257155 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036986 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816817 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596648 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376479 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156310 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000027-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:09:43] train finished: 43.540 seconds
:::MLL 1560881345.728361 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.729277 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.729934 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.861208 47838828467072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.862315 47838828467072 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyi0cj__7
I0618 12:09:05.863439 47838828467072 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyi0cj__7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b82a0f6de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.863889 47838828467072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.868885 47838828467072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881345.732138 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.732888 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.733575 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.874517 47999130612608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.875591 47999130612608 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpojv3t5xq
I0618 12:09:05.876631 47999130612608 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpojv3t5xq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7f3b78e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.877036 47999130612608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.881656 47999130612608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881345.743891 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.744648 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.745324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.883620 47449442177920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.746273 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.746994 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.747709 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.883943 47096026694528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.884732 47449442177920 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuaxegffk
I0618 12:09:05.885837 47449442177920 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuaxegffk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b27f7bbae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:05.885039 47096026694528 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpn_lt2pyp
I0618 12:09:05.886153 47096026694528 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpn_lt2pyp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5ae875dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.886292 47449442177920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.886604 47096026694528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.888437 47838828467072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.891612 47449442177920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.891817 47096026694528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.900858 47999130612608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.913415 47449442177920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.913786 47096026694528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881345.732966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.733824 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.734621 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.919586 47335001924480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.737200 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.737923 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.738584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.919699 48003108676480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.920816 47335001924480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpokp0j3bk
W0618 12:09:05.921011 48003108676480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyev5kdzx
I0618 12:09:05.921895 47335001924480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpokp0j3bk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d528ffe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.922040 48003108676480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyev5kdzx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba8e0d3eda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.922300 47335001924480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.922434 48003108676480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881345.825294 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.826054 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.826816 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.925698 47345532281728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.806592 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.807507 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.808418 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.925699 47263204721536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.802717 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.803332 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.803804 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.926100 47554546418560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.807493 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.807961 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.808383 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.926976 47630032941952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.927072 47335001924480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.927125 48003108676480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.926805 47263204721536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmptqikwmni
W0618 12:09:05.927104 47554546418560 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpm5p4fmip
W0618 12:09:05.926833 47345532281728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp63ms81df
I0618 12:09:05.928078 47554546418560 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpm5p4fmip', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40706f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.927908 47263204721536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmptqikwmni', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc9b1d9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.927934 47345532281728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp63ms81df', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0fc6386e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.928480 47554546418560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.928362 47263204721536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.928385 47345532281728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.927963 47630032941952 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpk6lndg6g
I0618 12:09:05.928930 47630032941952 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpk6lndg6g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5203c80e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.929329 47630032941952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.933176 47554546418560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.933483 47345532281728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.933507 47263204721536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.933841 47630032941952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.936692 47838828467072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881345.826878 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.827299 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.827664 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.940377 47371252237184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.823102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.823606 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.824064 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.940612 46924391252864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.941072 47838828467072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:05.941416 47371252237184 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpn6xjd4s5
W0618 12:09:05.941604 46924391252864 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7am_al9a
I0618 12:09:05.942409 47371252237184 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpn6xjd4s5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b15c33fde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.942579 46924391252864 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7am_al9a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aadb8425e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.942808 47371252237184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.942969 46924391252864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.946201 47838828467072 estimator.py:1111] Calling model_fn.
W0618 12:09:05.946312 47838828467072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:05.946919 47335001924480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.947166 48003108676480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.947689 47838828467072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:05.947481 47371252237184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.947548 46924391252864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.948227 47999130612608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.952073 47554546418560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.952551 47999130612608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:05.952940 47630032941952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.952889 47263204721536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.952926 47345532281728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:09:05.957644 47999130612608 estimator.py:1111] Calling model_fn.
W0618 12:09:05.957755 47999130612608 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:05.959122 47999130612608 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881345.804192 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.804656 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.805113 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.960308 47478683534208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.805908 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.806362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.806761 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.961779 47836189062016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.961338 47478683534208 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbabhxdnp
I0618 12:09:05.962339 47478683534208 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbabhxdnp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ec6a75da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.962740 47478683534208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.962697 47449442177920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.962691 47096026694528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.962772 47836189062016 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkntttzk1
I0618 12:09:05.963745 47836189062016 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkntttzk1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8203a4cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.964140 47836189062016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881345.836881 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.837802 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.838719 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.964433 47077867164544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.841745 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.842467 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.843179 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.964853 47763235955584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.966574 46924391252864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.966620 47371252237184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.965572 47077867164544 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpnan_y1e9
W0618 12:09:05.967000 47449442177920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:05.967003 47096026694528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:05.966667 47077867164544 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpnan_y1e9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad17422ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:05.967409 47478683534208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:05.966019 47763235955584 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b71074cbcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.967108 47077867164544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:05.967294 47763235955584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.968647 47836189062016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:05.972093 47449442177920 estimator.py:1111] Calling model_fn.
I0618 12:09:05.972066 47096026694528 estimator.py:1111] Calling model_fn.
W0618 12:09:05.972177 47096026694528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:05.972201 47449442177920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:05.972408 47077867164544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.972742 47763235955584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:05.973567 47449442177920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:05.973526 47096026694528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:05.986460 47478683534208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.987743 47836189062016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.994873 48003108676480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.995141 47335001924480 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.994690 47077867164544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:05.995203 47763235955584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881345.901691 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.902170 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.902577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.996639 47532358968192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560881345.901539 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881345.902011 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881345.902422 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:05.996914 47991105483648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000026-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:09:05.997664 47532358968192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphg1gwogd
W0618 12:09:05.997910 47991105483648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpp0ef12th
I0618 12:09:05.998637 47532358968192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmphg1gwogd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3b45f57e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.998888 47991105483648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpp0ef12th', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba61561ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:05.999037 47532358968192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.999122 47554546418560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:05.999188 48003108676480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:05.999290 47991105483648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:05.999469 47335001924480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:05.999939 47630032941952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:06.001231 47263204721536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:06.001585 47345532281728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:06.003411 47554546418560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:06.003755 47532358968192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:06.003912 47991105483648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:06.004242 47630032941952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:06.004258 48003108676480 estimator.py:1111] Calling model_fn.
W0618 12:09:06.004370 48003108676480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:06.004526 47335001924480 estimator.py:1111] Calling model_fn.
W0618 12:09:06.004638 47335001924480 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:06.005741 48003108676480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:06.005540 47263204721536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:06.005997 47335001924480 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:06.005912 47345532281728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:06.008432 47554546418560 estimator.py:1111] Calling model_fn.
W0618 12:09:06.008536 47554546418560 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:06.009309 47630032941952 estimator.py:1111] Calling model_fn.
W0618 12:09:06.009418 47630032941952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:06.009872 47554546418560 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:06.010766 47630032941952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:09:06.010601 47263204721536 estimator.py:1111] Calling model_fn.
W0618 12:09:06.010710 47263204721536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:06.010997 47345532281728 estimator.py:1111] Calling model_fn.
W0618 12:09:06.011109 47345532281728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:06.012078 47263204721536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:06.012462 47345532281728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:06.013829 46924391252864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy([2019-06-18 12:09:45] divide_golden_chunk finished: 3.358 seconds
[2019-06-18 12:09:45] generate golden chunk: 3.372 seconds
[2019-06-18 12:09:45] moving /lfs/lfs12/gma_akey/results/epb134/models/000027-000017.index --> /lfs/lfs12/gma_akey/results/epb134/models/000027-000018.index
[2019-06-18 12:09:45] moving /lfs/lfs12/gma_akey/results/epb134/models/000027-000017.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000027-000018.meta
[2019-06-18 12:09:45] moving /lfs/lfs12/gma_akey/results/epb134/models/000027-000017.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb
[2019-06-18 12:09:45] moving /lfs/lfs12/gma_akey/results/epb134/models/000027-000017.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000027-000018.data-00000-of-00001
[2019-06-18 12:09:45] iteration time 26: 49.801 seconds
2019-06-18 12:09:47.039880: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881385.603508 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:09:50] minmax time: 3.238 seconds
2019-06-18 12:09:50.287875: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:09:50.293317: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:09:50.297982: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881390.309522 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 26}}
[2019-06-18 12:09:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000028-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:09:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=28 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=1023779859 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=2047559690 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=3071339521 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=4095119352 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=5118899183 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=6142679014 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=7166458845 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=8190238676 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=9214018507 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=10237798338 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=11261578169 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=12285358000 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=13309137831 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=14332917662 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=15356697493 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=16380477324 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=17404257155 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=18428036986 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=19451816817 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000026-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000027-000018 --seed=20475596648 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:10:01] eval finished: 10.839 seconds
[2019-06-18 12:10:01] Win rate 000027-000018 vs 000026-000017: 0.560
:::MLL 1560881401.224819 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:10:01] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=29 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=1023779860 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=2047559691 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=3071339522 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=4095119353 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=5118899184 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=6142679015 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=7166458846 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=8190238677 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=9214018508 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=10237798339 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=11261578170 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=12285358001 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=13309137832 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=14332917663 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=15356697494 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=16380477325 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=17404257156 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000028-000017 --seed=18428036987 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:10:31] selfplay finished: 30.077 seconds
[2019-06-18 12:10:31] selfplay mn: 30.099 seconds
[2019-06-18 12:10:31] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-29-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=29 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779860 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559691 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339522 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119353 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899184 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679015 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458846 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238677 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018508 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798339 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578170 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285358001 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137832 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917663 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697494 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477325 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257156 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036987 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816818 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596649 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376480 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156311 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000028-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:10:33] train finished: 43.423 seconds
:::MLL 1560881395.574342 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.575233 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.575979 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.709438 47082045879168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.574175 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.575017 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.575808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.709803 47909426815872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.710572 47082045879168 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpj58f5obk
I0618 12:09:55.711681 47082045879168 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpj58f5obk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad26d350e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:55.710916 47909426815872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvicv7ppa
I0618 12:09:55.712009 47909426815872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvicv7ppa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9310f43e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.712131 47082045879168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.712456 47909426815872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.717411 47082045879168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.717677 47909426815872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.739339 47082045879168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.739713 47909426815872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881395.628510 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.629375 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.630197 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.750488 47937579484032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.628381 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.629231 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.630034 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.750896 47651505099648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.751522 47937579484032 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0mempxqn
I0618 12:09:55.752522 47937579484032 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0mempxqn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b999efbde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:55.751901 47651505099648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpug4wx2dz
I0618 12:09:55.752904 47651505099648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpug4wx2dz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b57039f1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.752932 47937579484032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.753314 47651505099648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881395.593375 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.594244 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.595017 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.753115 47335462450048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.593302 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.594139 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.594929 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.753196 47514642158464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.754241 47335462450048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt9gwnz9p
W0618 12:09:55.754458 47514642158464 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpfa87bv3u
I0618 12:09:55.755345 47335462450048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt9gwnz9p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d6e030da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.755539 47514642158464 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpfa87bv3u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3725f44e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.755752 47335462450048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.755937 47514642158464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.757662 47937579484032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.757997 47651505099648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.760524 47335462450048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.760583 47514642158464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881395.662175 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.662591 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.662951 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.771995 47442859570048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.657999 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.658492 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.658911 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.772022 47021749040000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.773021 47442859570048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpyhtcs3ht
W0618 12:09:55.773050 47021749040000 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpcrg4mlzf
I0618 12:09:55.774006 47442859570048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpyhtcs3ht', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b266f60fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.774034 47021749040000 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpcrg4mlzf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4633c3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.774416 47442859570048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.774430 47021749040000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.776975 47937579484032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.777654 47651505099648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.779098 47442859570048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.779123 47021749040000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.780073 47514642158464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.780134 47335462450048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.788717 47082045879168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.788815 47909426815872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.793002 47082045879168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.793130 47909426815872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.798001 47442859570048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:09:55.798078 47082045879168 estimator.py:1111] Calling model_fn.
W0618 12:09:55.798050 47021749040000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.798194 47082045879168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:55.798183 47909426815872 estimator.py:1111] Calling model_fn.
W0618 12:09:55.798290 47909426815872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:55.799565 47082045879168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.799652 47909426815872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881395.677748 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.678672 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.679550 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.801560 47319112344448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.696284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.696971 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.697647 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.801624 47577221542784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.802623 47319112344448 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpwr_st9oe
W0618 12:09:55.802664 47577221542784 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpygacyl9r
:::MLL 1560881395.663855 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.664340 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.664741 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.803537 47687689270144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
I0618 12:09:55.803714 47319112344448 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpwr_st9oe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b099f782da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881395.665628 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.666117 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.666536 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.803555 47438137799552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
I0618 12:09:55.803786 47577221542784 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpygacyl9r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b45b7f9fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.804172 47319112344448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.804238 47577221542784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.804570 47687689270144 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpq5sobgur
W0618 12:09:55.804539 47438137799552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpa9cqx_hj
I0618 12:09:55.805524 47438137799552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpa9cqx_hj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2555f08da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.805558 47687689270144 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpq5sobgur', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f705dce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.805926 47438137799552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.805960 47687689270144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.809060 47577221542784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.809080 47319112344448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.810601 47687689270144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.810601 47438137799552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881395.705313 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.705781 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.706204 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.815636 47975938380672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.703054 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.703524 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.703923 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.816050 47479528838016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.686306 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.686914 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.687577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.816232 47340494513024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.816666 47975938380672 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt3c522__
:::MLL 1560881395.688701 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.689359 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.690037 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.816248 47520335930240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
I0618 12:09:55.817649 47975938380672 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt3c522__', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba28d5a2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:55.817030 47479528838016 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpbb4te45i
I0618 12:09:55.818001 47479528838016 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpbb4te45i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ef909ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.818054 47975938380672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.818398 47479528838016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.817339 47340494513024 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpgfs6n0_z
I0618 12:09:55.817397 47520335930240 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3879547d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.818430 47340494513024 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpgfs6n0_z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0e99f23e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:55.818659 47520335930240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:55.818870 47340494513024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:55.822703 47975938380672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.822980 47479528838016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.823946 47520335930240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.824081 47340494513024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:55.825078 47937579484032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.825426 47651505099648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.827847 47514642158464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.828476 47335462450048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.828716 47577221542784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.828717 47319112344448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.829372 47937579484032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.829604 47687689270144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.829736 47651505099648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.829825 47438137799552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.832145 47514642158464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.832833 47335462450048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:55.834418 47937579484032 estimator.py:1111] Calling model_fn.
W0618 12:09:55.834527 47937579484032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:55.834783 47651505099648 estimator.py:1111] Calling model_fn.
W0618 12:09:55.834895 47651505099648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:55.835881 47937579484032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.836265 47651505099648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:09:55.837198 47514642158464 estimator.py:1111] Calling model_fn.
W0618 12:09:55.837311 47514642158464 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:55.837927 47335462450048 estimator.py:1111] Calling model_fn.
W0618 12:09:55.838038 47335462450048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:55.838681 47514642158464 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.839395 47335462450048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.841581 47975938380672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.841852 47479528838016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.845098 47442859570048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.845505 47021749040000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.845950 47340494513024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.845963 47520335930240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:55.849373 47442859570048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.849834 47021749040000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:55.854400 47442859570048 estimator.py:1111] Calling model_fn.
W0618 12:09:55.854510 47442859570048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:55.854884 47021749040000 estimator.py:1111] Calling model_fn.
W0618 12:09:55.854990 47021749040000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:55.855859 47442859570048 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.856330 47021749040000 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:55.876879 47687689270144 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.877538 47438137799552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.877518 47577221542784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.877671 47319112344448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:55.881184 47687689270144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.881865 47438137799552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.881821 47577221542784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:55.881992 47319112344448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881395.772016 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.772734 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.773267 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.882086 46921216406400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.772548 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.773143 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.773584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.882223 47682188878720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.780110 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.780565 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.780972 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.881869 47284794467200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
:::MLL 1560881395.780915 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881395.781350 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881395.781724 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:55.881948 47430750987136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000027-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000018-000010.tfrecord.zz_0_0
W0618 12:09:55.883111 46921216406400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpt9uiln7g
W0618 12:09:55.883206 47682188878720 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp3tj3v1zs
I0618 12:09:55.884084 46921216406400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpt9uiln7g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacfb05fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas':[2019-06-18 12:10:34] divide_golden_chunk finished: 3.327 seconds
[2019-06-18 12:10:34] generate golden chunk: 3.341 seconds
[2019-06-18 12:10:34] moving /lfs/lfs12/gma_akey/results/epb134/models/000028-000018.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000028-000019.meta
[2019-06-18 12:10:34] moving /lfs/lfs12/gma_akey/results/epb134/models/000028-000018.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000028-000019.data-00000-of-00001
[2019-06-18 12:10:34] moving /lfs/lfs12/gma_akey/results/epb134/models/000028-000018.index --> /lfs/lfs12/gma_akey/results/epb134/models/000028-000019.index
[2019-06-18 12:10:34] moving /lfs/lfs12/gma_akey/results/epb134/models/000028-000018.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb
[2019-06-18 12:10:34] iteration time 27: 49.108 seconds
2019-06-18 12:10:36.207637: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881434.711900 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:10:39] minmax time: 3.265 seconds
2019-06-18 12:10:39.482689: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:10:39.488076: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:10:39.492698: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881439.503892 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 27}}
[2019-06-18 12:10:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000029-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:10:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=29 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=1023779860 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=2047559691 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=3071339522 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=4095119353 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=5118899184 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=6142679015 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=7166458846 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=8190238677 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=9214018508 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=10237798339 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=11261578170 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=12285358001 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=13309137832 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=14332917663 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=15356697494 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=16380477325 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=17404257156 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=18428036987 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=19451816818 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000027-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000028-000019 --seed=20475596649 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:10:50] eval finished: 11.000 seconds
[2019-06-18 12:10:50] Win rate 000028-000019 vs 000027-000018: 0.610
:::MLL 1560881450.580713 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:10:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=30 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=1023779861 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=2047559692 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=3071339523 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=4095119354 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=5118899185 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=6142679016 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=7166458847 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=8190238678 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=9214018509 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=10237798340 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=11261578171 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=12285358002 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=13309137833 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=14332917664 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=15356697495 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=16380477326 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=17404257157 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000029-000018 --seed=18428036988 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:11:20] selfplay finished: 30.014 seconds
[2019-06-18 12:11:20] selfplay mn: 30.032 seconds
[2019-06-18 12:11:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-30-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=30 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779861 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559692 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339523 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119354 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899185 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679016 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458847 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238678 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018509 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798340 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578171 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285358002 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137833 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917664 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697495 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477326 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257157 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036988 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816819 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596650 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376481 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156312 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000029-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:11:23] train finished: 43.744 seconds
:::MLL 1560881444.777393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.778247 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.778977 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.901387 47769206989696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.902496 47769206989696 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjw1fzwp7
I0618 12:10:44.903638 47769206989696 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjw1fzwp7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b726b337e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.904093 47769206989696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881444.781256 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.781948 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.782605 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.905214 47851761488768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.906330 47851761488768 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp9i0qi84k
I0618 12:10:44.907413 47851761488768 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp9i0qi84k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b85a3d52e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.907913 47851761488768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:44.909379 47769206989696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.913228 47851761488768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881444.789401 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.790138 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.790822 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.923328 47965753734016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.786082 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.786854 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.787508 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.923404 47028994831232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.924345 47965753734016 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp754z__3b
W0618 12:10:44.924405 47028994831232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp3huyinah
I0618 12:10:44.925409 47965753734016 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp754z__3b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba02e4cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.925477 47028994831232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp3huyinah', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac6131e4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.925853 47965753734016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:44.925926 47028994831232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:44.930609 47965753734016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.930606 47028994831232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.931169 47769206989696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.935317 47851761488768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.949809 47965753734016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.949880 47028994831232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881444.832233 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.833131 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.833974 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.952863 47823141999488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.848017 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.848757 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.849434 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.953168 47696622936960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.953906 47823141999488 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpar3o2bok
I0618 12:10:44.954904 47823141999488 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpar3o2bok', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ef9fa6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:44.954200 47696622936960 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7b0ncax0
I0618 12:10:44.955213 47696622936960 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7b0ncax0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6184dabe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.955302 47823141999488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:44.955617 47696622936960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:44.960175 47823141999488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.960394 47696622936960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881444.869393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.869799 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.870161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.973223 47790835803008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.866506 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.866996 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.867360 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.973220 47726228054912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.974253 47790835803008 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpclj83kcf
W0618 12:10:44.974283 47726228054912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpazvkk8z_
I0618 12:10:44.975237 47790835803008 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpclj83kcf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b777460fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.975270 47726228054912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpazvkk8z_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b686974fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.975638 47790835803008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:44.975676 47726228054912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:44.979345 47823141999488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.979762 47696622936960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.980360 47790835803008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.980361 47726228054912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.981065 47769206989696 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:44.984459 47851761488768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:44.985367 47769206989696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881444.874946 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.875422 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.875839 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.987339 47307886392192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.877111 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.877626 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.878043 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:44.987493 47209685668736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:44.988380 47307886392192 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpss394dw4
W0618 12:10:44.988758 47851761488768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:44.988517 47209685668736 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpmqqnzk6p
I0618 12:10:44.989364 47307886392192 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpss394dw4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b070259be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.989510 47209685668736 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpmqqnzk6p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af025219e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:44.989759 47307886392192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:44.989911 47209685668736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:44.990398 47769206989696 estimator.py:1111] Calling model_fn.
W0618 12:10:44.990505 47769206989696 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:44.991839 47769206989696 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:10:44.993814 47851761488768 estimator.py:1111] Calling model_fn.
W0618 12:10:44.993920 47851761488768 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:44.994418 47307886392192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.994470 47209685668736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:44.995299 47851761488768 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:44.998549 47028994831232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:44.998831 47965753734016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:44.999359 47726228054912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:44.999487 47790835803008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.002838 47028994831232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.003140 47965753734016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:45.007871 47028994831232 estimator.py:1111] Calling model_fn.
W0618 12:10:45.007977 47028994831232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:45.008173 47965753734016 estimator.py:1111] Calling model_fn.
W0618 12:10:45.008284 47965753734016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:45.009340 47028994831232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.009647 47965753734016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.013389 47209685668736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.013422 47307886392192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.027377 47823141999488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.027922 47696622936960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.031692 47823141999488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.032213 47696622936960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881444.937144 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.937566 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.937938 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.033267 47813341488000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.935906 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.936317 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.936669 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.033263 47819212329856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:45.034265 47813341488000 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7vh_pems
W0618 12:10:45.034296 47819212329856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc9qojpeu
I0618 12:10:45.035245 47813341488000 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7vh_pems', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7cb1d27da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:45.035284 47819212329856 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpc9qojpeu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7e0fc06da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:45.035646 47813341488000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:45.035681 47819212329856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:45.036730 47823141999488 estimator.py:1111] Calling model_fn.
W0618 12:10:45.036844 47823141999488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:45.037267 47696622936960 estimator.py:1111] Calling model_fn.
W0618 12:10:45.037377 47696622936960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:45.038201 47823141999488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.038750 47696622936960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.040388 47819212329856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:45.040397 47813341488000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881444.909981 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.910974 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.911856 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.042347 47719029171072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881444.930082 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.930851 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.931522 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.042807 47854778942336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:45.043397 47719029171072 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpr4_2f313
I0618 12:10:45.044410 47719029171072 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpr4_2f313', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b66bc5ebda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:45.043796 47854778942336 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1yvwp_9b
I0618 12:10:45.044806 47854778942336 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1yvwp_9b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8657afce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:45.044819 47719029171072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:45.045214 47854778942336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:45.046688 47726228054912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.047021 47790835803008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.049674 47719029171072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:45.049991 47854778942336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:45.050977 47726228054912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.051358 47790835803008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:45.056013 47726228054912 estimator.py:1111] Calling model_fn.
W0618 12:10:45.056125 47726228054912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:45.056446 47790835803008 estimator.py:1111] Calling model_fn.
W0618 12:10:45.056553 47790835803008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:45.057474 47726228054912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.057923 47790835803008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.059493 47813341488000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.059552 47819212329856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.060553 47209685668736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.061129 47307886392192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.064822 47209685668736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.065466 47307886392192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881444.900385 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.901350 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.902237 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.066982 47720990262144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:45.068905 47719029171072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.069386 47854778942336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:10:45.068147 47720990262144 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6731429d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:45.069854 47209685668736 estimator.py:1111] Calling model_fn.
W0618 12:10:45.069957 47209685668736 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:45.069411 47720990262144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:45.070536 47307886392192 estimator.py:1111] Calling model_fn.
W0618 12:10:45.070642 47307886392192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:45.071330 47209685668736 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881444.905431 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881444.906173 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881444.906847 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.070882 47954759840640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:45.072007 47307886392192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:45.071990 47954759840640 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmprit48jeq
I0618 12:10:45.073104 47954759840640 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmprit48jeq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d9f033e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:45.073558 47954759840640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:45.075031 47720990262144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:45.078711 47954759840640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:45.097292 47720990262144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.100281 47954759840640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:45.106984 47813341488000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:45.107220 47819212329856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881445.002599 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881445.003098 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881445.003505 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.110614 47081885819776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
:::MLL 1560881445.001808 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881445.002379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881445.002822 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:45.110738 47380616233856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000028-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000019-000011.tfrecord.zz_0_0
W0618 12:10:45.111291 47813341488000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.111549 47819212329856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:45.111690 47081885819776 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpc80by43e
W0618 12:10:45.111820 47380616233856 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmphmebuw7r
I0618 12:10:45.112764 47081885819776 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpc80by43e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_di[2019-06-18 12:11:24] divide_golden_chunk finished: 3.414 seconds
[2019-06-18 12:11:24] generate golden chunk: 3.428 seconds
[2019-06-18 12:11:24] moving /lfs/lfs12/gma_akey/results/epb134/models/000029-000019.index --> /lfs/lfs12/gma_akey/results/epb134/models/000029-000020.index
[2019-06-18 12:11:24] moving /lfs/lfs12/gma_akey/results/epb134/models/000029-000019.meta --> /lfs/lfs12/gma_akey/results/epb134/models/000029-000020.meta
[2019-06-18 12:11:24] moving /lfs/lfs12/gma_akey/results/epb134/models/000029-000019.pb --> /lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb
[2019-06-18 12:11:24] moving /lfs/lfs12/gma_akey/results/epb134/models/000029-000019.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb134/models/000029-000020.data-00000-of-00001
[2019-06-18 12:11:24] iteration time 28: 49.370 seconds
2019-06-18 12:11:25.606128: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881484.082003 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:11:28] minmax time: 3.263 seconds
2019-06-18 12:11:28.879085: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:11:28.884413: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:11:28.888849: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881488.899826 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 28}}
[2019-06-18 12:11:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:11:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=30 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=1023779861 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=2047559692 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=3071339523 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=4095119354 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=5118899185 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=6142679016 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=7166458847 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=8190238678 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=9214018509 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=10237798340 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=11261578171 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=12285358002 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=13309137833 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=14332917664 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=15356697495 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=16380477326 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=17404257157 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=18428036988 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=19451816819 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000029-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000029-000020 --seed=20475596650 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:11:38] eval finished: 10.020 seconds
[2019-06-18 12:11:38] Win rate 000029-000020 vs 000028-000019: 0.430
:::MLL 1560881498.994071 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:11:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=31 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=1023779862 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=2047559693 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=3071339524 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=4095119355 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=5118899186 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=6142679017 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=7166458848 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=8190238679 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=9214018510 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=10237798341 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=11261578172 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=12285358003 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=13309137834 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=14332917665 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=15356697496 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=16380477327 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=17404257158 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000030-000019 --seed=18428036989 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:12:09] selfplay finished: 30.014 seconds
[2019-06-18 12:12:09] selfplay mn: 30.035 seconds
[2019-06-18 12:12:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-31-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=31 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779862 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559693 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339524 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119355 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899186 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679017 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458848 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238679 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018510 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798341 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578172 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285358003 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137834 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917665 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697496 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477327 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257158 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036989 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816820 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596651 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376482 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156313 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000030-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:12:12] divide_golden_chunk finished: 3.469 seconds
[2019-06-18 12:12:12] generate golden chunk: 3.483 seconds
[2019-06-18 12:12:12] train finished: 43.679 seconds
:::MLL 1560881494.173411 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.174163 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.174989 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.297558 47463038579584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.175011 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.175783 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.176434 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.297559 47413625619328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.298633 47413625619328 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpimjq9li8
W0618 12:11:34.298675 47463038579584 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpqibj8pzi
I0618 12:11:34.299630 47413625619328 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpimjq9li8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1fa0e65da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.299695 47463038579584 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpqibj8pzi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2b22244e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.300043 47413625619328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.300099 47463038579584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.304827 47413625619328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.304891 47463038579584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.324366 47413625619328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.324437 47463038579584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881494.214891 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.215792 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.216648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.369245 47294755517312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.370298 47294755517312 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpuhg4m4s_
I0618 12:11:34.371286 47294755517312 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpuhg4m4s_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b03f3b06e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.371692 47294755517312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.372337 47413625619328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.373317 47463038579584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881494.214913 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.215809 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.216664 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.373395 47684298396544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.374478 47684298396544 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvzuh5c95
I0618 12:11:34.375595 47684298396544 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvzuh5c95', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ea6412da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881494.200064 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.200788 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.201432 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.375392 47782614602624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
I0618 12:11:34.376090 47684298396544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.376489 47294755517312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.376721 47413625619328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.376525 47782614602624 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpny4gwyz2
W0618 12:11:34.377758 47463038579584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:34.377669 47782614602624 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpny4gwyz2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b758a5b6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.378138 47782614602624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.381181 47684298396544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:34.381980 47413625619328 estimator.py:1111] Calling model_fn.
W0618 12:11:34.382089 47413625619328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:34.382857 47463038579584 estimator.py:1111] Calling model_fn.
W0618 12:11:34.383008 47463038579584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.383588 47413625619328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.383503 47782614602624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.384497 47463038579584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881494.254520 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.255402 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.256236 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.385500 47603800150912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.255172 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.256058 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.256843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.385648 47621015040896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.386638 47603800150912 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpw6988u9h
W0618 12:11:34.386772 47621015040896 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpatqj4l2o
I0618 12:11:34.387638 47603800150912 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpw6988u9h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4be82f3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.387803 47621015040896 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpatqj4l2o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4fea45be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.388041 47603800150912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.388209 47621015040896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.392811 47603800150912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.392848 47621015040896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881494.281528 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.282047 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.282394 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.392981 47496634712960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.284565 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.284999 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.285346 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.393097 47495508530048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.202980 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.203701 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.204358 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.392915 47696147583872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.394001 47496634712960 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpn6ft8aft
W0618 12:11:34.394131 47495508530048 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmppfdxmwfe
I0618 12:11:34.394973 47496634712960 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpn6ft8aft', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b32f4a08e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:34.394003 47696147583872 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7sy1sf3s
I0618 12:11:34.395142 47495508530048 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmppfdxmwfe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b32b1805da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.395101 47696147583872 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7sy1sf3s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6168855e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.395366 47496634712960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.395551 47495508530048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.395561 47696147583872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.395904 47294755517312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.400011 47496634712960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.400224 47495508530048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.400677 47696147583872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.402437 47684298396544 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.405913 47782614602624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.412048 47603800150912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.412312 47621015040896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881494.282060 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.282830 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.283530 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.415381 47779545924480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.275249 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.276171 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.277052 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.415520 47504767341440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.416550 47779545924480 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0lgych4t
I0618 12:11:34.416673 47504767341440 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b34d95e9d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.417622 47779545924480 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0lgych4t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b74d3731e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.417937 47504767341440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.418827 47496634712960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:11:34.418058 47779545924480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.419514 47495508530048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.422351 47696147583872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.423236 47779545924480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.423321 47504767341440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881494.286950 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.287440 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.287934 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.427608 47589420168064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.288111 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.288597 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.289014 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.427607 46949350568832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.428613 47589420168064 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpi_i49aaz
W0618 12:11:34.428645 46949350568832 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp73q51hjx
I0618 12:11:34.429614 47589420168064 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpi_i49aaz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b488f124e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.429640 46949350568832 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp73q51hjx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab387f34e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.430026 47589420168064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.430050 46949350568832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.434700 47589420168064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.434701 46949350568832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881494.271373 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.271884 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.272324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.436424 47449547170688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.282012 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.282446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.282797 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.437557 47250378986368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.437472 47449547170688 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp1nskw9kl
I0618 12:11:34.438471 47449547170688 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp1nskw9kl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b27fdfdbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.438873 47449547170688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.438570 47250378986368 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpsqdymd7w
I0618 12:11:34.439568 47250378986368 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpsqdymd7w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af99ea46e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.439960 47250378986368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.443517 47449547170688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.443982 47294755517312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.444501 47250378986368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.445090 47779545924480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.445647 47504767341440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.448263 47294755517312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.449949 47684298396544 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881494.335273 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.335831 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.336303 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.450516 47525415879552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.338849 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.339319 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.339731 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.452130 46958043304832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.451551 47525415879552 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_1_nkmlk
I0618 12:11:34.452539 47525415879552 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_1_nkmlk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39a81e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:34.452938 47525415879552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:34.453354 47294755517312 estimator.py:1111] Calling model_fn.
W0618 12:11:34.453467 47294755517312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.453754 47589420168064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.453756 46949350568832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.453114 46958043304832 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpaz50a3wl
I0618 12:11:34.454073 46958043304832 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpaz50a3wl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab58e13de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:34.454262 47684298396544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:34.454475 46958043304832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:34.454784 47294755517312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.455016 47782614602624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.457612 47525415879552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:34.459078 46958043304832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:34.459388 47684298396544 estimator.py:1111] Calling model_fn.
W0618 12:11:34.459498 47684298396544 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.459310 47782614602624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.460129 47621015040896 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.460251 47603800150912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.460833 47684298396544 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.462691 47449547170688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.464433 47621015040896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.464565 47603800150912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:34.464421 47782614602624 estimator.py:1111] Calling model_fn.
W0618 12:11:34.464533 47782614602624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.464614 47250378986368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:34.465562 47496634712960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.465906 47782614602624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.467343 47495508530048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:34.469531 47621015040896 estimator.py:1111] Calling model_fn.
W0618 12:11:34.469639 47621015040896 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:34.469635 47603800150912 estimator.py:1111] Calling model_fn.
W0618 12:11:34.469743 47603800150912 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.469821 47496634712960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.469799 47696147583872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:34.470999 47621015040896 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.471097 47603800150912 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.471725 47495508530048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:34.474115 47696147583872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:34.474866 47496634712960 estimator.py:1111] Calling model_fn.
W0618 12:11:34.474975 47496634712960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:34.476325 47496634712960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:34.476686 47525415879552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:11:34.476867 47495508530048 estimator.py:1111] Calling model_fn.
W0618 12:11:34.476980 47495508530048 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881494.355732 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.356278 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.356737 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.476452 47481049502592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
:::MLL 1560881494.363773 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881494.364270 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881494.364693 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:34.476703 47770580435840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000029-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000020-000011.tfrecord.zz_0_0
W0618 12:11:34.478230 4695[2019-06-18 12:12:12] iteration time 29: 48.518 seconds
2019-06-18 12:12:14.182581: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881532.599943 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:12:17] minmax time: 3.252 seconds
2019-06-18 12:12:17.444899: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:17.450334: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:17.455030: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881537.467992 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 29}}
[2019-06-18 12:12:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb210 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb207 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb134/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb134/models/000031-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb189 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:12:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-eval-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=31 : \
-host epb171 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=1023779862 : \
-host epb133 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=2047559693 : \
-host epb132 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=3071339524 : \
-host epb173 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=4095119355 : \
-host epb131 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=5118899186 : \
-host epb174 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=6142679017 : \
-host epb130 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=7166458848 : \
-host epb175 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=8190238679 : \
-host epb219 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=9214018510 : \
-host epb176 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=10237798341 : \
-host epb218 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=11261578172 : \
-host epb217 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=12285358003 : \
-host epb178 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=13309137834 : \
-host epb215 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=14332917665 : \
-host epb214 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=15356697496 : \
-host epb200 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=16380477327 : \
-host epb179 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=17404257158 : \
-host epb201 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=18428036989 : \
-host epb213 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=19451816820 : \
-host epb202 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000030-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/000030-000020 --seed=20475596651 : \
-host epb203 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:12:28] eval finished: 10.736 seconds
[2019-06-18 12:12:28] Win rate 000030-000020 vs 000028-000019: 0.430
:::MLL 1560881548.279110 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:12:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-selfplay-32-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb134 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=32 : \
-host epb171 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=1023779863 : \
-host epb133 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=2047559694 : \
-host epb132 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=3071339525 : \
-host epb173 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=4095119356 : \
-host epb131 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=5118899187 : \
-host epb174 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=6142679018 : \
-host epb130 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=7166458849 : \
-host epb175 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=8190238680 : \
-host epb219 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=9214018511 : \
-host epb176 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=10237798342 : \
-host epb218 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=11261578173 : \
-host epb217 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=12285358004 : \
-host epb178 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=13309137835 : \
-host epb215 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=14332917666 : \
-host epb214 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=15356697497 : \
-host epb200 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=16380477328 : \
-host epb179 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=17404257159 : \
-host epb201 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb134/data/holdout/000031-000019 --seed=18428036990 : \
-host epb213 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb134/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000028-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb13
[2019-06-18 12:12:59] selfplay finished: 31.140 seconds
[2019-06-18 12:12:59] selfplay mn: 31.160 seconds
[2019-06-18 12:12:59] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb134/mpi/out-divide_golden_chunk-32-%r.txt \
-host epb134 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=32 : \
-host epb171 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=1023779863 : \
-host epb133 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=2047559694 : \
-host epb132 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=3071339525 : \
-host epb173 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=4095119356 : \
-host epb131 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=5118899187 : \
-host epb174 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=6142679018 : \
-host epb130 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=7166458849 : \
-host epb175 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=8190238680 : \
-host epb219 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=9214018511 : \
-host epb176 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=10237798342 : \
-host epb218 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=11261578173 : \
-host epb217 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=12285358004 : \
-host epb178 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=13309137835 : \
-host epb215 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=14332917666 : \
-host epb214 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=15356697497 : \
-host epb200 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=16380477328 : \
-host epb179 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=17404257159 : \
-host epb201 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=18428036990 : \
-host epb213 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=19451816821 : \
-host epb202 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=20475596652 : \
-host epb203 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=21499376483 : \
-host epb211 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000031-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb134 --seed=22523156314 : \
-host epb204 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb134/data/selfplay/000031-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb134/data/golde
[2019-06-18 12:13:00] train finished: 43.491 seconds
:::MLL 1560881542.720505 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.721353 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.722166 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.848511 47929481081728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.729722 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.730472 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.731173 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.848848 47859312108416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.849565 47929481081728 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_8h35rrv
I0618 12:12:22.850684 47929481081728 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_8h35rrv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b97bc480e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.849869 47859312108416 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpkjyaup55
I0618 12:12:22.851004 47859312108416 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpkjyaup55', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8765e27e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.851139 47929481081728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.851434 47859312108416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.855832 47929481081728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.856106 47859312108416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881542.733857 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.734671 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.735476 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.864911 47316123407232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.734452 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.735327 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.736078 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.865218 47650821104512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.865952 47316123407232 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpww89ddv9
I0618 12:12:22.866966 47316123407232 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpww89ddv9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b08ed50ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.866262 47650821104512 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpjzcbznjs
I0618 12:12:22.867284 47650821104512 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpjzcbznjs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b56dada3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.867367 47316123407232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.867692 47650821104512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881542.729677 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.730596 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.731466 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.869568 47229253268352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.735995 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.736746 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.737430 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.869789 47088518038400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.870713 47229253268352 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp_2g8ofbl
W0618 12:12:22.870897 47088518038400 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp8_zqv3ei
I0618 12:12:22.871820 47229253268352 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp_2g8ofbl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af4b3737e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.872124 47316123407232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:22.871996 47088518038400 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp8_zqv3ei', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad3eefa5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.872422 47650821104512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:22.872283 47229253268352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.872457 47088518038400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.874975 47929481081728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.875201 47859312108416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.877569 47229253268352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.877738 47088518038400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.891237 47316123407232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.891974 47650821104512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.899247 47229253268352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.899676 47088518038400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.922701 47929481081728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.923334 47859312108416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.926959 47929481081728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:22.927649 47859312108416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:22.931985 47929481081728 estimator.py:1111] Calling model_fn.
W0618 12:12:22.932096 47929481081728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:22.932743 47859312108416 estimator.py:1111] Calling model_fn.
W0618 12:12:22.932857 47859312108416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881542.822575 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.823079 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.823445 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.933010 47535339410304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.933455 47929481081728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881542.825690 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.826159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.826528 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.933490 47979587433344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.934232 47859312108416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:22.934038 47535339410304 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpltt2rcfz
I0618 12:12:22.935039 47535339410304 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpltt2rcfz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3bf79b6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.934492 47979587433344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpax7gsd6v
I0618 12:12:22.935446 47535339410304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.935484 47979587433344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpax7gsd6v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba366da4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.935888 47979587433344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881542.834023 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.834577 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.835062 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.938799 47190522315648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.938968 47316123407232 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.939623 47650821104512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.940031 47535339410304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.940498 47979587433344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.939839 47190522315648 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpb339pvbb
I0618 12:12:22.940834 47190522315648 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpb339pvbb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aebaee81e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.941234 47190522315648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881542.842745 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.843191 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.843574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.941367 47180619375488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.825685 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.826246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.826659 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.942517 46981799785344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.828591 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.829004 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.829428 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.942472 47301629297536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.942373 47180619375488 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp7ru0o2_f
I0618 12:12:22.943340 47180619375488 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp7ru0o2_f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae960a53e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:22.943303 47316123407232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:22.943742 47180619375488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.943994 47650821104512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:22.943627 46981799785344 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpvzbxvrhj
W0618 12:12:22.943594 47301629297536 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmp0pxrmr27
I0618 12:12:22.944655 47301629297536 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmp0pxrmr27', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b058d661e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.944687 46981799785344 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpvzbxvrhj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb16130e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.945075 47301629297536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.945114 46981799785344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.945907 47190522315648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.948339 47180619375488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:22.948384 47316123407232 estimator.py:1111] Calling model_fn.
W0618 12:12:22.948498 47316123407232 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:22.949124 47650821104512 estimator.py:1111] Calling model_fn.
W0618 12:12:22.949234 47650821104512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:22.949857 47316123407232 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:22.949636 47229253268352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.949889 46981799785344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.949858 47301629297536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.950594 47650821104512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:22.950795 47088518038400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:22.953943 47229253268352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:22.955199 47088518038400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:22.958869 47535339410304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:22.958952 47229253268352 estimator.py:1111] Calling model_fn.
W0618 12:12:22.959060 47229253268352 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:22.959472 47979587433344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:22.960310 47088518038400 estimator.py:1111] Calling model_fn.
W0618 12:12:22.960421 47088518038400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:22.960415 47229253268352 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:22.961774 47088518038400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881542.793085 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.793877 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.794719 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.964554 47993865220992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.964834 47190522315648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.965621 47993865220992 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmps604atmb
I0618 12:12:22.966771 47993865220992 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmps604atmb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6b9e00e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.967225 47993865220992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.967380 47180619375488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.968907 46981799785344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.968873 47301629297536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:22.972075 47993865220992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881542.800841 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.801744 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.802622 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.974766 46991407981440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560881542.805754 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.806452 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.807110 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.975377 47665565631360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.975864 46991407981440 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl9paf1kt
I0618 12:12:22.976936 46991407981440 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl9paf1kt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abd52c47e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.976445 47665565631360 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb134/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5a49b1dd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.977383 46991407981440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:22.977710 47665565631360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881542.794696 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.795487 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.796178 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:22.981925 47254379316096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000030-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:12:22.982669 46991407981440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.982969 47254379316096 estimator.py:1760] Using temporary folder as model directory: /tmp/96732.tmpdir/tmpl4egflo5
W0618 12:12:22.983082 47665565631360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:22.983961 47254379316096 estimator.py:201] Using config: {'_model_dir': '/tmp/96732.tmpdir/tmpl4egflo5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa8d149e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:22.984368 47254379316096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:22.988996 47254379316096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:22.991669 47993865220992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:23.004510 46991407981440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:23.005692 47535339410304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:23.005378 47665565631360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:23.006279 47979587433344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:23.008806 47254379316096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:23.009967 47535339410304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:23.010582 47979587433344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:23.011554 47190522315648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:23.014394 47180619375488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:12:23.015000 47535339410304 estimator.py:1111] Calling model_fn.
W0618 12:12:23.015108 47535339410304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:23.015698 47979587433344 estimator.py:1111] Calling model_fn.
W0618 12:12:23.015808 47979587433344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:23.015847 47190522315648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:23.016098 47301629297536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:23.016470 47535339410304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:23.016335 46981799785344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:23.017165 47979587433344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:23.018706 47180619375488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:23.020418 47301629297536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:23.020627 46981799785344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:23.020887 47190522315648 estimator.py:1111] Calling model_fn.
W0618 12:12:23.020996 47190522315648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:23.022349 47190522315648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:12:23.023790 47180619375488 estimator.py:1111] Calling model_fn.
W0618 12:12:23.023896 47180619375488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:23.025270 47180619375488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:12:23.025499 47301629297536 estimator.py:1111] Calling model_fn.
W0618 12:12:23.025608 47301629297536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:23.025673 46981799785344 estimator.py:1111] Calling model_fn.
W0618 12:12:23.025784 46981799785344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:23.026979 47301629297536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:23.027138 46981799785344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881542.866047 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881542.866601 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881542.867085 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750][2019-06-18 12:13:02] divide_golden_chunk finished: 3.439 seconds
[2019-06-18 12:13:02] generate golden chunk: 3.453 seconds
[2019-06-18 12:13:02] iteration time 30: 50.294 seconds
:::MLL 1560881582.894505 epoch_stop: {"value": null, "metadata": {'lineno': 737, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:13:02] Total time: 1705.033 seconds

numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000018-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000018-000012log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000019-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000019-000012log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000020-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000020-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000021-000014_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000021-000014log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000022-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000022-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000023-000016_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000023-000016log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000024-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000024-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000025-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000025-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000026-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000026-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000027-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000027-000018log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000028-000019_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000028-000019log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000029-000020_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000029-000020log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb134/models/000030-000020_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb134/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb134/models/000030-000020log.txt
:::MLL 1560881585.495171 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
I0618 12:13:05.495914 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=1
I0618 12:13:32.099957 47431397376896 utils.py:86] eval finished: 26.603 seconds
I0618 12:13:32.103419 47431397376896 reference_implementation.py:563] Win rate 000001-000001 vs target: 0.050
:::MLL 1560881612.104384 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560881612.104724 eval_accuracy: {"value": 0.05, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560881612.105040 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
I0618 12:13:32.105345 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=2
I0618 12:13:58.543394 47431397376896 utils.py:86] eval finished: 26.438 seconds
I0618 12:13:58.546262 47431397376896 reference_implementation.py:563] Win rate 000002-000002 vs target: 0.050
:::MLL 1560881638.546937 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560881638.547267 eval_accuracy: {"value": 0.05, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560881638.547575 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
I0618 12:13:58.547877 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000003-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=3
I0618 12:14:24.825629 47431397376896 utils.py:86] eval finished: 26.278 seconds
I0618 12:14:24.828486 47431397376896 reference_implementation.py:563] Win rate 000003-000002 vs target: 0.130
:::MLL 1560881664.829370 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560881664.829684 eval_accuracy: {"value": 0.13, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560881664.829999 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
I0618 12:14:24.830294 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=4
I0618 12:14:51.564111 47431397376896 utils.py:86] eval finished: 26.734 seconds
I0618 12:14:51.566997 47431397376896 reference_implementation.py:563] Win rate 000004-000003 vs target: 0.080
:::MLL 1560881691.567643 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560881691.567960 eval_accuracy: {"value": 0.08, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560881691.568265 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
I0618 12:14:51.568565 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=5
I0618 12:15:19.226423 47431397376896 utils.py:86] eval finished: 27.658 seconds
I0618 12:15:19.229311 47431397376896 reference_implementation.py:563] Win rate 000005-000004 vs target: 0.050
:::MLL 1560881719.230005 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560881719.230332 eval_accuracy: {"value": 0.05, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560881719.230659 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
I0618 12:15:19.230966 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000006-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=6
I0618 12:15:43.839005 47431397376896 utils.py:86] eval finished: 24.608 seconds
I0618 12:15:43.841865 47431397376896 reference_implementation.py:563] Win rate 000006-000005 vs target: 0.120
:::MLL 1560881743.843037 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560881743.843351 eval_accuracy: {"value": 0.12, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560881743.843657 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
I0618 12:15:43.843969 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000007-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=7
I0618 12:16:09.128721 47431397376896 utils.py:86] eval finished: 25.285 seconds
I0618 12:16:09.131559 47431397376896 reference_implementation.py:563] Win rate 000007-000005 vs target: 0.120
:::MLL 1560881769.132222 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560881769.132532 eval_accuracy: {"value": 0.12, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560881769.132836 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
I0618 12:16:09.133144 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000008-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=8
I0618 12:16:34.892564 47431397376896 utils.py:86] eval finished: 25.759 seconds
I0618 12:16:34.895360 47431397376896 reference_implementation.py:563] Win rate 000008-000006 vs target: 0.110
:::MLL 1560881794.896019 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560881794.896332 eval_accuracy: {"value": 0.11, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560881794.896640 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
I0618 12:16:34.896950 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000009-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=9
I0618 12:16:59.241783 47431397376896 utils.py:86] eval finished: 24.345 seconds
I0618 12:16:59.244644 47431397376896 reference_implementation.py:563] Win rate 000009-000007 vs target: 0.110
:::MLL 1560881819.245528 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560881819.245864 eval_accuracy: {"value": 0.11, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560881819.246171 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
I0618 12:16:59.246478 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=10
I0618 12:17:24.237370 47431397376896 utils.py:86] eval finished: 24.991 seconds
I0618 12:17:24.240361 47431397376896 reference_implementation.py:563] Win rate 000010-000007 vs target: 0.160
:::MLL 1560881844.241022 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560881844.241336 eval_accuracy: {"value": 0.16, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560881844.241641 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
I0618 12:17:24.241940 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=11
I0618 12:17:48.320883 47431397376896 utils.py:86] eval finished: 24.079 seconds
I0618 12:17:48.323847 47431397376896 reference_implementation.py:563] Win rate 000011-000008 vs target: 0.170
:::MLL 1560881868.333431 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560881868.333751 eval_accuracy: {"value": 0.17, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560881868.334072 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
I0618 12:17:48.334381 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=12
I0618 12:18:13.737491 47431397376896 utils.py:86] eval finished: 25.403 seconds
I0618 12:18:13.740300 47431397376896 reference_implementation.py:563] Win rate 000012-000008 vs target: 0.190
:::MLL 1560881893.744991 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560881893.745310 eval_accuracy: {"value": 0.19, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560881893.745616 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
I0618 12:18:13.745934 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=13
I0618 12:18:40.490737 47431397376896 utils.py:86] eval finished: 26.745 seconds
I0618 12:18:40.493509 47431397376896 reference_implementation.py:563] Win rate 000013-000009 vs target: 0.300
:::MLL 1560881920.494150 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560881920.494459 eval_accuracy: {"value": 0.3, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560881920.494764 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
I0618 12:18:40.495067 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=14
I0618 12:19:05.978242 47431397376896 utils.py:86] eval finished: 25.483 seconds
I0618 12:19:05.981108 47431397376896 reference_implementation.py:563] Win rate 000014-000010 vs target: 0.190
:::MLL 1560881945.981755 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560881945.982072 eval_accuracy: {"value": 0.19, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560881945.982376 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
I0618 12:19:05.982679 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000015-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=15
I0618 12:19:30.330813 47431397376896 utils.py:86] eval finished: 24.348 seconds
I0618 12:19:30.333655 47431397376896 reference_implementation.py:563] Win rate 000015-000010 vs target: 0.340
:::MLL 1560881970.338306 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560881970.338629 eval_accuracy: {"value": 0.34, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560881970.338943 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
I0618 12:19:30.339249 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=16
I0618 12:19:55.648559 47431397376896 utils.py:86] eval finished: 25.309 seconds
I0618 12:19:55.651381 47431397376896 reference_implementation.py:563] Win rate 000016-000011 vs target: 0.420
:::MLL 1560881995.652339 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560881995.652653 eval_accuracy: {"value": 0.42, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560881995.652964 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
I0618 12:19:55.653275 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000017-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=17
I0618 12:20:18.745919 47431397376896 utils.py:86] eval finished: 23.092 seconds
I0618 12:20:18.748753 47431397376896 reference_implementation.py:563] Win rate 000017-000011 vs target: 0.480
:::MLL 1560882018.749413 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882018.749727 eval_accuracy: {"value": 0.48, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882018.750039 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
I0618 12:20:18.750343 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000018-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=18
I0618 12:20:42.008352 47431397376896 utils.py:86] eval finished: 23.258 seconds
I0618 12:20:42.011233 47431397376896 reference_implementation.py:563] Win rate 000018-000012 vs target: 0.360
:::MLL 1560882042.012120 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
:::MLL 1560882042.012433 eval_accuracy: {"value": 0.36, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 17}}
:::MLL 1560882042.012737 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 18}}
I0618 12:20:42.013056 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000019-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=19
I0618 12:21:06.031433 47431397376896 utils.py:86] eval finished: 24.018 seconds
I0618 12:21:06.034250 47431397376896 reference_implementation.py:563] Win rate 000019-000012 vs target: 0.400
:::MLL 1560882066.035277 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 18}}
:::MLL 1560882066.035592 eval_accuracy: {"value": 0.4, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 18}}
:::MLL 1560882066.035904 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 19}}
I0618 12:21:06.036218 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=20
I0618 12:21:29.299256 47431397376896 utils.py:86] eval finished: 23.263 seconds
I0618 12:21:29.302155 47431397376896 reference_implementation.py:563] Win rate 000020-000013 vs target: 0.490
:::MLL 1560882089.302797 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 19}}
:::MLL 1560882089.303116 eval_accuracy: {"value": 0.49, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 19}}
:::MLL 1560882089.303420 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 20}}
I0618 12:21:29.303709 47431397376896 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb134/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb134/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb134/sgf/eval/target --seed=21
I0618 12:21:50.867409 47431397376896 utils.py:86] eval finished: 21.564 seconds
I0618 12:21:50.870218 47431397376896 reference_implementation.py:563] Win rate 000021-000014 vs target: 0.620
:::MLL 1560882110.871099 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 20}}
:::MLL 1560882110.871414 eval_accuracy: {"value": 0.62, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 20}}
:::MLL 1560882110.871728 eval_result: {"value": null, "metadata": {'lineno': 52, 'file': 'ml_perf/eval_models.py', 'iteration': 20, 'timestamp': 1040.279}}
:::MLL 1560882110.872039 run_stop: {"value": null, "metadata": {'lineno': 53, 'file': 'ml_perf/eval_models.py', 'status': 'success'}}
Model 000021-000014 beat target after 1040.279s
~/submission/benchmarks/minigo/clx-8260l-2s-x32
